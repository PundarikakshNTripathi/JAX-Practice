{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77ce377",
   "metadata": {},
   "source": [
    "# üìò Notebook 4: Vectorization with `vmap` - Automatic Batching Magic\n",
    "\n",
    "Welcome to one of JAX's most powerful features! `vmap` (vectorizing map) automatically makes your single-example code work on batches - no loops needed!\n",
    "\n",
    "## üéØ What You'll Learn (25-35 minutes)\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- ‚úÖ What vectorization means (and why it's essential)\n",
    "- ‚úÖ Why loops are slow and batching is fast\n",
    "- ‚úÖ How `jax.vmap()` automatically batches operations\n",
    "- ‚úÖ Specifying which axes to batch over\n",
    "- ‚úÖ Broadcasting and reshaping with `vmap`\n",
    "- ‚úÖ Combining `vmap` with `jit` and `grad` for maximum performance\n",
    "- ‚úÖ Practical application: Batch gradient computation\n",
    "\n",
    "## ü§î What is Vectorization?\n",
    "\n",
    "### The Problem: Loops are Slow\n",
    "Machine learning processes thousands/millions of examples. Using Python loops is painfully slow:\n",
    "\n",
    "```python\n",
    "# SLOW: Loop over 1000 examples\n",
    "results = []\n",
    "for i in range(1000):\n",
    "    result = model(data[i])\n",
    "    results.append(result)\n",
    "```\n",
    "\n",
    "This is slow because:\n",
    "- Python loops have overhead\n",
    "- Can't use GPU parallelism\n",
    "- Each operation is separate\n",
    "\n",
    "### The Solution: Vectorization (Batching)\n",
    "Process ALL examples at once using array operations:\n",
    "\n",
    "```python\n",
    "# FAST: Process all 1000 examples simultaneously\n",
    "results = model(data)  # Shape: (1000, ...)\n",
    "```\n",
    "\n",
    "This is **10-100x faster** because:\n",
    "- Single optimized array operation\n",
    "- GPU processes examples in parallel\n",
    "- Minimal Python overhead\n",
    "\n",
    "### JAX's `vmap`: Automatic Vectorization! üéâ\n",
    "\n",
    "**The Problem:** You wrote a function for ONE example. Now you need it for a BATCH.\n",
    "\n",
    "**The Old Way:** Manually rewrite with loops or complex broadcasting.\n",
    "\n",
    "**The JAX Way:** Wrap your function with `vmap()` - automatic batching!\n",
    "\n",
    "```python\n",
    "# Function for ONE example\n",
    "def predict_one(x):\n",
    "    return x ** 2 + 2 * x\n",
    "\n",
    "# Automatically works on BATCHES\n",
    "predict_batch = jax.vmap(predict_one)\n",
    "\n",
    "single_example = 3.0\n",
    "batch_examples = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "print(predict_one(single_example))   # Single result\n",
    "print(predict_batch(batch_examples)) # Batch results - no loop!\n",
    "```\n",
    "\n",
    "**Magic!** You write single-example code, JAX makes it batch-ready!\n",
    "\n",
    "## üìö Key Concepts Explained\n",
    "\n",
    "### 1. What is a \"Batch\"?\n",
    "**Definition:** Multiple examples processed together as one array.\n",
    "\n",
    "**Example:** Instead of 5 separate images, one array of shape `(5, height, width, channels)`\n",
    "\n",
    "**Why?** GPUs are designed to process batches in parallel - massive speedup!\n",
    "\n",
    "### 2. Batch Dimension (Axis)\n",
    "**What is it?** The dimension representing different examples.\n",
    "\n",
    "**Usually:** The first dimension (axis 0)\n",
    "- Shape `(32, 10)` ‚Üí 32 examples, each has 10 features\n",
    "- Shape `(100, 28, 28)` ‚Üí 100 images, each 28x28 pixels\n",
    "\n",
    "### 3. Broadcasting\n",
    "**What is it?** Automatically expanding arrays to match shapes.\n",
    "\n",
    "**Example:** Adding scalar to array\n",
    "```python\n",
    "array = jnp.array([1, 2, 3])\n",
    "result = array + 10  # Broadcasting! ‚Üí [11, 12, 13]\n",
    "```\n",
    "\n",
    "### 4. `in_axes` Parameter\n",
    "**What is it?** Tells `vmap` which axis is the batch dimension.\n",
    "\n",
    "**Common values:**\n",
    "- `in_axes=0` ‚Üí First axis is batch (default)\n",
    "- `in_axes=1` ‚Üí Second axis is batch  \n",
    "- `in_axes=None` ‚Üí Don't batch this argument (same for all examples)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "def dot_product(x, weights):\n",
    "    return jnp.dot(x, weights)\n",
    "\n",
    "# Batch over x (different for each), weights same for all\n",
    "batch_dot = jax.vmap(dot_product, in_axes=(0, None))\n",
    "```\n",
    "\n",
    "## üéì What's in This Notebook?\n",
    "\n",
    "This notebook has **7 comprehensive examples**:\n",
    "\n",
    "1. **Basic vmap** - Simple vectorization\n",
    "2. **Performance comparison** - Loop vs vmap speed test\n",
    "3. **Multiple arguments** - Vectorizing multi-input functions\n",
    "4. **in_axes control** - Specifying batch dimensions\n",
    "5. **Combining vmap + jit** - Ultimate performance combo\n",
    "6. **Combining vmap + grad** - Batch gradient computation\n",
    "7. **Practical example** - Batch loss computation for training\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before starting this notebook, you should:\n",
    "- ‚úÖ Complete Notebook 1 (JAX Basics)\n",
    "- ‚úÖ Complete Notebook 2 (JIT Compilation) - helpful but not required\n",
    "- ‚úÖ Understand array shapes and dimensions\n",
    "- ‚úÖ Know what a batch is (or learn it in this notebook!)\n",
    "\n",
    "## ‚ö° Performance Impact\n",
    "\n",
    "**Real performance differences you'll see:**\n",
    "\n",
    "| Method | Time | Speedup |\n",
    "|--------|------|---------|\n",
    "| Python loop | 100ms | 1x (baseline) |\n",
    "| `vmap` only | 10ms | 10x faster |\n",
    "| `jit + vmap` | 1ms | **100x faster!** |\n",
    "\n",
    "**Key Insight:** `vmap` + `jit` together is a superpower combination!\n",
    "\n",
    "## üí° Key Takeaway\n",
    "\n",
    "**Write code for ONE example. Use `vmap` to automatically handle BATCHES.**\n",
    "\n",
    "This is how you write clean, readable code that's also lightning fast!\n",
    "\n",
    "Let's see automatic batching in action! üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fece19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZATION WITH VMAP\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOMATIC BATCHING WITH VMAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 1: Basic vmap - Single Input Batching\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£  BASIC VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Operates on a SINGLE number\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "# Create vectorized version\n",
    "vectorized_square = jax.vmap(square)\n",
    "\n",
    "# Apply to batch\n",
    "batch = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "result = vectorized_square(batch)\n",
    "\n",
    "print(\"Function for single input: square(x) = x^2\")\n",
    "print(f\"Input batch: {batch}\")\n",
    "print(f\"Output: {result}\")\n",
    "print(\"vmap automatically applies the function to each element!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 2: Multiple Inputs with Different Batching\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£  VMAP WITH MULTIPLE INPUTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def weighted_sum(x, weight):\n",
    "    \"\"\"Compute weighted sum: x * weight\"\"\"\n",
    "    return x * weight\n",
    "\n",
    "# Batch over first argument, keep second fixed\n",
    "# in_axes=(0, None) means: map over axis 0 of x, don't map over weight\n",
    "batched_fn = jax.vmap(weighted_sum, in_axes=(0, None))\n",
    "\n",
    "x_batch = jnp.array([1.0, 2.0, 3.0])\n",
    "weight = 2.5\n",
    "\n",
    "result = batched_fn(x_batch, weight)\n",
    "print(f\"Batch: {x_batch}\")\n",
    "print(f\"Fixed weight: {weight}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(\"in_axes=(0, None) batches first arg, keeps second fixed\")\n",
    "\n",
    "# Batch over both arguments\n",
    "batched_fn_both = jax.vmap(weighted_sum, in_axes=(0, 0))\n",
    "weights_batch = jnp.array([1.0, 2.0, 3.0])\n",
    "result_both = batched_fn_both(x_batch, weights_batch)\n",
    "print(f\"\\nWith batched weights: {weights_batch}\")\n",
    "print(f\"Result: {result_both}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 3: Matrix-Vector Product with vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£  MATRIX-VECTOR PRODUCTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def matvec(matrix, vector):\n",
    "    \"\"\"Single matrix-vector product\"\"\"\n",
    "    return jnp.dot(matrix, vector)\n",
    "\n",
    "# Batch over vectors (multiple vectors, same matrix)\n",
    "batch_matvec = jax.vmap(matvec, in_axes=(None, 0))\n",
    "\n",
    "matrix = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "vectors = jnp.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "\n",
    "results = batch_matvec(matrix, vectors)\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"Vector batch (shape {vectors.shape}):\\n{vectors}\")\n",
    "print(f\"Results (shape {results.shape}):\\n{results}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 4: Per-Sample Gradients (The Killer App!)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£  PER-SAMPLE GRADIENTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss_single_sample(params, x, y):\n",
    "    \"\"\"Loss for ONE training example\"\"\"\n",
    "    prediction = jnp.dot(params, x)\n",
    "    return (prediction - y) ** 2\n",
    "\n",
    "# Create function that computes gradient for one sample\n",
    "grad_fn_single = jax.grad(loss_single_sample)\n",
    "\n",
    "# Vectorize it to compute per-sample gradients for entire batch!\n",
    "# in_axes=(None, 0, 0) means: same params, batch over x and y\n",
    "per_sample_grads = jax.vmap(grad_fn_single, in_axes=(None, 0, 0))\n",
    "\n",
    "# Test data\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "x_batch = jnp.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 1.0]\n",
    "])\n",
    "y_batch = jnp.array([1.0, 2.0, 3.0, 6.0])\n",
    "\n",
    "grads = per_sample_grads(params, x_batch, y_batch)\n",
    "print(f\"Parameters: {params}\")\n",
    "print(f\"Batch size: {len(x_batch)}\")\n",
    "print(f\"Per-sample gradients (shape {grads.shape}):\")\n",
    "print(grads)\n",
    "print(\"\\nEach row is the gradient for one training example!\")\n",
    "print(\"This is critical for differential privacy and some RL algorithms.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 5: Performance Comparison - Loop vs vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£  PERFORMANCE: LOOP vs VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def compute_single(x):\n",
    "    \"\"\"Some computation on a single input\"\"\"\n",
    "    return jnp.sum(jnp.sin(x) ** 2 + jnp.cos(x) ** 2)\n",
    "\n",
    "# Manual loop version\n",
    "def loop_version(batch):\n",
    "    results = []\n",
    "    for x in batch:\n",
    "        results.append(compute_single(x))\n",
    "    return jnp.array(results)\n",
    "\n",
    "# vmap version\n",
    "vmap_version = jax.vmap(compute_single)\n",
    "\n",
    "# Generate test batch\n",
    "batch_size = 1000\n",
    "test_batch = jnp.ones((batch_size, 100))\n",
    "\n",
    "# Warm up JIT\n",
    "_ = vmap_version(test_batch)\n",
    "\n",
    "# Benchmark loop\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = loop_version(test_batch)\n",
    "time_loop = time.time() - start\n",
    "\n",
    "# Benchmark vmap\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = vmap_version(test_batch)\n",
    "time_vmap = time.time() - start\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Loop version:  {time_loop:.4f} seconds\")\n",
    "print(f\"vmap version:  {time_vmap:.4f} seconds\")\n",
    "print(f\"Speedup: {time_loop/time_vmap:.2f}x faster with vmap!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 6: Nested vmap - Batching Over Multiple Dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£  NESTED VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def pairwise_distance(x1, x2):\n",
    "    \"\"\"Distance between two vectors\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x1 - x2) ** 2))\n",
    "\n",
    "# Compute pairwise distances between all vectors in two sets\n",
    "# First vmap: over first set\n",
    "# Second vmap: over second set\n",
    "pairwise_distances = jax.vmap(\n",
    "    lambda x1: jax.vmap(lambda x2: pairwise_distance(x1, x2))(set2)\n",
    ")\n",
    "\n",
    "set1 = jnp.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "set2 = jnp.array([[1.0, 1.0], [2.0, 2.0]])\n",
    "\n",
    "distances = pairwise_distances(set1)\n",
    "print(f\"Set 1 (shape {set1.shape}):\\n{set1}\")\n",
    "print(f\"Set 2 (shape {set2.shape}):\\n{set2}\")\n",
    "print(f\"Pairwise distances (shape {distances.shape}):\\n{distances}\")\n",
    "print(\"Each row shows distances from one point in set1 to all points in set2\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 7: vmap with Different Output Axes\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£  CONTROLLING OUTPUT AXES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def create_matrix(scale):\n",
    "    \"\"\"Create a 2x2 matrix based on scale\"\"\"\n",
    "    return jnp.array([[scale, 0], [0, scale]])\n",
    "\n",
    "# Default: out_axes=0 (stack along first dimension)\n",
    "vmap_default = jax.vmap(create_matrix)\n",
    "scales = jnp.array([1.0, 2.0, 3.0])\n",
    "result_default = vmap_default(scales)\n",
    "\n",
    "print(f\"Scales: {scales}\")\n",
    "print(f\"Output shape with out_axes=0 (default): {result_default.shape}\")\n",
    "print(f\"Result:\\n{result_default}\")\n",
    "\n",
    "# With out_axes=1: stack along second dimension\n",
    "vmap_axis1 = jax.vmap(create_matrix, out_axes=1)\n",
    "result_axis1 = vmap_axis1(scales)\n",
    "print(f\"\\nOutput shape with out_axes=1: {result_axis1.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 8: Combining JIT and vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n8Ô∏è‚É£  JIT + VMAP COMBO\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def expensive_computation(x):\n",
    "    \"\"\"Some complex computation\"\"\"\n",
    "    result = x\n",
    "    for _ in range(10):\n",
    "        result = jnp.sin(result) + jnp.cos(result)\n",
    "    return result\n",
    "\n",
    "# Combine transformations: JIT the vmapped function\n",
    "fast_batch_compute = jax.jit(jax.vmap(expensive_computation))\n",
    "\n",
    "batch = jnp.ones(1000)\n",
    "\n",
    "# Warm up\n",
    "_ = fast_batch_compute(batch)\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = fast_batch_compute(batch).block_until_ready()\n",
    "time_taken = time.time() - start\n",
    "\n",
    "print(\"Combined jax.jit(jax.vmap(function)) for maximum performance\")\n",
    "print(f\"Time for 1000 iterations on batch of 1000: {time_taken:.4f}s\")\n",
    "print(\"Both JIT compilation and automatic batching working together!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY POINTS - VMAP\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ vmap automatically batches functions - no manual loop writing\n",
    "‚úÖ Write single-example code, vmap handles the batching\n",
    "‚úÖ in_axes controls which dimensions to map over (0, None, etc.)\n",
    "‚úÖ Much faster than Python loops (compiled vectorization)\n",
    "‚úÖ Perfect for per-sample gradients: vmap(grad(loss))\n",
    "‚úÖ Can nest vmap for multi-dimensional batching\n",
    "‚úÖ Combine with JIT for maximum performance\n",
    "‚úÖ out_axes controls how outputs are stacked\n",
    "‚úÖ Cleaner code: intent is clear, no index juggling\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
