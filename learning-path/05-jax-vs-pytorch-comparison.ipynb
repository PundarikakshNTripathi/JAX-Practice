{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31af583b",
   "metadata": {},
   "source": [
    "# üìò Notebook 5: JAX vs PyTorch - A Side-by-Side Comparison\n",
    "\n",
    "Welcome to the \"Rosetta Stone\" of deep learning frameworks! This notebook compares JAX and PyTorch directly, showing you how to translate between them.\n",
    "\n",
    "## üéØ What You'll Learn (25-35 minutes)\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- ‚úÖ Core philosophy differences between JAX and PyTorch\n",
    "- ‚úÖ How to do the same operations in both frameworks\n",
    "- ‚úÖ Array creation and manipulation in both\n",
    "- ‚úÖ Automatic differentiation differences\n",
    "- ‚úÖ Model definition approaches\n",
    "- ‚úÖ Training loops in JAX vs PyTorch\n",
    "- ‚úÖ When to choose JAX vs PyTorch\n",
    "\n",
    "## ü§î JAX vs PyTorch: What's the Difference?\n",
    "\n",
    "### PyTorch: The Deep Learning Framework\n",
    "**Philosophy:** Object-oriented, imperative, mutable state\n",
    "\n",
    "**Best for:**\n",
    "- Traditional deep learning workflows\n",
    "- Researchers who want flexibility\n",
    "- Projects with complex dynamic architectures\n",
    "- Quick prototyping with `nn.Module`\n",
    "\n",
    "**Key features:**\n",
    "- Built-in neural network layers (`torch.nn`)\n",
    "- Automatic differentiation with `.backward()`\n",
    "- Training utilities (optimizers, data loaders)\n",
    "- Mutable tensors (can modify in-place)\n",
    "\n",
    "### JAX: The NumPy++ for ML\n",
    "**Philosophy:** Functional, immutable, composable transformations\n",
    "\n",
    "**Best for:**\n",
    "- Research requiring custom operations\n",
    "- High-performance scientific computing\n",
    "- Projects needing advanced transformations (vmap, pmap)\n",
    "- When you want fine-grained control\n",
    "\n",
    "**Key features:**\n",
    "- NumPy-compatible API\n",
    "- Functional transformations (jit, grad, vmap, pmap)\n",
    "- Immutable arrays (safer, easier to optimize)\n",
    "- Composability (combine transformations freely)\n",
    "\n",
    "## üìä Quick Comparison Table\n",
    "\n",
    "| Feature | JAX | PyTorch |\n",
    "|---------|-----|---------|\n",
    "| **Arrays** | `jax.numpy` (immutable) | `torch.Tensor` (mutable) |\n",
    "| **Gradients** | `jax.grad(fn)` (functional) | `tensor.backward()` (OOP) |\n",
    "| **JIT Compilation** | `@jax.jit` decorator | `@torch.compile` (PyTorch 2.0+) |\n",
    "| **Batching** | `jax.vmap` (automatic) | Manual or `torch.vmap` (newer) |\n",
    "| **Neural Networks** | DIY or Flax/Haiku | `torch.nn.Module` (built-in) |\n",
    "| **Optimizers** | DIY or Optax | `torch.optim` (built-in) |\n",
    "| **GPU Support** | Automatic | `.to('cuda')` or `.cuda()` |\n",
    "| **Parallelism** | `jax.pmap` (powerful) | `torch.distributed` |\n",
    "| **Learning Curve** | Steeper (functional style) | Gentler (OOP familiar) |\n",
    "| **Ecosystem** | Growing | Massive |\n",
    "\n",
    "## üß© Key Philosophical Differences\n",
    "\n",
    "### 1. Mutability\n",
    "**PyTorch:** Tensors are mutable (can be modified)\n",
    "```python\n",
    "x = torch.tensor([1.0, 2.0])\n",
    "x[0] = 10.0  # ‚úÖ Works! Modifies in-place\n",
    "```\n",
    "\n",
    "**JAX:** Arrays are immutable (cannot be modified)\n",
    "```python\n",
    "x = jnp.array([1.0, 2.0])\n",
    "x[0] = 10.0  # ‚ùå Error! Cannot modify\n",
    "x = x.at[0].set(10.0)  # ‚úÖ Creates new array\n",
    "```\n",
    "\n",
    "### 2. Function vs Method Style\n",
    "**PyTorch:** Object-oriented (methods on tensors)\n",
    "```python\n",
    "loss = ((y_pred - y_true) ** 2).mean()\n",
    "loss.backward()  # Method call\n",
    "```\n",
    "\n",
    "**JAX:** Functional (functions that transform functions)\n",
    "```python\n",
    "loss_fn = lambda params: ((predict(params, x) - y) ** 2).mean()\n",
    "grad_fn = jax.grad(loss_fn)  # Function transformation\n",
    "grads = grad_fn(params)\n",
    "```\n",
    "\n",
    "### 3. Neural Network Definition\n",
    "**PyTorch:** Class-based with `nn.Module`\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "```\n",
    "\n",
    "**JAX:** Function-based (or use Flax/Haiku)\n",
    "```python\n",
    "def mlp(params, x):\n",
    "    x = jnp.dot(x, params['w1']) + params['b1']\n",
    "    return x\n",
    "```\n",
    "\n",
    "## üéì What's in This Notebook?\n",
    "\n",
    "This notebook has **10 side-by-side comparisons**:\n",
    "\n",
    "1. **Array creation** - Making tensors/arrays\n",
    "2. **Basic operations** - Math, indexing, reshaping\n",
    "3. **Random numbers** - Random generation differences\n",
    "4. **Gradients** - Computing derivatives\n",
    "5. **Simple functions** - Forward + backward pass\n",
    "6. **Loss functions** - MSE, cross-entropy\n",
    "7. **Model definition** - Linear layer, MLP\n",
    "8. **Optimization step** - SGD update\n",
    "9. **Full training loop** - Complete training example\n",
    "10. **Performance** - Speed comparison\n",
    "\n",
    "Each example shows **BOTH frameworks side-by-side** so you can see the translation!\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before starting this notebook, you should:\n",
    "- ‚úÖ Complete Notebooks 1-3 (JAX Basics, JIT, Autodiff)\n",
    "- ‚úÖ Basic Python knowledge\n",
    "- ‚ùå **Don't need**: Prior PyTorch experience (we explain everything!)\n",
    "\n",
    "## üí° Which Framework Should You Choose?\n",
    "\n",
    "**Choose JAX if you:**\n",
    "- Want fine-grained control over everything\n",
    "- Need advanced transformations (vmap, pmap)\n",
    "- Prefer functional programming style\n",
    "- Are doing research with custom operations\n",
    "- Want NumPy-like code that's GPU-ready\n",
    "\n",
    "**Choose PyTorch if you:**\n",
    "- Want a complete ecosystem (models, datasets, utilities)\n",
    "- Prefer object-oriented style\n",
    "- Need extensive community support\n",
    "- Want to use pre-trained models (torchvision, transformers)\n",
    "- Are building production applications quickly\n",
    "\n",
    "**Good news:** After this notebook, you'll understand both! üéâ\n",
    "\n",
    "## üîÑ Key Takeaway\n",
    "\n",
    "**JAX and PyTorch solve the same problems with different philosophies:**\n",
    "- PyTorch: \"Here's a complete framework\"\n",
    "- JAX: \"Here's NumPy + composable transformations\"\n",
    "\n",
    "Both are excellent! This notebook helps you choose and translate between them.\n",
    "\n",
    "Let's compare them side by side! üî¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# JAX VS PYTORCH - SIDE-BY-SIDE FEATURE COMPARISON\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FEATURE COMPARISON: JAX vs PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. ARRAY/TENSOR CREATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£  ARRAY/TENSOR CREATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# JAX\n",
    "jax_array = jnp.array([1.0, 2.0, 3.0])\n",
    "jax_zeros = jnp.zeros((3, 3))\n",
    "jax_random = jax.random.normal(jax.random.PRNGKey(0), (3, 3))\n",
    "\n",
    "# PyTorch\n",
    "torch_tensor = torch.tensor([1.0, 2.0, 3.0])\n",
    "torch_zeros = torch.zeros((3, 3))\n",
    "torch_random = torch.randn(3, 3)\n",
    "\n",
    "print(\"JAX:\")\n",
    "print(f\"  Array: {jax_array}, type: {type(jax_array)}\")\n",
    "print(f\"  Random array shape: {jax_random.shape}\")\n",
    "print(\"\\nPyTorch:\")\n",
    "print(f\"  Tensor: {torch_tensor}, type: {type(torch_tensor)}\")\n",
    "print(f\"  Random tensor shape: {torch_random.shape}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Explicit random keys (no global state)\")\n",
    "print(\"  PyTorch: Global random state (torch.manual_seed)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. MUTABILITY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£  MUTABILITY\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# PyTorch - Mutable\n",
    "torch_x = torch.tensor([1.0, 2.0, 3.0])\n",
    "torch_x[0] = 10.0\n",
    "print(f\"PyTorch (mutable): {torch_x}\")\n",
    "\n",
    "# JAX - Immutable\n",
    "jax_x = jnp.array([1.0, 2.0, 3.0])\n",
    "try:\n",
    "    jax_x[0] = 10.0  # This will fail\n",
    "except TypeError as e:\n",
    "    print(f\"JAX (immutable): Error - {e}\")\n",
    "    jax_x_updated = jax_x.at[0].set(10.0)\n",
    "    print(f\"JAX update method: {jax_x_updated}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Arrays are immutable (functional style)\")\n",
    "print(\"  PyTorch: Tensors are mutable (in-place operations)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. AUTOMATIC DIFFERENTIATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£  AUTOMATIC DIFFERENTIATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def simple_func(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "# JAX approach - functional\n",
    "jax_grad_fn = jax.grad(lambda x: simple_func(x))\n",
    "x = 3.0\n",
    "jax_gradient = jax_grad_fn(x)\n",
    "\n",
    "# PyTorch approach - tensor-based\n",
    "torch_x = torch.tensor(3.0, requires_grad=True)\n",
    "torch_output = simple_func(torch_x)\n",
    "torch_output.backward()\n",
    "torch_gradient = torch_x.grad\n",
    "\n",
    "print(f\"Function: f(x) = x¬≤ + 2x + 1\")\n",
    "print(f\"At x = {x}:\")\n",
    "print(f\"  JAX gradient: {jax_gradient}\")\n",
    "print(f\"  PyTorch gradient: {torch_gradient.item()}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Transform functions with grad()\")\n",
    "print(\"  PyTorch: Call .backward() on tensors\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. JIT COMPILATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£  JIT COMPILATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def compute(x):\n",
    "    for _ in range(10):\n",
    "        x = jnp.sin(x) + jnp.cos(x)\n",
    "    return x\n",
    "\n",
    "# JAX JIT\n",
    "jax_jit_fn = jax.jit(compute)\n",
    "\n",
    "# PyTorch JIT (TorchScript)\n",
    "@torch.jit.script\n",
    "def torch_compute(x):\n",
    "    for _ in range(10):\n",
    "        x = torch.sin(x) + torch.cos(x)\n",
    "    return x\n",
    "\n",
    "# Benchmark\n",
    "test_size = 10000\n",
    "jax_input = jnp.ones(test_size)\n",
    "torch_input = torch.ones(test_size)\n",
    "\n",
    "# Warm up\n",
    "_ = jax_jit_fn(jax_input).block_until_ready()\n",
    "_ = torch_compute(torch_input)\n",
    "\n",
    "# Time JAX\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = jax_jit_fn(jax_input).block_until_ready()\n",
    "jax_time = time.time() - start\n",
    "\n",
    "# Time PyTorch\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = torch_compute(torch_input)\n",
    "torch_time = time.time() - start\n",
    "\n",
    "print(f\"Array size: {test_size}, 100 iterations\")\n",
    "print(f\"  JAX JIT:     {jax_time:.4f}s\")\n",
    "print(f\"  PyTorch JIT: {torch_time:.4f}s\")\n",
    "print(f\"  Ratio: {torch_time/jax_time:.2f}x\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Built-in with @jax.jit decorator\")\n",
    "print(\"  PyTorch: TorchScript with @torch.jit.script\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. BATCHING/VECTORIZATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£  BATCHING/VECTORIZATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def single_example_fn(x):\n",
    "    \"\"\"Function designed for single input.\"\"\"\n",
    "    return x ** 2 + 2 * x\n",
    "\n",
    "# JAX - Automatic with vmap\n",
    "jax_batched = jax.vmap(single_example_fn)\n",
    "jax_batch = jnp.array([1.0, 2.0, 3.0, 4.0])\n",
    "jax_result = jax_batched(jax_batch)\n",
    "\n",
    "# PyTorch - Manual batching or broadcasting\n",
    "torch_batch = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "torch_result = single_example_fn(torch_batch)  # Works due to broadcasting\n",
    "\n",
    "print(f\"Batch: {jax_batch}\")\n",
    "print(f\"  JAX vmap result: {jax_result}\")\n",
    "print(f\"  PyTorch result:  {torch_result}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: vmap for automatic batching\")\n",
    "print(\"  PyTorch: Broadcasting handles most cases\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. GRADIENT OF BATCHED OPERATIONS (Per-Sample Gradients)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£  PER-SAMPLE GRADIENTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss_fn(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# JAX - Easy with vmap(grad())\n",
    "jax_per_sample_grad = jax.vmap(jax.grad(loss_fn))\n",
    "jax_batch_input = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "jax_grads = jax_per_sample_grad(jax_batch_input)\n",
    "\n",
    "# PyTorch - More complex, need functorch or manual loop\n",
    "torch_batch_input = torch.tensor([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], requires_grad=True)\n",
    "torch_grads = []\n",
    "for i in range(len(torch_batch_input)):\n",
    "    loss = torch.sum(torch_batch_input[i] ** 2)\n",
    "    grad = torch.autograd.grad(loss, torch_batch_input, retain_graph=True)[0][i]\n",
    "    torch_grads.append(grad)\n",
    "torch_grads = torch.stack(torch_grads)\n",
    "\n",
    "print(f\"Batch of 3 samples, 2 features each\")\n",
    "print(f\"JAX per-sample gradients:\\n{jax_grads}\")\n",
    "print(f\"PyTorch per-sample gradients:\\n{torch_grads}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: vmap(grad()) is natural and fast\")\n",
    "print(\"  PyTorch: Requires functorch or manual loops\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 7. RANDOM NUMBER GENERATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£  RANDOM NUMBER GENERATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# JAX - Explicit keys (functional)\n",
    "key = jax.random.PRNGKey(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "jax_rand1 = jax.random.normal(subkey, (3,))\n",
    "key, subkey = jax.random.split(key)\n",
    "jax_rand2 = jax.random.normal(subkey, (3,))\n",
    "\n",
    "# PyTorch - Global state\n",
    "torch.manual_seed(42)\n",
    "torch_rand1 = torch.randn(3)\n",
    "torch_rand2 = torch.randn(3)\n",
    "\n",
    "print(\"JAX (explicit keys):\")\n",
    "print(f\"  Random 1: {jax_rand1}\")\n",
    "print(f\"  Random 2: {jax_rand2}\")\n",
    "print(\"\\nPyTorch (global state):\")\n",
    "print(f\"  Random 1: {torch_rand1}\")\n",
    "print(f\"  Random 2: {torch_rand2}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Functional RNG with explicit key splitting\")\n",
    "print(\"  PyTorch: Global RNG state (easier but less reproducible)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 8. HARDWARE ACCELERATION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n8Ô∏è‚É£  HARDWARE ACCELERATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "print(\"JAX:\")\n",
    "print(f\"  Default backend: {jax.default_backend()}\")\n",
    "print(f\"  Available devices: {jax.devices()}\")\n",
    "\n",
    "print(\"\\nPyTorch:\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(f\"  Device: CPU\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Automatic device placement, explicit device control\")\n",
    "print(\"  PyTorch: Manual .to(device) for GPU placement\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 9. COMPOSABILITY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n9Ô∏è‚É£  COMPOSABILITY OF TRANSFORMATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def f(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# JAX - Compose transformations freely\n",
    "composed = jax.jit(jax.vmap(jax.grad(f)))\n",
    "batch = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "result = composed(batch)\n",
    "\n",
    "print(\"JAX: jax.jit(jax.vmap(jax.grad(f)))\")\n",
    "print(f\"  Result shape: {result.shape}\")\n",
    "print(f\"  Result:\\n{result}\")\n",
    "\n",
    "print(\"\\nüîë Key Difference:\")\n",
    "print(\"  JAX: Free composition of jit, grad, vmap\")\n",
    "print(\"  PyTorch: Less flexible transformation composition\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY TABLE\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY: JAX vs PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary = \"\"\"\n",
    "Feature                 | JAX                      | PyTorch\n",
    "------------------------|--------------------------|---------------------------\n",
    "Paradigm                | Functional               | Object-oriented\n",
    "Mutability              | Immutable arrays         | Mutable tensors\n",
    "Autodiff API            | grad(function)           | tensor.backward()\n",
    "JIT Compilation         | @jax.jit (XLA)           | @torch.jit.script\n",
    "Batching                | vmap (automatic)         | Broadcasting (manual)\n",
    "Per-sample gradients    | vmap(grad()) - easy      | Loops or functorch\n",
    "Random numbers          | Explicit keys            | Global state\n",
    "Composability           | High (jit+grad+vmap)     | Moderate\n",
    "Learning curve          | Steeper (functional)     | Gentler (imperative)\n",
    "Ecosystem               | Growing (Flax, Optax)    | Mature (torchvision, etc)\n",
    "Best for                | Research, custom algos   | Production, quick start\n",
    "\"\"\"\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
