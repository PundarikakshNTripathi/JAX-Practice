{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45e1fc0",
   "metadata": {},
   "source": [
    "# üìò Notebook 3: Automatic Differentiation - Computing Gradients Like Magic\n",
    "\n",
    "Welcome to the \"calculus\" chapter of JAX! Don't worry - you don't need to remember your calculus classes. JAX does the math for you!\n",
    "\n",
    "## üéØ What You'll Learn (30-40 minutes)\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "- ‚úÖ What automatic differentiation is (and why it's amazing)\n",
    "- ‚úÖ How to compute gradients with `jax.grad()`\n",
    "- ‚úÖ Multi-variable functions and partial derivatives\n",
    "- ‚úÖ Getting both function value and gradient together\n",
    "- ‚úÖ Jacobian and Hessian matrices (we'll explain these!)\n",
    "- ‚úÖ Higher-order derivatives (gradients of gradients)\n",
    "- ‚úÖ Practical application: Gradient descent\n",
    "\n",
    "## ü§î What is Automatic Differentiation?\n",
    "\n",
    "### The Problem: Manual Calculus is Hard\n",
    "Machine learning requires computing **derivatives** (gradients) of complex functions. Doing this by hand is:\n",
    "- Time-consuming \n",
    "- Error-prone\n",
    "- Tedious\n",
    "- Nearly impossible for neural networks with millions of parameters\n",
    "\n",
    "### The Solution: Automatic Differentiation (Autodiff)\n",
    "JAX **automatically** computes exact derivatives for you!\n",
    "\n",
    "```python\n",
    "# You write the FORWARD function:\n",
    "def f(x):\n",
    "    return x ** 2 + 2 * x + 1\n",
    "\n",
    "# JAX computes the DERIVATIVE automatically:\n",
    "df_dx = jax.grad(f)\n",
    "\n",
    "print(df_dx(3.0))  # 8.0 (which is 2*3 + 2)\n",
    "```\n",
    "\n",
    "No manual calculus needed! üéâ\n",
    "\n",
    "### Why This Matters for Machine Learning\n",
    "**Training neural networks = finding gradients**\n",
    "- Forward pass: Compute predictions\n",
    "- Backward pass: Compute gradients (how to improve)\n",
    "- Update: Adjust weights using gradients\n",
    "\n",
    "JAX handles the backward pass automatically!\n",
    "\n",
    "## üìö Key Concepts (Don't Worry, We'll Explain!)\n",
    "\n",
    "### 1. Gradient (First Derivative)\n",
    "**What is it?** How much a function changes when you change its input.\n",
    "\n",
    "**Example:** If `f(x) = x¬≤`, then `f'(x) = 2x`\n",
    "- At x=3: gradient is 6 (function increases by 6 for each unit increase in x)\n",
    "\n",
    "### 2. Partial Derivatives\n",
    "**What is it?** Gradient with respect to ONE variable in a multi-variable function.\n",
    "\n",
    "**Example:** If `f(x,y) = x¬≤ + 3xy + y¬≤`\n",
    "- ‚àÇf/‚àÇx = 2x + 3y (how f changes when x changes, y fixed)\n",
    "- ‚àÇf/‚àÇy = 3x + 2y (how f changes when y changes, x fixed)\n",
    "\n",
    "### 3. Jacobian Matrix\n",
    "**What is it?** All partial derivatives of a vector-valued function.\n",
    "\n",
    "**When?** When your function returns multiple outputs.\n",
    "\n",
    "**Example:** `f(x) = [x¬≤, x¬≥, sin(x)]` ‚Üí Jacobian has 3 derivatives\n",
    "\n",
    "### 4. Hessian Matrix  \n",
    "**What is it?** Matrix of second derivatives (derivatives of derivatives).\n",
    "\n",
    "**When?** For optimization algorithms that use second-order information.\n",
    "\n",
    "**Don't worry!** JAX computes these automatically - you don't need to understand the math deeply!\n",
    "\n",
    "## üéì What's in This Notebook?\n",
    "\n",
    "This notebook has **8 comprehensive examples**:\n",
    "\n",
    "1. **Basic gradient** - Single variable functions\n",
    "2. **Multi-variable gradient** - Functions with multiple inputs\n",
    "3. **value_and_grad** - Get function value and gradient together (efficient!)\n",
    "4. **Jacobian** - Vector-valued functions\n",
    "5. **Hessian** - Second derivatives\n",
    "6. **Higher-order derivatives** - Gradients of gradients\n",
    "7. **Multiple arguments** - Gradients w.r.t. different parameters\n",
    "8. **Practical example** - Linear regression with gradient descent\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before starting this notebook, you should:\n",
    "- ‚úÖ Complete Notebook 1 (JAX Basics)\n",
    "- ‚úÖ Understand what a function is\n",
    "- ‚úÖ Know basic math (addition, multiplication, powers)\n",
    "- ‚ùå **Don't need**: Deep calculus knowledge (JAX handles it!)\n",
    "\n",
    "## üí° Key Takeaway\n",
    "\n",
    "**You write the forward function. JAX computes exact gradients automatically.**\n",
    "\n",
    "This is the secret sauce that makes modern machine learning possible!\n",
    "\n",
    "Let's see autodiff in action! üìà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20322cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING GRADIENTS WITH JAX\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  BASIC GRADIENT\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = x^2\n",
      "f(3.0) = 9.0\n",
      "f'(3.0) = 6.0\n",
      "Expected: 2*3.0 = 6.0 ‚úì\n",
      "\n",
      "2Ô∏è‚É£  MULTI-VARIABLE GRADIENT\n",
      "----------------------------------------------------------------------\n",
      "Function: g(x,y) = x^2 + 3xy + y^2\n",
      "g(2.0, 1.0) = 11.0\n",
      "‚àÇg/‚àÇx at (2.0,1.0) = 7.0\n",
      "‚àÇg/‚àÇy at (2.0,1.0) = 8.0\n",
      "Expected: ‚àÇg/‚àÇx = 2x + 3y = 7.0 ‚úì\n",
      "Expected: ‚àÇg/‚àÇy = 3x + 2y = 8.0 ‚úì\n",
      "\n",
      "3Ô∏è‚É£  VALUE AND GRADIENT TOGETHER\n",
      "----------------------------------------------------------------------\n",
      "Parameters: [1. 2. 3.]\n",
      "Loss value: 14.0\n",
      "Gradient: [2. 4. 6.]\n",
      "Expected gradient: 2*params = [2. 4. 6.] ‚úì\n",
      "\n",
      "4Ô∏è‚É£  JACOBIAN (for vector-valued functions)\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = [x^2, x^3, sin(x)]\n",
      "Jacobian at x=2.0:\n",
      "  df‚ÇÅ/dx = 2x = 4.0000\n",
      "  df‚ÇÇ/dx = 3x^2 = 12.0000\n",
      "  df‚ÇÉ/dx = cos(x) = -0.4161\n",
      "\n",
      "5Ô∏è‚É£  HESSIAN (second derivatives)\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x,y) = x^2 + y^2\n",
      "Hessian matrix at [1. 2.]:\n",
      "[[2. 0.]\n",
      " [0. 2.]]\n",
      "(Second derivatives: diagonal is 2, off-diagonal is 0)\n",
      "\n",
      "6Ô∏è‚É£  HIGHER-ORDER DERIVATIVES\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = x^3\n",
      "f(2.0) = 8.0\n",
      "f'(2.0) = 12.0 (expected: 3x^2 = 12.0)\n",
      "f''(2.0) = 12.0 (expected: 6x = 12.0)\n",
      "f'''(2.0) = 6.0 (expected: 6)\n",
      "\n",
      "7Ô∏è‚É£  GRADIENTS W.R.T. MULTIPLE ARGUMENTS\n",
      "----------------------------------------------------------------------\n",
      "Model: y = w¬∑x + b\n",
      "Gradient w.r.t. weights: [1. 1. 1.]\n",
      "Gradient w.r.t. bias: 1.0\n",
      "(dL/dw = x, dL/db = 1)\n",
      "\n",
      "8Ô∏è‚É£  GRADIENT DESCENT OPTIMIZATION\n",
      "----------------------------------------------------------------------\n",
      "Training linear regression with gradient descent...\n",
      "True parameters: slope=2.0, intercept=1.0\n",
      "Initial params: [0. 0.]\n",
      "Step   0: loss=14.2483, params=[1.4424324 0.218287 ]\n",
      "Step  20: loss=0.3895, params=[2.0481887 0.4179262]\n",
      "Step  40: loss=0.3413, params=[2.0338674 0.5151596]\n",
      "Step  60: loss=0.3023, params=[2.0209825 0.6026397]\n",
      "Step  80: loss=0.2707, params=[2.00939   0.6813449]\n",
      "\n",
      "Final params: slope=1.999, intercept=0.749\n",
      "Final loss: 0.2463\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS - AUTOMATIC DIFFERENTIATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ jax.grad() computes exact derivatives (not numerical approximations)\n",
      "‚úÖ Works with any Python/JAX function - no manual chain rule needed\n",
      "‚úÖ Efficient reverse-mode autodiff (backpropagation)\n",
      "‚úÖ Can compute higher-order derivatives by composing grad()\n",
      "‚úÖ value_and_grad() gives both function value and gradient\n",
      "‚úÖ Jacobian and Hessian for vector functions and second derivatives\n",
      "‚úÖ argnums parameter controls which arguments to differentiate\n",
      "‚úÖ Combine with JIT for ultra-fast gradient computations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AUTOMATIC DIFFERENTIATION - BASICS\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING GRADIENTS WITH JAX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 1: Basic Gradient - Single Variable\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£  BASIC GRADIENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Simple quadratic function: f(x) = x^2\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "# Create gradient function\n",
    "df_dx = jax.grad(f)\n",
    "\n",
    "# Evaluate at different points\n",
    "x = 3.0\n",
    "print(f\"Function: f(x) = x^2\")\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {df_dx(x)}\")\n",
    "print(f\"Expected: 2*{x} = {2*x} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 2: Multi-variable Function\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£  MULTI-VARIABLE GRADIENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def g(x, y):\n",
    "    \"\"\"Function with two inputs: g(x,y) = x^2 + 3xy + y^2\"\"\"\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# grad() computes gradient w.r.t. FIRST argument by default\n",
    "dg_dx = jax.grad(g)\n",
    "\n",
    "# To get gradient w.r.t. other arguments, use argnums\n",
    "dg_dy = jax.grad(g, argnums=1)\n",
    "\n",
    "x, y = 2.0, 1.0\n",
    "print(f\"Function: g(x,y) = x^2 + 3xy + y^2\")\n",
    "print(f\"g({x}, {y}) = {g(x, y)}\")\n",
    "print(f\"‚àÇg/‚àÇx at ({x},{y}) = {dg_dx(x, y)}\")\n",
    "print(f\"‚àÇg/‚àÇy at ({x},{y}) = {dg_dy(x, y)}\")\n",
    "print(f\"Expected: ‚àÇg/‚àÇx = 2x + 3y = {2*x + 3*y} ‚úì\")\n",
    "print(f\"Expected: ‚àÇg/‚àÇy = 3x + 2y = {3*x + 2*y} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 3: value_and_grad - Get Both Function Value and Gradient\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£  VALUE AND GRADIENT TOGETHER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss(params):\n",
    "    \"\"\"Typical ML loss function\"\"\"\n",
    "    return jnp.sum(params ** 2)\n",
    "\n",
    "# This is super useful for optimization - get loss and gradient in one call\n",
    "val_and_grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "value, gradient = val_and_grad_fn(params)\n",
    "\n",
    "print(f\"Parameters: {params}\")\n",
    "print(f\"Loss value: {value}\")\n",
    "print(f\"Gradient: {gradient}\")\n",
    "print(f\"Expected gradient: 2*params = {2*params} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 4: Gradients of Vector Functions - Jacobian\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£  JACOBIAN (for vector-valued functions)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def vector_func(x):\n",
    "    \"\"\"Returns a vector: [x^2, x^3, sin(x)]\"\"\"\n",
    "    return jnp.array([x**2, x**3, jnp.sin(x)])\n",
    "\n",
    "# Jacobian computes all partial derivatives\n",
    "jacobian_fn = jax.jacobian(vector_func)\n",
    "\n",
    "x = 2.0\n",
    "jac = jacobian_fn(x)\n",
    "print(f\"Function: f(x) = [x^2, x^3, sin(x)]\")\n",
    "print(f\"Jacobian at x={x}:\")\n",
    "print(f\"  df‚ÇÅ/dx = 2x = {jac[0]:.4f}\")\n",
    "print(f\"  df‚ÇÇ/dx = 3x^2 = {jac[1]:.4f}\")\n",
    "print(f\"  df‚ÇÉ/dx = cos(x) = {jac[2]:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 5: Second Derivatives - Hessian\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£  HESSIAN (second derivatives)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def h(x):\n",
    "    \"\"\"Multi-variable function for Hessian demo\"\"\"\n",
    "    return jnp.array([x[0]**2 + x[1]**2, x[0]*x[1]])\n",
    "\n",
    "# Hessian: matrix of second partial derivatives\n",
    "hessian_fn = jax.hessian(lambda x: x[0]**2 + x[1]**2)\n",
    "\n",
    "x = jnp.array([1.0, 2.0])\n",
    "hess = hessian_fn(x)\n",
    "print(f\"Function: f(x,y) = x^2 + y^2\")\n",
    "print(f\"Hessian matrix at {x}:\")\n",
    "print(hess)\n",
    "print(\"(Second derivatives: diagonal is 2, off-diagonal is 0)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 6: Higher-Order Derivatives\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£  HIGHER-ORDER DERIVATIVES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def cubic(x):\n",
    "    \"\"\"f(x) = x^3\"\"\"\n",
    "    return x ** 3\n",
    "\n",
    "# First derivative\n",
    "first_deriv = jax.grad(cubic)\n",
    "# Second derivative (gradient of gradient)\n",
    "second_deriv = jax.grad(first_deriv)\n",
    "# Third derivative\n",
    "third_deriv = jax.grad(second_deriv)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"Function: f(x) = x^3\")\n",
    "print(f\"f({x}) = {cubic(x)}\")\n",
    "print(f\"f'({x}) = {first_deriv(x)} (expected: 3x^2 = {3*x**2})\")\n",
    "print(f\"f''({x}) = {second_deriv(x)} (expected: 6x = {6*x})\")\n",
    "print(f\"f'''({x}) = {third_deriv(x)} (expected: 6)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 7: Gradient with Respect to Multiple Arguments\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£  GRADIENTS W.R.T. MULTIPLE ARGUMENTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def model(weights, bias, x):\n",
    "    \"\"\"Simple linear model: y = w*x + b\"\"\"\n",
    "    return jnp.dot(weights, x) + bias\n",
    "\n",
    "# Get gradients w.r.t. first two arguments (weights and bias)\n",
    "grad_fn = jax.grad(model, argnums=(0, 1))\n",
    "\n",
    "w = jnp.array([1.0, 2.0, 3.0])\n",
    "b = 0.5\n",
    "x = jnp.array([1.0, 1.0, 1.0])\n",
    "\n",
    "grad_w, grad_b = grad_fn(w, b, x)\n",
    "print(f\"Model: y = w¬∑x + b\")\n",
    "print(f\"Gradient w.r.t. weights: {grad_w}\")\n",
    "print(f\"Gradient w.r.t. bias: {grad_b}\")\n",
    "print(f\"(dL/dw = x, dL/db = 1)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 8: Practical Example - Gradient Descent\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n8Ô∏è‚É£  GRADIENT DESCENT OPTIMIZATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"MSE loss for linear regression\"\"\"\n",
    "    prediction = params[0] * x + params[1]\n",
    "    return jnp.mean((prediction - y) ** 2)\n",
    "\n",
    "# Training data: y = 2x + 1 with some noise\n",
    "np.random.seed(42)\n",
    "x_data = jnp.array(np.linspace(0, 10, 20))\n",
    "y_data = 2 * x_data + 1 + np.random.randn(20) * 0.5\n",
    "\n",
    "# Initialize parameters\n",
    "params = jnp.array([0.0, 0.0])  # [slope, intercept]\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "print(\"Training linear regression with gradient descent...\")\n",
    "print(f\"True parameters: slope=2.0, intercept=1.0\")\n",
    "print(f\"Initial params: {params}\")\n",
    "\n",
    "for step in range(100):\n",
    "    grads = grad_fn(params, x_data, y_data)\n",
    "    params = params - learning_rate * grads\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        loss = loss_fn(params, x_data, y_data)\n",
    "        print(f\"Step {step:3d}: loss={loss:.4f}, params={params}\")\n",
    "\n",
    "final_loss = loss_fn(params, x_data, y_data)\n",
    "print(f\"\\nFinal params: slope={params[0]:.3f}, intercept={params[1]:.3f}\")\n",
    "print(f\"Final loss: {final_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY POINTS - AUTOMATIC DIFFERENTIATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ jax.grad() computes exact derivatives (not numerical approximations)\n",
    "‚úÖ Works with any Python/JAX function - no manual chain rule needed\n",
    "‚úÖ Efficient reverse-mode autodiff (backpropagation)\n",
    "‚úÖ Can compute higher-order derivatives by composing grad()\n",
    "‚úÖ value_and_grad() gives both function value and gradient\n",
    "‚úÖ Jacobian and Hessian for vector functions and second derivatives\n",
    "‚úÖ argnums parameter controls which arguments to differentiate\n",
    "‚úÖ Combine with JIT for ultra-fast gradient computations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76120407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
