{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fa76044",
   "metadata": {},
   "source": [
    "# üìò Notebook 6: Neural Networks & Fraud Detection - Putting It All Together\n",
    "\n",
    "Welcome to the grand finale! This notebook brings together **everything you've learned** to build a real fraud detection system from scratch.\n",
    "\n",
    "## üéØ What You'll Learn (40-50 minutes)\n",
    "\n",
    "By the end of this notebook, you'll have built:\n",
    "- ‚úÖ A complete neural network classifier in JAX\n",
    "- ‚úÖ A real-world fraud detection model\n",
    "- ‚úÖ End-to-end training pipeline\n",
    "- ‚úÖ Performance evaluation and metrics\n",
    "- ‚úÖ Model interpretation and analysis\n",
    "- ‚úÖ Practical deployment considerations\n",
    "\n",
    "**This is where theory meets practice!** üöÄ\n",
    "\n",
    "## ü§î What is Fraud Detection?\n",
    "\n",
    "### The Real-World Problem\n",
    "Credit card companies process millions of transactions daily. A tiny fraction (<0.2%) are fraudulent, but catching them is critical:\n",
    "\n",
    "**Challenges:**\n",
    "- **Extreme imbalance:** 99.8% legitimate, 0.2% fraud\n",
    "- **High stakes:** Miss fraud = $$ lost; False alarm = angry customer\n",
    "- **Real-time:** Must decide in milliseconds\n",
    "- **Evolving patterns:** Fraudsters constantly adapt\n",
    "\n",
    "**Your goal:** Build a neural network that identifies fraudulent transactions!\n",
    "\n",
    "### The Dataset: Credit Card Fraud Detection\n",
    "**Source:** Real credit card transactions from European cardholders (anonymized)\n",
    "\n",
    "**Size:** 284,807 transactions over 2 days\n",
    "\n",
    "**Features:**\n",
    "- `Time`: Seconds since first transaction\n",
    "- `V1-V28`: Anonymized features (PCA transformed for privacy)\n",
    "- `Amount`: Transaction amount\n",
    "- `Class`: 0 = Legitimate, 1 = Fraud\n",
    "\n",
    "**Why this dataset?**\n",
    "- Real-world imbalanced classification problem\n",
    "- Demonstrates practical ML challenges\n",
    "- Commonly used benchmark\n",
    "\n",
    "## üß† Neural Network Architecture\n",
    "\n",
    "### What You'll Build\n",
    "A **Multi-Layer Perceptron (MLP)** with:\n",
    "- **Input layer:** Features from the dataset (V1-V28 + Time + Amount, typically 29-30 features)\n",
    "- **Hidden layer 1:** 64 neurons + ReLU activation\n",
    "- **Hidden layer 2:** 32 neurons + ReLU activation\n",
    "- **Hidden layer 3:** 16 neurons + ReLU activation\n",
    "- **Output layer:** 1 neuron + Sigmoid activation (probability of fraud)\n",
    "\n",
    "### Why This Architecture?\n",
    "- **Not too complex:** Small dataset (284K samples) doesn't need huge network\n",
    "- **Enough capacity:** 3 hidden layers can learn complex patterns\n",
    "- **Fast training:** Small enough to train on CPU in minutes\n",
    "- **Proven effective:** This architecture works well for tabular data\n",
    "\n",
    "### Architecture Diagram\n",
    "```\n",
    "Input (dynamic) ‚Üí Dense(64) + ReLU ‚Üí Dense(32) + ReLU ‚Üí Dense(16) + ReLU ‚Üí Dense(1) + Sigmoid ‚Üí Fraud Probability\n",
    "```\n",
    "\n",
    "## üìö Key Concepts for Beginners\n",
    "\n",
    "### 1. What is a Neural Network?\n",
    "**Simple answer:** A function that learns patterns from data!\n",
    "\n",
    "**How it works:**\n",
    "1. Takes input features (transaction data)\n",
    "2. Multiplies by weights and adds biases (learned parameters)\n",
    "3. Applies activation functions (introduces non-linearity)\n",
    "4. Produces output (fraud probability)\n",
    "\n",
    "**Learning = adjusting weights to minimize errors**\n",
    "\n",
    "### 2. Activation Functions\n",
    "\n",
    "**ReLU (Rectified Linear Unit):**\n",
    "- Formula: `max(0, x)`\n",
    "- Purpose: Introduces non-linearity (lets network learn complex patterns)\n",
    "- Why: Simple, fast, works well\n",
    "\n",
    "**Sigmoid:**\n",
    "- Formula: `1 / (1 + e^(-x))`\n",
    "- Purpose: Squashes output to [0, 1] range\n",
    "- Why: Perfect for probabilities!\n",
    "\n",
    "### 3. Loss Function: Binary Cross-Entropy\n",
    "**What:** Measures how wrong the model's predictions are\n",
    "\n",
    "**Formula:** `-[y*log(p) + (1-y)*log(1-p)]`\n",
    "- `y`: True label (0 or 1)\n",
    "- `p`: Predicted probability\n",
    "\n",
    "**Why:** Penalizes confident wrong predictions heavily\n",
    "\n",
    "### 4. Optimizer: Stochastic Gradient Descent (SGD)\n",
    "**What:** Algorithm that updates weights to minimize loss\n",
    "\n",
    "**How:**\n",
    "1. Compute gradient (how to change weights to reduce loss)\n",
    "2. Update: `weight = weight - learning_rate * gradient`\n",
    "3. Repeat until loss stops decreasing\n",
    "\n",
    "**Learning rate:** Step size (too big = unstable, too small = slow)\n",
    "\n",
    "### 5. Metrics for Imbalanced Data\n",
    "\n",
    "**Accuracy is misleading!**\n",
    "- If 99.8% are legitimate, predicting \"all legitimate\" gives 99.8% accuracy\n",
    "- But catches ZERO fraud!\n",
    "\n",
    "**Better metrics explained:**\n",
    "\n",
    "#### **Confusion Matrix - The Foundation**\n",
    "Every prediction falls into one of four categories:\n",
    "```\n",
    "                    Predicted: Fraud    Predicted: Legitimate\n",
    "Actual: Fraud       TP (True Positive)  FN (False Negative)\n",
    "Actual: Legitimate  FP (False Positive) TN (True Negative)\n",
    "```\n",
    "- **TP (True Positive):** Correctly caught fraud ‚úÖ (Good!)\n",
    "- **TN (True Negative):** Correctly identified legitimate ‚úÖ (Good!)\n",
    "- **FP (False Positive):** Flagged legitimate as fraud ‚ùå (Annoying customer)\n",
    "- **FN (False Negative):** Missed actual fraud ‚ùå (Lost money!)\n",
    "\n",
    "#### **Precision - \"How accurate are our fraud alerts?\"**\n",
    "**Formula:** `Precision = TP / (TP + FP)`\n",
    "\n",
    "**What it means:** Of all transactions we flagged as fraud, what percentage were actually fraud?\n",
    "\n",
    "**Example:** \n",
    "- Flagged 100 transactions as fraud (our predictions)\n",
    "- Only 80 were actually fraud (true positives)\n",
    "- 20 were legitimate (false positives - we annoyed 20 customers!)\n",
    "- Precision = 80/100 = 0.80 or 80%\n",
    "\n",
    "**Interpretation:**\n",
    "- **High precision (close to 1.0):** Few false alarms, customers rarely get bothered\n",
    "- **Low precision (close to 0.0):** Many false alarms, customers get angry\n",
    "- **Target:** Usually want >0.70 (70%) in fraud detection\n",
    "\n",
    "#### **Recall (Sensitivity) - \"How many frauds did we catch?\"**\n",
    "**Formula:** `Recall = TP / (TP + FN)`\n",
    "\n",
    "**What it means:** Of all actual fraud cases, what percentage did we successfully catch?\n",
    "\n",
    "**Example:**\n",
    "- 100 actual fraud transactions happened (reality)\n",
    "- We caught 90 of them (true positives)\n",
    "- We missed 10 (false negatives - lost money!)\n",
    "- Recall = 90/100 = 0.90 or 90%\n",
    "\n",
    "**Interpretation:**\n",
    "- **High recall (close to 1.0):** Catching most frauds, minimizing losses\n",
    "- **Low recall (close to 0.0):** Missing many frauds, big financial loss!\n",
    "- **Target:** Usually want >0.80 (80%) in fraud detection\n",
    "\n",
    "#### **The Precision-Recall Trade-off**\n",
    "**The dilemma:** You can't maximize both simultaneously!\n",
    "\n",
    "**Make model more sensitive (predict fraud more often):**\n",
    "- ‚Üë Recall increases (catch more frauds) ‚úÖ\n",
    "- ‚Üì Precision decreases (more false alarms) ‚ùå\n",
    "\n",
    "**Make model more conservative (predict fraud less often):**\n",
    "- ‚Üë Precision increases (fewer false alarms) ‚úÖ\n",
    "- ‚Üì Recall decreases (miss more frauds) ‚ùå\n",
    "\n",
    "**Real-world decision:**\n",
    "- Banks often prefer **higher recall** (catch frauds, even with false alarms)\n",
    "- Why? Losing $1000 to fraud >> annoying one customer with a call\n",
    "\n",
    "#### **F1-Score - \"Overall balance of precision and recall\"**\n",
    "**Formula:** `F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)`\n",
    "\n",
    "**What it means:** Harmonic mean that balances precision and recall. Only high if BOTH are high!\n",
    "\n",
    "**Example scenarios:**\n",
    "- Precision=0.90, Recall=0.90 ‚Üí F1=0.90 (Excellent! ‚≠ê)\n",
    "- Precision=0.95, Recall=0.50 ‚Üí F1=0.66 (Unbalanced)\n",
    "- Precision=0.50, Recall=0.95 ‚Üí F1=0.66 (Unbalanced)\n",
    "- Precision=1.00, Recall=0.10 ‚Üí F1=0.18 (Terrible!)\n",
    "\n",
    "**Interpretation:**\n",
    "- **F1 > 0.80:** Excellent model for imbalanced data\n",
    "- **F1 = 0.60-0.80:** Good, room for improvement\n",
    "- **F1 < 0.60:** Poor, needs significant work\n",
    "- **Why use it:** Single metric that punishes extreme imbalance\n",
    "\n",
    "#### **PR-AUC (Precision-Recall Area Under Curve)**\n",
    "**Formula:** Area under the Precision-Recall curve across all thresholds\n",
    "\n",
    "**What it means:** How well the model performs across ALL possible decision thresholds (0.1, 0.5, 0.9, etc.)\n",
    "\n",
    "**Threshold concept:**\n",
    "- Model outputs probability: 0.83 = \"83% chance of fraud\"\n",
    "- We pick threshold (e.g., 0.5): if prob ‚â• 0.5, predict fraud\n",
    "- Different thresholds give different precision/recall trade-offs\n",
    "\n",
    "**Example:**\n",
    "- Threshold=0.9 (very strict): High precision, low recall (few predictions)\n",
    "- Threshold=0.3 (very lenient): Low precision, high recall (many predictions)\n",
    "\n",
    "**Interpretation:**\n",
    "- **PR-AUC = 1.0:** Perfect model (impossible in practice)\n",
    "- **PR-AUC > 0.80:** Excellent performance\n",
    "- **PR-AUC = 0.40-0.80:** Decent to good\n",
    "- **PR-AUC < 0.40:** Poor, barely better than random\n",
    "- **Baseline:** Random guessing = fraud prevalence rate (0.002 for this dataset)\n",
    "\n",
    "**Why use PR-AUC:** Best metric for imbalanced data! Better than ROC-AUC because it focuses on the minority class (fraud).\n",
    "\n",
    "#### **ROC-AUC (Receiver Operating Characteristic)**\n",
    "**Formula:** Area under the ROC curve (True Positive Rate vs False Positive Rate)\n",
    "\n",
    "**What it means:** Model's ability to distinguish between classes across all thresholds\n",
    "\n",
    "**Components:**\n",
    "- True Positive Rate (TPR) = Recall = TP/(TP+FN)\n",
    "- False Positive Rate (FPR) = FP/(FP+TN)\n",
    "\n",
    "**Interpretation:**\n",
    "- **ROC-AUC = 1.0:** Perfect discrimination\n",
    "- **ROC-AUC > 0.90:** Excellent\n",
    "- **ROC-AUC = 0.70-0.90:** Good\n",
    "- **ROC-AUC = 0.50:** Random guessing (useless!)\n",
    "- **ROC-AUC < 0.50:** Worse than random (model is backwards!)\n",
    "\n",
    "**Caveat for imbalanced data:** \n",
    "- ROC-AUC can be misleading with severe imbalance (like 577:1)\n",
    "- May look good even when model performs poorly on minority class\n",
    "- **Prefer PR-AUC for this fraud dataset!**\n",
    "\n",
    "#### **Quick Decision Guide:**\n",
    "```\n",
    "Question                          ‚Üí Metric to Check\n",
    "---------------------------       ‚Üí -----------------\n",
    "\"Are fraud alerts accurate?\"      ‚Üí Precision\n",
    "\"Are we catching most frauds?\"    ‚Üí Recall  \n",
    "\"Overall balance of both?\"        ‚Üí F1-Score\n",
    "\"Performance across thresholds?\"  ‚Üí PR-AUC (best for imbalance)\n",
    "\"General discrimination ability?\" ‚Üí ROC-AUC\n",
    "```\n",
    "\n",
    "#### **What's \"Good\" for Fraud Detection?**\n",
    "Based on industry standards:\n",
    "- **Precision:** 0.70-0.90 (70-90%)\n",
    "- **Recall:** 0.75-0.95 (75-95%)\n",
    "- **F1-Score:** 0.70-0.85 (70-85%)\n",
    "- **PR-AUC:** 0.60-0.90 (60-90%)\n",
    "- **ROC-AUC:** 0.85-0.98 (85-98%)\n",
    "\n",
    "**Remember:** There's always a trade-off! The \"best\" model depends on business priorities (lose money vs. annoy customers).\n",
    "\n",
    "## üéì What's in This Notebook?\n",
    "\n",
    "This comprehensive notebook includes:\n",
    "\n",
    "1. **Data Loading & Exploration**\n",
    "   - Load credit card fraud dataset\n",
    "   - Understand data distribution and imbalance\n",
    "   - Visualize key patterns\n",
    "\n",
    "2. **Data Preprocessing**\n",
    "   - Normalization (scale features to same range)\n",
    "   - Train/test split (evaluate on unseen data)\n",
    "   - Batch preparation using Polars\n",
    "\n",
    "3. **Model Definition**\n",
    "   - Neural network architecture in pure JAX\n",
    "   - Weight initialization\n",
    "   - Forward pass implementation\n",
    "\n",
    "4. **Training Pipeline**\n",
    "   - Loss function with binary cross-entropy\n",
    "   - Gradient computation using `jax.grad`\n",
    "   - Optimization step with SGD\n",
    "   - Full training loop with `jit` and `vmap`\n",
    "\n",
    "5. **Evaluation**\n",
    "   - Compute predictions on test set\n",
    "   - Calculate precision, recall, F1, ROC-AUC\n",
    "   - Confusion matrix\n",
    "   - Identify optimal threshold\n",
    "\n",
    "6. **Analysis & Insights**\n",
    "   - Feature importance\n",
    "   - Error analysis (false positives/negatives)\n",
    "   - Model interpretation\n",
    "   - Deployment considerations\n",
    "\n",
    "## üöÄ Prerequisites\n",
    "\n",
    "Before starting this notebook, you should:\n",
    "- ‚úÖ Complete Notebooks 1-4 (JAX Basics through vmap)\n",
    "- ‚úÖ Understand what a neural network is (conceptually)\n",
    "- ‚úÖ Know basic Python and NumPy\n",
    "- ‚ùå **Don't need**: Deep learning expertise (we build everything from scratch!)\n",
    "\n",
    "## üèÜ JAX Transformations in Action\n",
    "\n",
    "This notebook showcases **all JAX superpowers together:**\n",
    "\n",
    "| Transformation | Purpose in This Project |\n",
    "|----------------|-------------------------|\n",
    "| `jit` | 10-100x faster training |\n",
    "| `grad` | Automatic gradient computation |\n",
    "| `vmap` | Batch processing (no loops!) |\n",
    "| Functional style | Clean, composable code |\n",
    "\n",
    "**This is JAX at its best!** ‚ö°\n",
    "\n",
    "## üí° Key Takeaway\n",
    "\n",
    "**You're building a complete ML system:**\n",
    "- Data ‚Üí Preprocessing ‚Üí Model ‚Üí Training ‚Üí Evaluation ‚Üí Insights\n",
    "\n",
    "**Using only JAX + basic libraries** - no high-level frameworks!\n",
    "\n",
    "This shows you how everything works under the hood. üîç\n",
    "\n",
    "## üéØ Learning Outcomes\n",
    "\n",
    "After completing this notebook, you'll be able to:\n",
    "- ‚úÖ Build neural networks from scratch in JAX\n",
    "- ‚úÖ Handle imbalanced datasets\n",
    "- ‚úÖ Train models efficiently with JAX transformations\n",
    "- ‚úÖ Evaluate models with appropriate metrics\n",
    "- ‚úÖ Apply ML to real-world problems\n",
    "\n",
    "**You'll have a complete, working fraud detection system!** üéâ\n",
    "\n",
    "Let's build something real! üí≥üõ°Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632de86",
   "metadata": {},
   "source": [
    "## üìö Part 1: Understanding Data Handling\n",
    "\n",
    "### What is Data Preprocessing?\n",
    "\n",
    "**Think of it like cooking:** Before you cook a meal, you need to prep ingredients - wash vegetables, cut meat, measure spices. Similarly, before training a neural network, you need to prep your data!\n",
    "\n",
    "**Raw data ‚Üí Preprocessed data ‚Üí Model training ‚Üí Predictions**\n",
    "\n",
    "### Why Can't We Use Raw Data?\n",
    "\n",
    "**Problem 1: Different Scales**\n",
    "- Feature 1 (Transaction Amount): ranges from $0 to $10,000\n",
    "- Feature 2 (Time): ranges from 0 to 172,792 seconds\n",
    "- Neural networks struggle when features have vastly different scales!\n",
    "\n",
    "**Solution:** Standardization (make all features have similar ranges)\n",
    "\n",
    "**Problem 2: Data Leakage**\n",
    "- If we train and test on the same data, model just memorizes!\n",
    "- Like studying with the exact test questions - cheating!\n",
    "\n",
    "**Solution:** Split data into train/validation/test sets\n",
    "\n",
    "**Problem 3: Class Imbalance**\n",
    "- 99.8% legitimate, 0.2% fraud\n",
    "- Model learns to always predict \"legitimate\" and gets 99.8% accuracy!\n",
    "- But catches zero fraud!\n",
    "\n",
    "**Solution:** Class weights (penalize errors on rare class more)\n",
    "\n",
    "---\n",
    "\n",
    "### The Data Pipeline (Step-by-Step)\n",
    "\n",
    "#### **Step 1: Load Raw Data**\n",
    "```python\n",
    "data = fetch_openml('creditcard')  # Download dataset\n",
    "```\n",
    "**What happens:** Downloads 284,807 credit card transactions with 30 features\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Separate Features and Labels**\n",
    "```python\n",
    "X = features (V1, V2, ..., V28, Time, Amount)\n",
    "y = labels (0=legitimate, 1=fraud)\n",
    "```\n",
    "**Why:** Model learns from X (inputs) to predict y (outputs)\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Train/Validation/Test Split**\n",
    "\n",
    "**The Three Sets:**\n",
    "\n",
    "1. **Training Set (70%):** Model learns from this\n",
    "   - Like practice problems when studying\n",
    "   - Model adjusts weights to minimize errors here\n",
    "   - Example: 199,365 transactions\n",
    "\n",
    "2. **Validation Set (15%):** Tune hyperparameters\n",
    "   - Like practice exams before the real test\n",
    "   - Check if model is overfitting (memorizing instead of learning)\n",
    "   - Helps decide when to stop training\n",
    "   - Example: 42,721 transactions\n",
    "\n",
    "3. **Test Set (15%):** Final evaluation (touch ONCE at the end!)\n",
    "   - Like the real exam\n",
    "   - Model has never seen this data during training\n",
    "   - Gives honest assessment of real-world performance\n",
    "   - Example: 42,721 transactions\n",
    "\n",
    "**Critical Rule:** NEVER train on validation or test data! That's cheating!\n",
    "\n",
    "**Stratified Splitting:**\n",
    "```python\n",
    "train_test_split(..., stratify=y)\n",
    "```\n",
    "- Ensures each split has the same fraud ratio (0.173%)\n",
    "- Without this, test set might have no fraud cases!\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Feature Standardization (Normalization)**\n",
    "\n",
    "**Problem:** Features have wildly different ranges\n",
    "- Time: [0, 172,792]\n",
    "- V1: [-56.4, 2.5]\n",
    "- Amount: [0, 25,691]\n",
    "\n",
    "**Solution: StandardScaler**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)  # Learn mean & std, then transform\n",
    "X_val = scaler.transform(X_val)          # Use same mean & std from training\n",
    "X_test = scaler.transform(X_test)        # Use same mean & std from training\n",
    "```\n",
    "\n",
    "**What it does:** Transforms each feature to have:\n",
    "- **Mean = 0** (centered around zero)\n",
    "- **Standard deviation = 1** (similar spread)\n",
    "\n",
    "**Formula:** `x_scaled = (x - mean) / std`\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Original Amount: [10, 100, 1000]\n",
    "Mean: 370, Std: 500\n",
    "Scaled: [-0.72, -0.54, 1.26]  (all similar magnitude!)\n",
    "```\n",
    "\n",
    "**CRITICAL:** \n",
    "- **Fit only on training data** (compute mean & std from training)\n",
    "- **Transform validation and test** using training statistics\n",
    "- Why? Test set should represent \"unseen future data\" - we won't know its statistics!\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Handle Class Imbalance with Weights**\n",
    "\n",
    "**The Problem:**\n",
    "- 284,315 normal transactions\n",
    "- 492 fraud transactions\n",
    "- Ratio: 577:1 (extreme imbalance!)\n",
    "\n",
    "**Naive approach:** Model predicts \"all normal\" ‚Üí 99.8% accuracy, 0% fraud detection ‚ùå\n",
    "\n",
    "**Solution: Class Weights**\n",
    "```python\n",
    "weight_normal = n_samples / (2 * n_normal) = 0.5\n",
    "weight_fraud = n_samples / (2 * n_fraud) = 289.2\n",
    "```\n",
    "\n",
    "**What this means:**\n",
    "- When model makes error on fraud: penalty √ó 289.2\n",
    "- When model makes error on normal: penalty √ó 0.5\n",
    "- Forces model to pay attention to rare fraud cases!\n",
    "\n",
    "**In loss function:**\n",
    "```python\n",
    "loss = weight √ó error\n",
    "```\n",
    "- Fraud errors hurt much more ‚Üí model learns to catch fraud!\n",
    "\n",
    "---\n",
    "\n",
    "### Data Shape Summary\n",
    "\n",
    "**Before preprocessing:**\n",
    "```\n",
    "Raw data: (284,807 transactions, 30 features)\n",
    "```\n",
    "\n",
    "**After preprocessing:**\n",
    "```\n",
    "X_train: (199,365, 29)  # 70% of data, standardized features\n",
    "y_train: (199,365,)     # Labels (0 or 1)\n",
    "\n",
    "X_val:   (42,721, 29)   # 15% of data, standardized\n",
    "y_val:   (42,721,)      # Labels\n",
    "\n",
    "X_test:  (42,721, 29)   # 15% of data, standardized (unseen until final eval)\n",
    "y_test:  (42,721,)      # Labels (used only for final metrics)\n",
    "```\n",
    "\n",
    "**Note:** 29 features (not 30) because we dropped the 'Class' column (that's our label y!)\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Standardization:** Makes features comparable (same scale)\n",
    "‚úÖ **Train/Val/Test Split:** Prevents cheating, gives honest evaluation\n",
    "‚úÖ **Stratified Split:** Preserves class distribution across splits\n",
    "‚úÖ **Class Weights:** Handles extreme imbalance (577:1 ratio)\n",
    "‚úÖ **Fit on Train Only:** Never let model \"peek\" at validation/test statistics\n",
    "‚úÖ **Test Set is Sacred:** Use only once at the very end!\n",
    "\n",
    "**Now we're ready to build the neural network!** üß†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e024d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LOADING CREDIT CARD FRAUD DETECTION DATASET\n",
      "======================================================================\n",
      "\n",
      "Downloading dataset from OpenML (may take a moment)...\n",
      "‚úÖ Dataset loaded: 284,807 transactions, 29 features\n",
      "\n",
      "üìä Dataset Overview:\n",
      "  Shape: (284807, 30)\n",
      "  Features: ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n",
      "\n",
      "  Class distribution:\n",
      "    Normal transactions: 284,315 (99.827%)\n",
      "    Fraud transactions:  492 (0.173%)\n",
      "    Imbalance ratio: 577:1\n",
      "\n",
      "  First few rows:\n",
      "         V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9       V10  ...       V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  0.090794  ... -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425 -0.166974  ... -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  0.207643  ...  0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024 -0.054952  ... -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  0.753074  ... -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "======================================================================\n",
      "DATA PREPROCESSING\n",
      "======================================================================\n",
      "\n",
      "üìä Data Splits:\n",
      "  Train: 199,356 samples (70.0%)\n",
      "  Val:   42,729 samples (15.0%)\n",
      "  Test:  42,722 samples (15.0%)\n",
      "\n",
      "  Class distribution in splits:\n",
      "    Train - Fraud: 344 (0.173%)\n",
      "    Val   - Fraud: 74 (0.173%)\n",
      "    Test  - Fraud: 74 (0.173%)\n",
      "\n",
      "‚öñÔ∏è  Class Weights (for balanced loss):\n",
      "  Normal: 0.5009\n",
      "  Fraud:  289.7616\n",
      "  Ratio:  578.52x (frauds weighted higher)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SETUP AND DATA LOADING\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report,\n",
    "    average_precision_score, roc_auc_score\n",
    ")\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING CREDIT CARD FRAUD DETECTION DATASET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Load dataset from OpenML\n",
    "print(\"\\nDownloading dataset from OpenML (may take a moment)...\")\n",
    "data = fetch_openml('creditcard', version=1, as_frame=True, parser='auto')\n",
    "df = data.frame\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded: {df.shape[0]:,} transactions, {df.shape[1]-1} features\")\n",
    "\n",
    "# Inspect the data\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  Shape: {df.shape}\")\n",
    "print(f\"  Features: {df.columns.tolist()}\")\n",
    "print(f\"\\n  Class distribution:\")\n",
    "fraud_count = (df['Class'] == '1').sum()\n",
    "normal_count = (df['Class'] == '0').sum()\n",
    "total = len(df)\n",
    "print(f\"    Normal transactions: {normal_count:,} ({100*normal_count/total:.3f}%)\")\n",
    "print(f\"    Fraud transactions:  {fraud_count:,} ({100*fraud_count/total:.3f}%)\")\n",
    "print(f\"    Imbalance ratio: {normal_count//fraud_count}:1\")\n",
    "\n",
    "print(f\"\\n  First few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1).values.astype(np.float32)\n",
    "y = df['Class'].astype(int).values\n",
    "\n",
    "# Split data: 70% train, 15% val, 15% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1765, random_state=42, stratify=y_temp  # 0.1765 * 0.85 ‚âà 0.15\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nüìä Data Splits:\")\n",
    "print(f\"  Train: {X_train.shape[0]:,} samples ({100*len(X_train)/total:.1f}%)\")\n",
    "print(f\"  Val:   {X_val.shape[0]:,} samples ({100*len(X_val)/total:.1f}%)\")\n",
    "print(f\"  Test:  {X_test.shape[0]:,} samples ({100*len(X_test)/total:.1f}%)\")\n",
    "\n",
    "print(f\"\\n  Class distribution in splits:\")\n",
    "print(f\"    Train - Fraud: {y_train.sum():,} ({100*y_train.sum()/len(y_train):.3f}%)\")\n",
    "print(f\"    Val   - Fraud: {y_val.sum():,} ({100*y_val.sum()/len(y_val):.3f}%)\")\n",
    "print(f\"    Test  - Fraud: {y_test.sum():,} ({100*y_test.sum()/len(y_test):.3f}%)\")\n",
    "\n",
    "# Calculate class weights for imbalance\n",
    "n_samples = len(y_train)\n",
    "n_fraud = y_train.sum()\n",
    "n_normal = n_samples - n_fraud\n",
    "weight_fraud = n_samples / (2 * n_fraud)\n",
    "weight_normal = n_samples / (2 * n_normal)\n",
    "\n",
    "print(f\"\\n‚öñÔ∏è  Class Weights (for balanced loss):\")\n",
    "print(f\"  Normal: {weight_normal:.4f}\")\n",
    "print(f\"  Fraud:  {weight_fraud:.4f}\")\n",
    "print(f\"  Ratio:  {weight_fraud/weight_normal:.2f}x (frauds weighted higher)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123e1ff",
   "metadata": {},
   "source": [
    "## üìö Part 2: Understanding Neural Networks & Training\n",
    "\n",
    "### What is a Neural Network? (Beginner-Friendly Explanation)\n",
    "\n",
    "**Simple answer:** A neural network is a mathematical function that learns patterns from data!\n",
    "\n",
    "**Analogy:** Think of it as a smart pattern recognition machine:\n",
    "- **Input:** Transaction features (amount, time, location, etc.)\n",
    "- **Processing:** Lots of mathematical operations (matrix multiplications + activations)\n",
    "- **Output:** Probability (\"This transaction is 87% likely to be fraud\")\n",
    "\n",
    "**How it \"learns\":** By adjusting internal numbers (weights and biases) to make better predictions!\n",
    "\n",
    "---\n",
    "\n",
    "### Neural Network Architecture Explained\n",
    "\n",
    "**Our Architecture:** Input (29) ‚Üí Dense(64) ‚Üí Dense(32) ‚Üí Dense(16) ‚Üí Output(1)\n",
    "\n",
    "#### **Layer-by-Layer Breakdown:**\n",
    "\n",
    "**1. Input Layer (29 neurons)**\n",
    "- Not really a \"layer\" - just your input data\n",
    "- 29 features: V1, V2, ..., V28, Time\n",
    "- Each neuron = one feature value\n",
    "\n",
    "**2. Hidden Layer 1 (64 neurons)**\n",
    "```python\n",
    "x = W1 @ input + b1  # Linear transformation (matrix multiplication)\n",
    "x = ReLU(x)          # Activation (keeps only positive values)\n",
    "```\n",
    "- **Purpose:** Learn 64 different \"feature combinations\"\n",
    "- **Example feature combo:** \"High amount + unusual time + rare location = suspicious\"\n",
    "- Each neuron learns one pattern\n",
    "\n",
    "**3. Hidden Layer 2 (32 neurons)**\n",
    "```python\n",
    "x = W2 @ x + b2      # Another transformation\n",
    "x = ReLU(x)          # Activation\n",
    "```\n",
    "- **Purpose:** Combine patterns from layer 1 into higher-level patterns\n",
    "- **Example:** \"Multiple suspicious patterns together = likely fraud\"\n",
    "\n",
    "**4. Hidden Layer 3 (16 neurons)**\n",
    "```python\n",
    "x = W3 @ x + b3      # Another transformation\n",
    "x = ReLU(x)          # Activation\n",
    "```\n",
    "- **Purpose:** Further refine patterns\n",
    "- Gets more abstract and specific to fraud detection\n",
    "\n",
    "**5. Output Layer (1 neuron)**\n",
    "```python\n",
    "output = W4 @ x + b4  # Final transformation\n",
    "output = Sigmoid(output)  # Squashes to probability [0, 1]\n",
    "```\n",
    "- **Output:** Single number between 0 and 1\n",
    "- **Interpretation:** Probability that transaction is fraud\n",
    "  - 0.05 = \"5% chance of fraud\" ‚Üí Predict: Legitimate\n",
    "  - 0.92 = \"92% chance of fraud\" ‚Üí Predict: Fraud\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components Explained\n",
    "\n",
    "#### **1. Weights (W) and Biases (b)**\n",
    "\n",
    "**Weights:** Numbers that determine \"how much each input matters\"\n",
    "```python\n",
    "W1 shape: (29, 64)  # Connects 29 inputs to 64 neurons\n",
    "```\n",
    "- Each connection has a weight\n",
    "- Positive weight = \"this feature increases fraud probability\"\n",
    "- Negative weight = \"this feature decreases fraud probability\"\n",
    "\n",
    "**Biases:** Offset values (one per neuron)\n",
    "```python\n",
    "b1 shape: (64,)  # One bias per neuron in layer 1\n",
    "```\n",
    "- Shifts the activation threshold\n",
    "- Allows neuron to activate even when inputs are zero\n",
    "\n",
    "**Initialization:** He initialization\n",
    "```python\n",
    "W = sqrt(2/input_size) * random_normal()\n",
    "```\n",
    "- Not too small (learning would be slow)\n",
    "- Not too large (training would be unstable)\n",
    "- Magic formula that works well for ReLU activations!\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Activation Functions**\n",
    "\n",
    "**ReLU (Rectified Linear Unit)** - Used in hidden layers\n",
    "```python\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "**What it does:**\n",
    "- If x > 0: Keep it (output = x)\n",
    "- If x ‚â§ 0: Set to 0 (output = 0)\n",
    "\n",
    "**Why we need it:**\n",
    "- Without activation: Network is just linear (like y = mx + b)\n",
    "- With ReLU: Network can learn complex, non-linear patterns!\n",
    "- **Example:** \"Fraud happens when amount > 1000 AND time is between 2am-4am\" (non-linear!)\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "Input:  [-2, -1, 0, 1, 2]\n",
    "ReLU:   [ 0,  0, 0, 1, 2]  (zeros out negatives)\n",
    "```\n",
    "\n",
    "**Sigmoid** - Used in output layer\n",
    "```python\n",
    "Sigmoid(x) = 1 / (1 + e^(-x))\n",
    "```\n",
    "**What it does:**\n",
    "- Squashes any number to range [0, 1]\n",
    "- Perfect for probabilities!\n",
    "\n",
    "**Visual:**\n",
    "```\n",
    "Input:    [-10, -2, 0, 2, 10]\n",
    "Sigmoid:  [0.00, 0.12, 0.5, 0.88, 1.00]  (all between 0 and 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Loss Function: Binary Cross-Entropy**\n",
    "\n",
    "**Purpose:** Measures \"how wrong\" the model's predictions are\n",
    "\n",
    "**Formula:**\n",
    "```python\n",
    "loss = -[y * log(p) + (1-y) * log(1-p)]\n",
    "```\n",
    "Where:\n",
    "- `y` = true label (0 or 1)\n",
    "- `p` = predicted probability\n",
    "\n",
    "**Example 1:** True label = 1 (fraud), Prediction = 0.95\n",
    "```\n",
    "loss = -[1 * log(0.95) + 0 * log(0.05)]\n",
    "     = -log(0.95)\n",
    "     = 0.051  (small loss - good prediction!)\n",
    "```\n",
    "\n",
    "**Example 2:** True label = 1 (fraud), Prediction = 0.10\n",
    "```\n",
    "loss = -[1 * log(0.10) + 0 * log(0.90)]\n",
    "     = -log(0.10)\n",
    "     = 2.303  (big loss - terrible prediction!)\n",
    "```\n",
    "\n",
    "**Key insight:** Loss is low when prediction matches reality, high when they differ!\n",
    "\n",
    "**With Class Weights:**\n",
    "```python\n",
    "weighted_loss = weight * loss\n",
    "```\n",
    "- Fraud errors get weight = 289.2 (huge penalty!)\n",
    "- Normal errors get weight = 0.5 (small penalty)\n",
    "- Forces model to focus on catching fraud!\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Training Process (How the Model Learns)**\n",
    "\n",
    "**The Learning Algorithm: Gradient Descent**\n",
    "\n",
    "Think of it like hiking down a mountain in fog:\n",
    "- You can't see the bottom (optimal weights)\n",
    "- But you can feel the slope under your feet (gradient)\n",
    "- Take small steps downhill (opposite of gradient)\n",
    "- Eventually reach the valley (minimum loss)\n",
    "\n",
    "**Step-by-Step Training Loop:**\n",
    "\n",
    "**1. Forward Pass** (Make Predictions)\n",
    "```python\n",
    "predictions = model(X_batch)  # Run data through network\n",
    "```\n",
    "- Input flows through all layers\n",
    "- Produces predictions (probabilities)\n",
    "\n",
    "**2. Compute Loss** (How Wrong Are We?)\n",
    "```python\n",
    "loss = binary_cross_entropy(predictions, y_batch, weights)\n",
    "```\n",
    "- Compare predictions to true labels\n",
    "- Higher loss = worse predictions\n",
    "\n",
    "**3. Backward Pass** (Compute Gradients)\n",
    "```python\n",
    "grads = jax.grad(loss_fn)(params, X_batch, y_batch)\n",
    "```\n",
    "- **Gradient:** Direction and magnitude to change each weight\n",
    "- JAX computes this automatically (magic of autodiff!)\n",
    "- Tells us: \"Increase W1[0,0] by 0.02 to reduce loss\"\n",
    "\n",
    "**4. Update Weights** (Learn!)\n",
    "```python\n",
    "W = W - learning_rate * gradient\n",
    "b = b - learning_rate * gradient\n",
    "```\n",
    "- **Learning rate (0.001):** Step size\n",
    "  - Too big: Overshoot the minimum, training unstable\n",
    "  - Too small: Takes forever to converge\n",
    "  - 0.001 is a good starting point!\n",
    "\n",
    "**5. Repeat** for all batches, all epochs\n",
    "- **Batch:** Subset of data (256 transactions)\n",
    "- **Epoch:** One pass through entire dataset\n",
    "- **10 epochs:** Model sees each transaction 10 times\n",
    "\n",
    "---\n",
    "\n",
    "### Training Hyperparameters Explained\n",
    "\n",
    "**1. Batch Size = 256**\n",
    "- Don't use all 199,365 transactions at once (too slow!)\n",
    "- Don't use 1 transaction at a time (too noisy!)\n",
    "- Use mini-batches of 256 (good balance)\n",
    "- **Math:** 199,365 / 256 ‚âà 779 batches per epoch\n",
    "\n",
    "**2. Learning Rate = 0.001**\n",
    "- How big each weight update step is\n",
    "- 0.001 is conservative but safe\n",
    "- Prevents overshooting and instability\n",
    "\n",
    "**3. Epochs = 10**\n",
    "- How many times model sees entire dataset\n",
    "- More epochs = more learning (but risk overfitting)\n",
    "- 10 is reasonable for this dataset size\n",
    "\n",
    "**4. Architecture: 64 ‚Üí 32 ‚Üí 16**\n",
    "- **Why decreasing?** Funnel pattern\n",
    "  - Layer 1: Learn many low-level patterns\n",
    "  - Layer 2: Combine into mid-level patterns\n",
    "  - Layer 3: Refine to high-level fraud indicators\n",
    "- **Why these numbers?** Empirically work well!\n",
    "  - Not too big: Faster training, less overfitting\n",
    "  - Not too small: Enough capacity to learn complex patterns\n",
    "\n",
    "---\n",
    "\n",
    "### JAX vs PyTorch: Key Differences\n",
    "\n",
    "**JAX (Functional Programming):**\n",
    "```python\n",
    "# Explicit parameter passing\n",
    "params = init_network_params(sizes, key)\n",
    "output = forward(params, x)\n",
    "loss, grads = value_and_grad(loss_fn)(params, x, y)\n",
    "params = update(params, grads, lr)  # Manual update\n",
    "```\n",
    "‚úÖ Explicit control\n",
    "‚úÖ Easy to compose transformations (jit + grad + vmap)\n",
    "‚úÖ Faster with JIT compilation\n",
    "‚ùå More boilerplate code\n",
    "\n",
    "**PyTorch (Object-Oriented):**\n",
    "```python\n",
    "# Stateful model class\n",
    "model = NeuralNet()\n",
    "output = model(x)\n",
    "loss.backward()  # Automatic gradient computation\n",
    "optimizer.step()  # Automatic update\n",
    "```\n",
    "‚úÖ Less boilerplate\n",
    "‚úÖ Familiar to most ML practitioners\n",
    "‚úÖ Great ecosystem\n",
    "‚ùå Less control over transformations\n",
    "\n",
    "**Both produce same results!** Just different coding styles.\n",
    "\n",
    "---\n",
    "\n",
    "### What Happens During Training?\n",
    "\n",
    "**Epoch 1:**\n",
    "- Weights are random ‚Üí Predictions are random ‚Üí Loss is high (‚âà0.7)\n",
    "- Gradients computed ‚Üí Weights adjusted\n",
    "- Model slightly better at end of epoch 1\n",
    "\n",
    "**Epoch 2-5:**\n",
    "- Model learning quickly\n",
    "- Loss decreasing steadily (‚âà0.5 ‚Üí 0.3)\n",
    "- Starting to recognize fraud patterns\n",
    "\n",
    "**Epoch 6-10:**\n",
    "- Model refining predictions\n",
    "- Loss decreasing slowly (‚âà0.3 ‚Üí 0.2)\n",
    "- Fine-tuning weights for best performance\n",
    "\n",
    "**After Training:**\n",
    "- Model has learned optimal weights\n",
    "- Can make predictions on new transactions!\n",
    "\n",
    "---\n",
    "\n",
    "### Common Questions\n",
    "\n",
    "**Q: Why not just use more layers?**\n",
    "A: More layers = more capacity, but also:\n",
    "- Slower training\n",
    "- Risk of overfitting (memorizing training data)\n",
    "- Diminishing returns (3 layers often enough for tabular data)\n",
    "\n",
    "**Q: What if loss doesn't decrease?**\n",
    "A: Could be:\n",
    "- Learning rate too high (try 0.0001)\n",
    "- Learning rate too low (try 0.01)\n",
    "- Bad initialization (run again with different seed)\n",
    "- Insufficient model capacity (add more neurons)\n",
    "\n",
    "**Q: How do we know when to stop training?**\n",
    "A: Monitor validation loss:\n",
    "- If training loss ‚Üì but validation loss ‚Üë ‚Üí Overfitting! Stop.\n",
    "- If both ‚Üì ‚Üí Keep training\n",
    "- If both plateau ‚Üí Done! Converged.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Neural Network:** Function with adjustable weights that learns patterns\n",
    "‚úÖ **Forward Pass:** Input ‚Üí Hidden Layers ‚Üí Output (predictions)\n",
    "‚úÖ **Loss Function:** Measures prediction errors (we want to minimize this)\n",
    "‚úÖ **Gradient:** Direction to change weights to reduce loss\n",
    "‚úÖ **Training:** Repeatedly adjust weights to minimize loss\n",
    "‚úÖ **Batch Training:** Process small batches for efficiency\n",
    "‚úÖ **Class Weights:** Handle imbalance by penalizing fraud errors more\n",
    "‚úÖ **Activation Functions:** Enable learning complex patterns\n",
    "\n",
    "**Now let's see it in action with JAX!** üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c32aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "JAX NEURAL NETWORK - FUNCTIONAL APPROACH\n",
      "======================================================================\n",
      "\n",
      "üîß Initializing JAX model...\n",
      "  Architecture: 29 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1\n",
      "  Total parameters: 4,545\n",
      "\n",
      "üèãÔ∏è  Training JAX model...\n",
      "  Epoch  1/10 - Train Loss: 0.5035, Val Loss: 0.3022\n",
      "  Epoch  2/10 - Train Loss: 0.3023, Val Loss: 0.2485\n",
      "  Epoch  1/10 - Train Loss: 0.5035, Val Loss: 0.3022\n",
      "  Epoch  2/10 - Train Loss: 0.3023, Val Loss: 0.2485\n",
      "  Epoch  3/10 - Train Loss: 0.2594, Val Loss: 0.2252\n",
      "  Epoch  4/10 - Train Loss: 0.2316, Val Loss: 0.2039\n",
      "  Epoch  3/10 - Train Loss: 0.2594, Val Loss: 0.2252\n",
      "  Epoch  4/10 - Train Loss: 0.2316, Val Loss: 0.2039\n",
      "  Epoch  5/10 - Train Loss: 0.2112, Val Loss: 0.1934\n",
      "  Epoch  6/10 - Train Loss: 0.1954, Val Loss: 0.1868\n",
      "  Epoch  5/10 - Train Loss: 0.2112, Val Loss: 0.1934\n",
      "  Epoch  6/10 - Train Loss: 0.1954, Val Loss: 0.1868\n",
      "  Epoch  7/10 - Train Loss: 0.1816, Val Loss: 0.1809\n",
      "  Epoch  8/10 - Train Loss: 0.1711, Val Loss: 0.1757\n",
      "  Epoch  7/10 - Train Loss: 0.1816, Val Loss: 0.1809\n",
      "  Epoch  8/10 - Train Loss: 0.1711, Val Loss: 0.1757\n",
      "  Epoch  9/10 - Train Loss: 0.1615, Val Loss: 0.1720\n",
      "  Epoch 10/10 - Train Loss: 0.1522, Val Loss: 0.1691\n",
      "\n",
      "‚úÖ JAX training complete in 2.30s\n",
      "\n",
      "üìä JAX Test Set Evaluation:\n",
      "  Epoch  9/10 - Train Loss: 0.1615, Val Loss: 0.1720\n",
      "  Epoch 10/10 - Train Loss: 0.1522, Val Loss: 0.1691\n",
      "\n",
      "‚úÖ JAX training complete in 2.30s\n",
      "\n",
      "üìä JAX Test Set Evaluation:\n",
      "  Precision: 0.0719\n",
      "  Recall:    0.8514\n",
      "  F1 Score:  0.1326\n",
      "  PR-AUC:    0.6471\n",
      "  ROC-AUC:   0.9536\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 41835  FP:   813\n",
      "    FN:    11  TP:    63\n",
      "  Precision: 0.0719\n",
      "  Recall:    0.8514\n",
      "  F1 Score:  0.1326\n",
      "  PR-AUC:    0.6471\n",
      "  ROC-AUC:   0.9536\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 41835  FP:   813\n",
      "    FN:    11  TP:    63\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# JAX IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"JAX NEURAL NETWORK - FUNCTIONAL APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Hyperparameters\n",
    "input_dim = X_train.shape[1]  # Dynamically get input dimension from data\n",
    "hidden_dims = [64, 32, 16]\n",
    "output_dim = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 256\n",
    "n_epochs = 10\n",
    "\n",
    "# Initialize network parameters\n",
    "def init_network_params(layer_sizes, key):\n",
    "    \"\"\"Initialize network with He initialization.\"\"\"\n",
    "    params = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # He initialization: scale by sqrt(2/fan_in)\n",
    "        scale = jnp.sqrt(2.0 / layer_sizes[i])\n",
    "        W = scale * jax.random.normal(subkey, (layer_sizes[i], layer_sizes[i+1]))\n",
    "        key, subkey = jax.random.split(key)\n",
    "        b = jnp.zeros(layer_sizes[i+1])\n",
    "        params.append({'W': W, 'b': b})\n",
    "    return params\n",
    "\n",
    "# Forward pass\n",
    "def forward(params, x):\n",
    "    \"\"\"Forward pass through the network.\"\"\"\n",
    "    for i, layer in enumerate(params[:-1]):\n",
    "        x = jnp.dot(x, layer['W']) + layer['b']\n",
    "        x = jax.nn.relu(x)  # ReLU activation for hidden layers\n",
    "    # Output layer (sigmoid activation)\n",
    "    x = jnp.dot(x, params[-1]['W']) + params[-1]['b']\n",
    "    return jax.nn.sigmoid(x)\n",
    "\n",
    "# Weighted binary cross-entropy loss\n",
    "def loss_fn(params, x, y, class_weights):\n",
    "    \"\"\"Binary cross-entropy with class weights.\"\"\"\n",
    "    predictions = forward(params, x).squeeze()\n",
    "    # Apply class weights\n",
    "    weights = jnp.where(y == 1, class_weights[1], class_weights[0])\n",
    "    # Binary cross-entropy\n",
    "    bce = -(y * jnp.log(predictions + 1e-7) + (1 - y) * jnp.log(1 - predictions + 1e-7))\n",
    "    return jnp.mean(weights * bce)\n",
    "\n",
    "# Prediction function\n",
    "def predict(params, x, threshold=0.5):\n",
    "    \"\"\"Make predictions with threshold.\"\"\"\n",
    "    probs = forward(params, x).squeeze()\n",
    "    return (probs >= threshold).astype(jnp.int32)\n",
    "\n",
    "# Training step (JIT compiled)\n",
    "@jax.jit\n",
    "def update(params, x, y, class_weights, learning_rate):\n",
    "    \"\"\"Single training step with gradient descent.\"\"\"\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params, x, y, class_weights)\n",
    "    # Update parameters\n",
    "    params = [\n",
    "        {\n",
    "            'W': layer['W'] - learning_rate * grad['W'],\n",
    "            'b': layer['b'] - learning_rate * grad['b']\n",
    "        }\n",
    "        for layer, grad in zip(params, grads)\n",
    "    ]\n",
    "    return params, loss\n",
    "\n",
    "# Initialize JAX model\n",
    "print(\"\\nüîß Initializing JAX model...\")\n",
    "layer_sizes = [input_dim] + hidden_dims + [output_dim]\n",
    "jax_params = init_network_params(layer_sizes, jax.random.PRNGKey(42))\n",
    "jax_class_weights = jnp.array([weight_normal, weight_fraud])\n",
    "\n",
    "print(f\"  Architecture: {' ‚Üí '.join(map(str, layer_sizes))}\")\n",
    "total_params = sum(layer['W'].size + layer['b'].size for layer in jax_params)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nüèãÔ∏è  Training JAX model...\")\n",
    "jax_train_losses = []\n",
    "jax_val_losses = []\n",
    "\n",
    "# Convert to JAX arrays\n",
    "X_train_jax = jnp.array(X_train)\n",
    "y_train_jax = jnp.array(y_train, dtype=jnp.float32)\n",
    "X_val_jax = jnp.array(X_val)\n",
    "y_val_jax = jnp.array(y_val, dtype=jnp.float32)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Shuffle training data\n",
    "    perm = np.random.permutation(len(X_train_jax))\n",
    "    X_shuffled = X_train_jax[perm]\n",
    "    y_shuffled = y_train_jax[perm]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    epoch_losses = []\n",
    "    for i in range(0, len(X_train_jax), batch_size):\n",
    "        batch_X = X_shuffled[i:i+batch_size]\n",
    "        batch_y = y_shuffled[i:i+batch_size]\n",
    "        jax_params, batch_loss = update(jax_params, batch_X, batch_y, jax_class_weights, learning_rate)\n",
    "        epoch_losses.append(batch_loss)\n",
    "    \n",
    "    # Compute validation loss\n",
    "    train_loss = jnp.mean(jnp.array(epoch_losses))\n",
    "    val_loss = loss_fn(jax_params, X_val_jax, y_val_jax, jax_class_weights)\n",
    "    \n",
    "    jax_train_losses.append(float(train_loss))\n",
    "    jax_val_losses.append(float(val_loss))\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1:2d}/{n_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "jax_train_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ JAX training complete in {jax_train_time:.2f}s\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nüìä JAX Test Set Evaluation:\")\n",
    "X_test_jax = jnp.array(X_test)\n",
    "y_pred_jax = predict(jax_params, X_test_jax)\n",
    "y_probs_jax = forward(jax_params, X_test_jax).squeeze()\n",
    "\n",
    "# Convert to numpy for sklearn metrics\n",
    "y_pred_jax_np = np.array(y_pred_jax)\n",
    "y_probs_jax_np = np.array(y_probs_jax)\n",
    "\n",
    "jax_precision = precision_score(y_test, y_pred_jax_np)\n",
    "jax_recall = recall_score(y_test, y_pred_jax_np)\n",
    "jax_f1 = f1_score(y_test, y_pred_jax_np)\n",
    "jax_pr_auc = average_precision_score(y_test, y_probs_jax_np)\n",
    "jax_roc_auc = roc_auc_score(y_test, y_probs_jax_np)\n",
    "\n",
    "print(f\"  Precision: {jax_precision:.4f}\")\n",
    "print(f\"  Recall:    {jax_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {jax_f1:.4f}\")\n",
    "print(f\"  PR-AUC:    {jax_pr_auc:.4f}\")\n",
    "print(f\"  ROC-AUC:   {jax_roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n  Confusion Matrix:\")\n",
    "cm_jax = confusion_matrix(y_test, y_pred_jax_np)\n",
    "print(f\"    TN: {cm_jax[0,0]:5d}  FP: {cm_jax[0,1]:5d}\")\n",
    "print(f\"    FN: {cm_jax[1,0]:5d}  TP: {cm_jax[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca9bd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PYTORCH NEURAL NETWORK - OBJECT-ORIENTED APPROACH\n",
      "======================================================================\n",
      "\n",
      "üîß Initializing PyTorch model...\n",
      "  Architecture: 29 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1\n",
      "  Total parameters: 4,545\n",
      "\n",
      "üèãÔ∏è  Training PyTorch model...\n",
      "  Architecture: 29 ‚Üí 64 ‚Üí 32 ‚Üí 16 ‚Üí 1\n",
      "  Total parameters: 4,545\n",
      "\n",
      "üèãÔ∏è  Training PyTorch model...\n",
      "  Epoch  1/10 - Train Loss: 0.2558, Val Loss: 0.1361\n",
      "  Epoch  1/10 - Train Loss: 0.2558, Val Loss: 0.1361\n",
      "  Epoch  2/10 - Train Loss: 0.1557, Val Loss: 0.1366\n",
      "  Epoch  2/10 - Train Loss: 0.1557, Val Loss: 0.1366\n",
      "  Epoch  3/10 - Train Loss: 0.1207, Val Loss: 0.1790\n",
      "  Epoch  3/10 - Train Loss: 0.1207, Val Loss: 0.1790\n",
      "  Epoch  4/10 - Train Loss: 0.1096, Val Loss: 0.1948\n",
      "  Epoch  4/10 - Train Loss: 0.1096, Val Loss: 0.1948\n",
      "  Epoch  5/10 - Train Loss: 0.1059, Val Loss: 0.1587\n",
      "  Epoch  5/10 - Train Loss: 0.1059, Val Loss: 0.1587\n",
      "  Epoch  6/10 - Train Loss: 0.0908, Val Loss: 0.1975\n",
      "  Epoch  6/10 - Train Loss: 0.0908, Val Loss: 0.1975\n",
      "  Epoch  7/10 - Train Loss: 0.0891, Val Loss: 0.1826\n",
      "  Epoch  7/10 - Train Loss: 0.0891, Val Loss: 0.1826\n",
      "  Epoch  8/10 - Train Loss: 0.0763, Val Loss: 0.1698\n",
      "  Epoch  8/10 - Train Loss: 0.0763, Val Loss: 0.1698\n",
      "  Epoch  9/10 - Train Loss: 0.0681, Val Loss: 0.2426\n",
      "  Epoch  9/10 - Train Loss: 0.0681, Val Loss: 0.2426\n",
      "  Epoch 10/10 - Train Loss: 0.0618, Val Loss: 0.2298\n",
      "\n",
      "‚úÖ PyTorch training complete in 9.17s\n",
      "\n",
      "üìä PyTorch Test Set Evaluation:\n",
      "  Precision: 0.0694\n",
      "  Recall:    0.8784\n",
      "  F1 Score:  0.1287\n",
      "  PR-AUC:    0.6472\n",
      "  ROC-AUC:   0.9584\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 41777  FP:   871\n",
      "    FN:     9  TP:    65\n",
      "  Epoch 10/10 - Train Loss: 0.0618, Val Loss: 0.2298\n",
      "\n",
      "‚úÖ PyTorch training complete in 9.17s\n",
      "\n",
      "üìä PyTorch Test Set Evaluation:\n",
      "  Precision: 0.0694\n",
      "  Recall:    0.8784\n",
      "  F1 Score:  0.1287\n",
      "  PR-AUC:    0.6472\n",
      "  ROC-AUC:   0.9584\n",
      "\n",
      "  Confusion Matrix:\n",
      "    TN: 41777  FP:   871\n",
      "    FN:     9  TP:    65\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# PYTORCH IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PYTORCH NEURAL NETWORK - OBJECT-ORIENTED APPROACH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define PyTorch model\n",
    "class FraudDetectionNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x).squeeze()\n",
    "\n",
    "# Initialize PyTorch model\n",
    "print(\"\\nüîß Initializing PyTorch model...\")\n",
    "torch.manual_seed(42)\n",
    "torch_model = FraudDetectionNet(input_dim, hidden_dims, output_dim)\n",
    "torch_optimizer = optim.Adam(torch_model.parameters(), lr=learning_rate)\n",
    "\n",
    "print(f\"  Architecture: {input_dim} ‚Üí {' ‚Üí '.join(map(str, hidden_dims))} ‚Üí {output_dim}\")\n",
    "total_params = sum(p.numel() for p in torch_model.parameters())\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "\n",
    "# Weighted BCE loss\n",
    "pos_weight = torch.tensor([weight_fraud / weight_normal])\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_torch = torch.FloatTensor(X_train)\n",
    "y_train_torch = torch.FloatTensor(y_train)\n",
    "X_val_torch = torch.FloatTensor(X_val)\n",
    "y_val_torch = torch.FloatTensor(y_val)\n",
    "X_test_torch = torch.FloatTensor(X_test)\n",
    "y_test_torch = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create class weights tensor\n",
    "class_weights_torch = torch.FloatTensor([weight_normal, weight_fraud])\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nüèãÔ∏è  Training PyTorch model...\")\n",
    "torch_train_losses = []\n",
    "torch_val_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    torch_model.train()\n",
    "    \n",
    "    # Shuffle training data\n",
    "    perm = torch.randperm(len(X_train_torch))\n",
    "    X_shuffled = X_train_torch[perm]\n",
    "    y_shuffled = y_train_torch[perm]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    epoch_losses = []\n",
    "    for i in range(0, len(X_train_torch), batch_size):\n",
    "        batch_X = X_shuffled[i:i+batch_size]\n",
    "        batch_y = y_shuffled[i:i+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        torch_optimizer.zero_grad()\n",
    "        predictions = torch_model(batch_X)\n",
    "        \n",
    "        # Compute weighted loss\n",
    "        losses = criterion(predictions, batch_y)\n",
    "        weights = torch.where(batch_y == 1, class_weights_torch[1], class_weights_torch[0])\n",
    "        loss = (losses * weights).mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch_optimizer.step()\n",
    "        \n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Validation\n",
    "    torch_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_predictions = torch_model(X_val_torch)\n",
    "        val_losses = criterion(val_predictions, y_val_torch)\n",
    "        val_weights = torch.where(y_val_torch == 1, class_weights_torch[1], class_weights_torch[0])\n",
    "        val_loss = (val_losses * val_weights).mean()\n",
    "    \n",
    "    train_loss = np.mean(epoch_losses)\n",
    "    torch_train_losses.append(train_loss)\n",
    "    torch_val_losses.append(val_loss.item())\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1:2d}/{n_epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "torch_train_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ PyTorch training complete in {torch_train_time:.2f}s\")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nüìä PyTorch Test Set Evaluation:\")\n",
    "torch_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_probs_torch = torch_model(X_test_torch).numpy()\n",
    "    y_pred_torch = (y_probs_torch >= 0.5).astype(int)\n",
    "\n",
    "torch_precision = precision_score(y_test, y_pred_torch)\n",
    "torch_recall = recall_score(y_test, y_pred_torch)\n",
    "torch_f1 = f1_score(y_test, y_pred_torch)\n",
    "torch_pr_auc = average_precision_score(y_test, y_probs_torch)\n",
    "torch_roc_auc = roc_auc_score(y_test, y_probs_torch)\n",
    "\n",
    "print(f\"  Precision: {torch_precision:.4f}\")\n",
    "print(f\"  Recall:    {torch_recall:.4f}\")\n",
    "print(f\"  F1 Score:  {torch_f1:.4f}\")\n",
    "print(f\"  PR-AUC:    {torch_pr_auc:.4f}\")\n",
    "print(f\"  ROC-AUC:   {torch_roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\n  Confusion Matrix:\")\n",
    "cm_torch = confusion_matrix(y_test, y_pred_torch)\n",
    "print(f\"    TN: {cm_torch[0,0]:5d}  FP: {cm_torch[0,1]:5d}\")\n",
    "print(f\"    FN: {cm_torch[1,0]:5d}  TP: {cm_torch[1,1]:5d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f40aced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL COMPARISON: JAX vs PYTORCH\n",
      "======================================================================\n",
      "\n",
      "üìä Performance Metrics:\n",
      "Metric                 JAX    PyTorch   Difference\n",
      "--------------------------------------------------\n",
      "Precision           0.0719     0.0694       0.0025\n",
      "Recall              0.8514     0.8784       0.0270\n",
      "F1 Score            0.1326     0.1287       0.0039\n",
      "PR-AUC              0.6471     0.6472       0.0001\n",
      "ROC-AUC             0.9536     0.9584       0.0049\n",
      "\n",
      "‚è±Ô∏è  Training Time:\n",
      "  JAX:     2.30s\n",
      "  PyTorch: 9.17s\n",
      "  JAX is 3.99x faster\n",
      "\n",
      "======================================================================\n",
      "KEY OBSERVATIONS\n",
      "======================================================================\n",
      "\n",
      "1. üìä Model Performance:\n",
      "   Both frameworks achieve similar predictive performance on this real-world\n",
      "   imbalanced dataset. The metrics (Precision, Recall, F1) are comparable,\n",
      "   showing that both handle class-weighted loss effectively.\n",
      "\n",
      "2. ‚è±Ô∏è  Training Speed:\n",
      "   JAX's JIT compilation (@jax.jit on update function) provides faster\n",
      "   training compared to standard PyTorch. The speedup is more pronounced\n",
      "   with larger datasets and more complex models.\n",
      "\n",
      "3. üíª Code Patterns:\n",
      "   JAX: Functional style with explicit parameter passing. JIT compilation\n",
      "        makes the update step extremely fast. Manual parameter management.\n",
      "\n",
      "   PyTorch: Object-oriented with stateful modules. Automatic parameter\n",
      "            tracking via nn.Module. Familiar to most ML practitioners.\n",
      "\n",
      "4. üéØ Handling Imbalance:\n",
      "   Both frameworks handle severe class imbalance (577:1) well with:\n",
      "   - Class-weighted loss function\n",
      "   - Proper evaluation metrics (F1, Precision, Recall, PR-AUC)\n",
      "   - Stratified train/val/test splits\n",
      "\n",
      "5. üöÄ Production Considerations:\n",
      "   JAX: Better for research, custom algorithms, need for composability\n",
      "   PyTorch: Better for production, larger ecosystem, easier debugging\n",
      "\n",
      "6. üìà Scalability:\n",
      "   Both scale well to this dataset size (284K samples). JAX's advantage\n",
      "   grows with:\n",
      "   - Larger batch sizes\n",
      "   - More complex gradient operations (vmap for per-sample gradients)\n",
      "   - Need for higher-order derivatives\n",
      "\n",
      "======================================================================\n",
      "CONCLUSION\n",
      "======================================================================\n",
      "\n",
      "On this real-world fraud detection task:\n",
      "\n",
      "‚úÖ JAX Strengths:\n",
      "   - Faster training (JIT compilation)\n",
      "   - Functional composability (jit + grad + vmap)\n",
      "   - Clean mathematical code\n",
      "   - Better for research and custom algorithms\n",
      "\n",
      "‚úÖ PyTorch Strengths:\n",
      "   - Easier to learn and debug\n",
      "   - Mature ecosystem (pretrained models, utilities)\n",
      "   - Industry standard for production\n",
      "   - Better documentation and community support\n",
      "\n",
      "Both frameworks are excellent for production ML. Choose based on your\n",
      "team's expertise and specific requirements rather than raw performance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPARISON AND ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FINAL COMPARISON: JAX vs PYTORCH\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Performance Metrics:\")\n",
    "print(f\"{'Metric':<15} {'JAX':>10} {'PyTorch':>10} {'Difference':>12}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Precision':<15} {jax_precision:>10.4f} {torch_precision:>10.4f} {abs(jax_precision-torch_precision):>12.4f}\")\n",
    "print(f\"{'Recall':<15} {jax_recall:>10.4f} {torch_recall:>10.4f} {abs(jax_recall-torch_recall):>12.4f}\")\n",
    "print(f\"{'F1 Score':<15} {jax_f1:>10.4f} {torch_f1:>10.4f} {abs(jax_f1-torch_f1):>12.4f}\")\n",
    "print(f\"{'PR-AUC':<15} {jax_pr_auc:>10.4f} {torch_pr_auc:>10.4f} {abs(jax_pr_auc-torch_pr_auc):>12.4f}\")\n",
    "print(f\"{'ROC-AUC':<15} {jax_roc_auc:>10.4f} {torch_roc_auc:>10.4f} {abs(jax_roc_auc-torch_roc_auc):>12.4f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Training Time:\")\n",
    "print(f\"  JAX:     {jax_train_time:.2f}s\")\n",
    "print(f\"  PyTorch: {torch_train_time:.2f}s\")\n",
    "if jax_train_time < torch_train_time:\n",
    "    print(f\"  JAX is {torch_train_time/jax_train_time:.2f}x faster\")\n",
    "else:\n",
    "    print(f\"  PyTorch is {jax_train_time/torch_train_time:.2f}x faster\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY OBSERVATIONS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. üìä Model Performance:\n",
    "   Both frameworks achieve similar predictive performance on this real-world\n",
    "   imbalanced dataset. The metrics (Precision, Recall, F1) are comparable,\n",
    "   showing that both handle class-weighted loss effectively.\n",
    "\n",
    "2. ‚è±Ô∏è  Training Speed:\n",
    "   JAX's JIT compilation (@jax.jit on update function) provides faster\n",
    "   training compared to standard PyTorch. The speedup is more pronounced\n",
    "   with larger datasets and more complex models.\n",
    "\n",
    "3. üíª Code Patterns:\n",
    "   JAX: Functional style with explicit parameter passing. JIT compilation\n",
    "        makes the update step extremely fast. Manual parameter management.\n",
    "   \n",
    "   PyTorch: Object-oriented with stateful modules. Automatic parameter\n",
    "            tracking via nn.Module. Familiar to most ML practitioners.\n",
    "\n",
    "4. üéØ Handling Imbalance:\n",
    "   Both frameworks handle severe class imbalance (577:1) well with:\n",
    "   - Class-weighted loss function\n",
    "   - Proper evaluation metrics (F1, Precision, Recall, PR-AUC)\n",
    "   - Stratified train/val/test splits\n",
    "\n",
    "5. üöÄ Production Considerations:\n",
    "   JAX: Better for research, custom algorithms, need for composability\n",
    "   PyTorch: Better for production, larger ecosystem, easier debugging\n",
    "\n",
    "6. üìà Scalability:\n",
    "   Both scale well to this dataset size (284K samples). JAX's advantage\n",
    "   grows with:\n",
    "   - Larger batch sizes\n",
    "   - More complex gradient operations (vmap for per-sample gradients)\n",
    "   - Need for higher-order derivatives\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "On this real-world fraud detection task:\n",
    "\n",
    "‚úÖ JAX Strengths:\n",
    "   - Faster training (JIT compilation)\n",
    "   - Functional composability (jit + grad + vmap)\n",
    "   - Clean mathematical code\n",
    "   - Better for research and custom algorithms\n",
    "\n",
    "‚úÖ PyTorch Strengths:\n",
    "   - Easier to learn and debug\n",
    "   - Mature ecosystem (pretrained models, utilities)\n",
    "   - Industry standard for production\n",
    "   - Better documentation and community support\n",
    "\n",
    "Both frameworks are excellent for production ML. Choose based on your\n",
    "team's expertise and specific requirements rather than raw performance.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c82b762",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
