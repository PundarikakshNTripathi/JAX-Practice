{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f1db1ab",
   "metadata": {},
   "source": [
    "# JAX Fundamentals\n",
    "\n",
    "## What is JAX?\n",
    "\n",
    "**JAX** is a library for high-performance numerical computing that brings together three key capabilities:\n",
    "\n",
    "- **J** - **JIT compilation** (Just-In-Time): Compiles Python functions to optimized machine code\n",
    "- **A** - **Automatic differentiation**: Computes gradients of any function automatically\n",
    "- **X** - **XLA** (Accelerated Linear Algebra): Google's optimizing compiler that targets CPU/GPU/TPU\n",
    "\n",
    "If you know NumPy, you already know most of JAX. The API is nearly identical, but JAX adds transformations that make it powerful for ML and scientific computing.\n",
    "\n",
    "## Key Characteristics\n",
    "\n",
    "1. **NumPy Compatible**: `jax.numpy` works like NumPy in most cases\n",
    "2. **Immutable Arrays**: JAX arrays can't be modified in place (functional style)\n",
    "3. **Hardware Accelerated**: Runs on GPU/TPU without code changes\n",
    "4. **Function Transformations**: Compose `jit`, `grad`, `vmap` however you need\n",
    "5. **Pure Functions**: Works best with functions that don't have side effects\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d9e1d",
   "metadata": {},
   "source": [
    "## JAX as NumPy - Basic Operations\n",
    "\n",
    "If you've used NumPy, this will look familiar. JAX arrays behave like NumPy arrays with one key difference: they're immutable. You can't modify them in place, but you get automatic GPU/TPU support in return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2dbfdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ARRAY CREATION AND BASIC OPERATIONS\n",
      "======================================================================\n",
      "\n",
      "üì¶ Array Creation:\n",
      "Array a: [1. 2. 3.]\n",
      "Array b: [4. 5. 6.]\n",
      "Array a shape: (3,), dtype: float32\n",
      "\n",
      "‚ûï Arithmetic Operations:\n",
      "Sum (a + b):                    [5. 7. 9.]\n",
      "Element-wise multiplication:    [ 4. 10. 18.]\n",
      "Dot product:                    32.0\n",
      "Matrix multiplication:          32.0\n",
      "\n",
      "üìê Mathematical Functions:\n",
      "Sine of a:                      [0.84147096 0.9092974  0.14112   ]\n",
      "Exponential of b:               [ 54.598152 148.41316  403.4288  ]\n",
      "Logarithm of a:                 [0.        0.6931472 1.0986123]\n",
      "Square root of b:               [2.        2.236068  2.4494898]\n",
      "Power (a^2):                    [1. 4. 9.]\n",
      "\n",
      "üìä Statistical Functions:\n",
      "Mean of a:                      2.0\n",
      "Standard deviation of b:        0.8164966106414795\n",
      "Variance of a:                  0.6666666865348816\n",
      "Median of b:                    5.0\n",
      "Maximum value in a:             3.0\n",
      "Minimum value in b:             4.0\n",
      "\n",
      "üî¢ Aggregation Functions:\n",
      "Sum of all elements in a:       6.0\n",
      "Product of all elements in b:   120.0\n",
      "Cumulative sum of a:            [1. 3. 6.]\n",
      "Cumulative product of a:        [1. 2. 6.]\n",
      "\n",
      "üîÑ Array Reshaping:\n",
      "Reshaped a to (3,1):\n",
      "[[1.]\n",
      " [2.]\n",
      " [3.]]\n",
      "Transpose of reshaped b:\n",
      "[[4. 5. 6.]]\n",
      "\n",
      "üìö Stacking and Concatenation:\n",
      "Vertical stack (vstack):\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "Horizontal stack (hstack):\n",
      "[1. 2. 3. 4. 5. 6.]\n",
      "Concatenate (same as hstack for 1D):\n",
      "[1. 2. 3. 4. 5. 6.]\n",
      "\n",
      "üîç Array Query Operations:\n",
      "Unique elements in b:           [4. 5. 6.]\n",
      "Sorted a (descending):          [3. 2. 1.]\n",
      "Indices where a > 2:            (Array([2], dtype=int32),)\n",
      "Boolean mask (a > 2):           [False False  True]\n",
      "Elements of a where a > 2:      [3.]\n",
      "\n",
      "üîÑ JAX ‚Üî NumPy Conversion:\n",
      "JAX to NumPy:                   [1. 2. 3.] (type: <class 'numpy.ndarray'>)\n",
      "NumPy to JAX:                   [1. 2. 3.] (type: <class 'jaxlib._jax.ArrayImpl'>)\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  JAX ARRAYS ARE IMMUTABLE\n",
      "======================================================================\n",
      "\n",
      "Unlike NumPy, JAX arrays cannot be modified in place.\n",
      "Operations return NEW arrays rather than modifying existing ones.\n",
      "\n",
      "‚ùå This will FAIL:\n",
      "   a[1] = 10.0  # TypeError: JAX arrays are immutable\n",
      "\n",
      "‚úÖ Use this instead:\n",
      "   a = a.at[1].set(10.0)  # Returns a new array with index 1 set to 10.0\n",
      "\n",
      "Original a:  [1. 2. 3.]\n",
      "Updated a:   [ 1. 10.  3.]\n",
      "Notice: Original 'a' is unchanged!\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ JAX provides a NumPy-compatible API (jax.numpy)\n",
      "‚úÖ Most NumPy operations work identically in JAX\n",
      "‚úÖ JAX arrays are immutable - use .at[].set() for updates\n",
      "‚úÖ JAX automatically runs on GPU/TPU when available\n",
      "‚úÖ Seamlessly convert between JAX and NumPy arrays\n",
      "‚úÖ JAX is designed for high-performance numerical computing\n",
      "‚úÖ Perfect for machine learning, scientific computing, and simulations\n",
      "\n",
      "Elements of a where a > 2:      [3.]\n",
      "\n",
      "üîÑ JAX ‚Üî NumPy Conversion:\n",
      "JAX to NumPy:                   [1. 2. 3.] (type: <class 'numpy.ndarray'>)\n",
      "NumPy to JAX:                   [1. 2. 3.] (type: <class 'jaxlib._jax.ArrayImpl'>)\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  JAX ARRAYS ARE IMMUTABLE\n",
      "======================================================================\n",
      "\n",
      "Unlike NumPy, JAX arrays cannot be modified in place.\n",
      "Operations return NEW arrays rather than modifying existing ones.\n",
      "\n",
      "‚ùå This will FAIL:\n",
      "   a[1] = 10.0  # TypeError: JAX arrays are immutable\n",
      "\n",
      "‚úÖ Use this instead:\n",
      "   a = a.at[1].set(10.0)  # Returns a new array with index 1 set to 10.0\n",
      "\n",
      "Original a:  [1. 2. 3.]\n",
      "Updated a:   [ 1. 10.  3.]\n",
      "Notice: Original 'a' is unchanged!\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS\n",
      "======================================================================\n",
      "\n",
      "‚úÖ JAX provides a NumPy-compatible API (jax.numpy)\n",
      "‚úÖ Most NumPy operations work identically in JAX\n",
      "‚úÖ JAX arrays are immutable - use .at[].set() for updates\n",
      "‚úÖ JAX automatically runs on GPU/TPU when available\n",
      "‚úÖ Seamlessly convert between JAX and NumPy arrays\n",
      "‚úÖ JAX is designed for high-performance numerical computing\n",
      "‚úÖ Perfect for machine learning, scientific computing, and simulations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# JAX AS NUMPY - COMPREHENSIVE DEMONSTRATION\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ARRAY CREATION AND BASIC OPERATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Creating JAX Arrays\n",
    "# -----------------------------------------------------------------------------\n",
    "# JAX arrays are created just like NumPy arrays but are immutable\n",
    "a = jnp.array([1.0, 2.0, 3.0])\n",
    "b = jnp.array([4.0, 5.0, 6.0])\n",
    "\n",
    "print(\"\\nüì¶ Array Creation:\")\n",
    "print(f\"Array a: {a}\")\n",
    "print(f\"Array b: {b}\")\n",
    "print(f\"Array a shape: {a.shape}, dtype: {a.dtype}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Arithmetic Operations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n‚ûï Arithmetic Operations:\")\n",
    "print(f\"Sum (a + b):                    {a + b}\")\n",
    "print(f\"Element-wise multiplication:    {a * b}\")\n",
    "print(f\"Dot product:                    {jnp.dot(a, b)}\")\n",
    "print(f\"Matrix multiplication:          {jnp.matmul(a, b)}\")  # Same as dot for 1D\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Mathematical Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìê Mathematical Functions:\")\n",
    "print(f\"Sine of a:                      {jnp.sin(a)}\")\n",
    "print(f\"Exponential of b:               {jnp.exp(b)}\")\n",
    "print(f\"Logarithm of a:                 {jnp.log(a)}\")\n",
    "print(f\"Square root of b:               {jnp.sqrt(b)}\")\n",
    "print(f\"Power (a^2):                    {jnp.power(a, 2)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Statistical Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä Statistical Functions:\")\n",
    "print(f\"Mean of a:                      {jnp.mean(a)}\")\n",
    "print(f\"Standard deviation of b:        {jnp.std(b)}\")\n",
    "print(f\"Variance of a:                  {jnp.var(a)}\")\n",
    "print(f\"Median of b:                    {jnp.median(b)}\")\n",
    "print(f\"Maximum value in a:             {jnp.max(a)}\")\n",
    "print(f\"Minimum value in b:             {jnp.min(b)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Aggregation Functions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüî¢ Aggregation Functions:\")\n",
    "print(f\"Sum of all elements in a:       {jnp.sum(a)}\")\n",
    "print(f\"Product of all elements in b:   {jnp.prod(b)}\")\n",
    "print(f\"Cumulative sum of a:            {jnp.cumsum(a)}\")\n",
    "print(f\"Cumulative product of a:        {jnp.cumprod(a)}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Array Manipulation - Reshaping\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüîÑ Array Reshaping:\")\n",
    "a_reshaped = a.reshape((3, 1))\n",
    "print(f\"Reshaped a to (3,1):\\n{a_reshaped}\")\n",
    "print(f\"Transpose of reshaped b:\\n{b.reshape((3, 1)).T}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Array Manipulation - Stacking and Concatenation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìö Stacking and Concatenation:\")\n",
    "print(f\"Vertical stack (vstack):\\n{jnp.vstack((a, b))}\")\n",
    "print(f\"Horizontal stack (hstack):\\n{jnp.hstack((a, b))}\")\n",
    "print(f\"Concatenate (same as hstack for 1D):\\n{jnp.concatenate((a, b))}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Array Query Operations\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüîç Array Query Operations:\")\n",
    "print(f\"Unique elements in b:           {jnp.unique(b)}\")\n",
    "print(f\"Sorted a (descending):          {jnp.sort(a)[::-1]}\")\n",
    "print(f\"Indices where a > 2:            {jnp.where(a > 2)}\")\n",
    "print(f\"Boolean mask (a > 2):           {a > 2}\")\n",
    "print(f\"Elements of a where a > 2:      {a[a > 2]}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Conversion Between JAX and NumPy\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüîÑ JAX ‚Üî NumPy Conversion:\")\n",
    "numpy_array = np.array(a)\n",
    "print(f\"JAX to NumPy:                   {numpy_array} (type: {type(numpy_array)})\")\n",
    "jax_array = jnp.array(numpy_array)\n",
    "print(f\"NumPy to JAX:                   {jax_array} (type: {type(jax_array)})\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# IMMUTABILITY - Key Difference from NumPy\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚ö†Ô∏è  JAX ARRAYS ARE IMMUTABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "Unlike NumPy, JAX arrays cannot be modified in place.\n",
    "Operations return NEW arrays rather than modifying existing ones.\n",
    "\n",
    "‚ùå This will FAIL:\n",
    "   a[1] = 10.0  # TypeError: JAX arrays are immutable\n",
    "\n",
    "‚úÖ Use this instead:\n",
    "   a = a.at[1].set(10.0)  # Returns a new array with index 1 set to 10.0\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate immutable update\n",
    "a_updated = a.at[1].set(10.0)\n",
    "print(f\"Original a:  {a}\")\n",
    "print(f\"Updated a:   {a_updated}\")\n",
    "print(\"Notice: Original 'a' is unchanged!\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"KEY POINTS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ JAX provides a NumPy-compatible API (jax.numpy)\n",
    "‚úÖ Most NumPy operations work identically in JAX\n",
    "‚úÖ JAX arrays are immutable - use .at[].set() for updates\n",
    "‚úÖ JAX automatically runs on GPU/TPU when available\n",
    "‚úÖ Seamlessly convert between JAX and NumPy arrays\n",
    "‚úÖ JAX is designed for high-performance numerical computing\n",
    "‚úÖ Perfect for machine learning, scientific computing, and simulations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a126adb9",
   "metadata": {},
   "source": [
    "# JIT Compilation in JAX\n",
    "\n",
    "## What is JIT Compilation?\n",
    "\n",
    "**JIT (Just-In-Time) compilation** makes your code run faster - often 10-100x faster. When you add `@jax.jit` to a function, JAX:\n",
    "1. **Traces** your function the first time it runs to understand what it does\n",
    "2. **Compiles** it to optimized machine code using XLA\n",
    "3. **Caches** the compiled version so future calls are fast\n",
    "4. **Executes** the optimized code on subsequent calls\n",
    "\n",
    "Regular Python is interpreted line-by-line. JIT-compiled code gets translated to machine instructions that run directly on your hardware. That's where the speed comes from.\n",
    "\n",
    "## Why Use JIT?\n",
    "\n",
    "- **Speed**: 10x-100x faster for numerical computations\n",
    "- **Hardware acceleration**: Automatically leverages GPUs/TPUs\n",
    "- **Optimization**: XLA fuses operations and eliminates redundant computations\n",
    "- **Parallelization**: Automatically parallelizes independent operations\n",
    "\n",
    "## When JIT Compilation FAILS or Behaves Unexpectedly\n",
    "\n",
    "### ‚ö†Ô∏è 1. DATA-DEPENDENT CONTROL FLOW\n",
    "\n",
    "This is probably the most common JIT gotcha. You can't use regular Python `if/else` statements when the condition depends on **array values**:\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS WILL FAIL OR BEHAVE INCORRECTLY:\n",
    "@jax.jit\n",
    "def bad_function(x):\n",
    "    if x > 0:  # ‚ùå Control flow depends on the VALUE of x\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "```\n",
    "\n",
    "**Why?** During tracing, JAX doesn't know the actual value of `x` - it only knows its shape and type. So it can't decide which branch to take!\n",
    "\n",
    "**Solution:** Use JAX's special control flow operations:\n",
    "- `jnp.where(condition, true_val, false_val)` - for element-wise conditionals\n",
    "- `jax.lax.cond(pred, true_fun, false_fun, operand)` - for scalar conditionals\n",
    "- `jax.lax.switch()` - for multiple branches\n",
    "- `jax.lax.select()` - for choosing between values\n",
    "\n",
    "```python\n",
    "# ‚úÖ THIS WORKS:\n",
    "@jax.jit\n",
    "def good_function(x):\n",
    "    return jnp.where(x > 0, x * 2, x * 3)  # ‚úÖ JAX-compatible conditional\n",
    "```\n",
    "\n",
    "### ‚ö†Ô∏è 2. PYTHON SIDE EFFECTS\n",
    "\n",
    "Side effects are operations that modify state outside the function or interact with the external world:\n",
    "\n",
    "```python\n",
    "# ‚ùå THESE DON'T WORK AS EXPECTED IN JIT:\n",
    "@jax.jit\n",
    "def has_side_effects(x):\n",
    "    print(f\"Value is {x}\")  # ‚ùå Print only happens during tracing!\n",
    "    global counter\n",
    "    counter += 1  # ‚ùå Modifying global state\n",
    "    my_list.append(x)  # ‚ùå Modifying external data structures\n",
    "    return x * 2\n",
    "```\n",
    "\n",
    "**Why?** JIT traces the function ONCE, then caches the compiled version. Side effects only execute during tracing, not during every call!\n",
    "\n",
    "**What happens:**\n",
    "- `print()` statements execute only the first time (during tracing)\n",
    "- Global variables are captured at trace time, not updated during execution\n",
    "- File I/O, database calls, etc. won't work as expected\n",
    "\n",
    "### ‚ö†Ô∏è 3. DATA-DEPENDENT LOOPS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def bad_loop(x):\n",
    "    for i in range(int(x)):  # ‚ùå Loop count depends on x's value\n",
    "        x = x + 1\n",
    "    return x\n",
    "```\n",
    "\n",
    "**Solution:** Use `jax.lax.fori_loop()` or `jax.lax.while_loop()` for dynamic loops.\n",
    "\n",
    "### ‚ö†Ô∏è 4. SHAPE-CHANGING OPERATIONS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def dynamic_shape(x):\n",
    "    if x.sum() > 0:\n",
    "        return x[:10]  # Shape changes based on condition\n",
    "    return x\n",
    "```\n",
    "\n",
    "JIT requires shapes to be known at compile time. Dynamic shapes break this requirement.\n",
    "\n",
    "### ‚ö†Ô∏è 5. IN-PLACE MUTATIONS\n",
    "\n",
    "```python\n",
    "# ‚ùå THIS FAILS:\n",
    "@jax.jit\n",
    "def mutate_array(x):\n",
    "    x[0] = 10  # ‚ùå JAX arrays are immutable!\n",
    "    return x\n",
    "```\n",
    "\n",
    "**Solution:** Use `.at[].set()` syntax:\n",
    "```python\n",
    "# ‚úÖ THIS WORKS:\n",
    "@jax.jit\n",
    "def update_array(x):\n",
    "    return x.at[0].set(10)  # ‚úÖ Returns new array\n",
    "```\n",
    "\n",
    "## When to Use JIT\n",
    "\n",
    "‚úÖ **USE JIT FOR:**\n",
    "- Pure functions (no side effects)\n",
    "- Functions operating on JAX arrays\n",
    "- Numerical computations (matrix operations, neural networks, simulations)\n",
    "- Functions called repeatedly with similar input shapes\n",
    "- Performance-critical code\n",
    "\n",
    "‚ùå **DON'T USE JIT FOR:**\n",
    "- Functions with print/debug statements\n",
    "- Code that modifies global state\n",
    "- Functions with Python control flow depending on array values\n",
    "- Small, one-off computations (compilation overhead > benefit)\n",
    "- Code interacting with external systems (files, databases, APIs)\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Keep functions pure**: Input ‚Üí Output, no side effects\n",
    "2. **Use JAX control flow**: `jnp.where()`, `jax.lax.cond()`, etc.\n",
    "3. **Warm up the JIT**: Run once before timing to avoid compilation overhead\n",
    "4. **Use `.block_until_ready()`**: JAX executes asynchronously by default\n",
    "5. **Inspect with `jax.make_jaxpr()`**: See the intermediate representation\n",
    "6. **Static arguments**: Use `static_argnums` for non-array arguments\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7894e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PERFORMANCE COMPARISON: WITH JIT vs WITHOUT JIT\n",
      "======================================================================\n",
      "\n",
      "üîß Warming up JIT compiler (first call triggers compilation)...\n",
      "‚úÖ JIT compilation complete! Compiled code is now cached.\n",
      "\n",
      "\n",
      "üìä SMALL ARRAY - 1,000 elements, 100 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.095370 seconds\n",
      "WITH JIT:            0.020290 seconds\n",
      "SPEEDUP:                 4.70x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "üìä MEDIUM ARRAY - 100,000 elements, 50 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.119402 seconds\n",
      "WITH JIT:            0.023932 seconds\n",
      "SPEEDUP:                 4.99x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "üìä LARGE ARRAY - 1,000,000 elements, 10 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.123240 seconds\n",
      "WITH JIT:            0.008304 seconds\n",
      "SPEEDUP:                14.84x faster\n",
      "WITHOUT JIT:         0.119402 seconds\n",
      "WITH JIT:            0.023932 seconds\n",
      "SPEEDUP:                 4.99x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "üìä LARGE ARRAY - 1,000,000 elements, 10 iterations:\n",
      "----------------------------------------------------------------------\n",
      "WITHOUT JIT:         0.123240 seconds\n",
      "WITH JIT:            0.008304 seconds\n",
      "SPEEDUP:                14.84x faster\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "======================================================================\n",
      "üìà PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "Small (1K):       4.70x speedup\n",
      "Medium (100K):    4.99x speedup\n",
      "Large (1M):      14.84x speedup\n",
      "\n",
      "‚úÖ Key Insight: JIT speedup increases with array size!\n",
      "\n",
      "======================================================================\n",
      "üîÑ UNDERSTANDING .block_until_ready()\n",
      "======================================================================\n",
      "\n",
      "JAX executes operations ASYNCHRONOUSLY by default for maximum performance.\n",
      "\n",
      "What this means:\n",
      "1. JAX queues operations and returns control to Python immediately\n",
      "2. Actual computation happens in the background on the accelerator\n",
      "3. Without .block_until_ready(), you'd measure queue time, not compute time!\n",
      "\n",
      "Example:\n",
      "    result = collatz(arr)           # Returns instantly, not done yet!\n",
      "    print(result)                   # NOW it blocks to get the value\n",
      "\n",
      "For accurate timing, ALWAYS use .block_until_ready() after the computation:\n",
      "    result = collatz(arr).block_until_ready()  # ‚úÖ Waits for completion\n",
      "\n",
      "======================================================================\n",
      "üîç JAXPR - JAX's Intermediate Representation\n",
      "======================================================================\n",
      "\n",
      "JAXPR is like assembly language for JAX. It shows the low-level operations\n",
      "that XLA compiles into machine code. Think of it as a peek under the hood!\n",
      "\n",
      "\n",
      "JAXPR for collatz_with_jit with input shape (10,):\n",
      "----------------------------------------------------------------------\n",
      "let _where = { \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[10]\u001b[39m b\u001b[35m:i32[10]\u001b[39m c\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:i32[10]\u001b[39m = select_n a c b\n",
      "  \u001b[34;1min \u001b[39;22m(d,) } in\n",
      "{ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22mf\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "      name=collatz_with_jit\n",
      "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "          \u001b[39;22mg\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=remainder\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] h\n",
      "                j\u001b[35m:bool[]\u001b[39m = eq i 0:i32[]\n",
      "                k\u001b[35m:i32[]\u001b[39m = pjit[\n",
      "                  name=_where\n",
      "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; j\u001b[35m:bool[]\u001b[39m l\u001b[35m:i32[]\u001b[39m i\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                      \u001b[39;22mk\u001b[35m:i32[]\u001b[39m = select_n j i l\n",
      "                    \u001b[34;1min \u001b[39;22m(k,) }\n",
      "                ] j 1:i32[] i\n",
      "                m\u001b[35m:i32[10]\u001b[39m = rem e k\n",
      "                n\u001b[35m:bool[10]\u001b[39m = ne m 0:i32[]\n",
      "                o\u001b[35m:bool[10]\u001b[39m = lt m 0:i32[]\n",
      "                p\u001b[35m:bool[]\u001b[39m = lt k 0:i32[]\n",
      "                q\u001b[35m:bool[10]\u001b[39m = ne o p\n",
      "                r\u001b[35m:bool[10]\u001b[39m = and q n\n",
      "                s\u001b[35m:i32[10]\u001b[39m = add m k\n",
      "                g\u001b[35m:i32[10]\u001b[39m = select_n r m s\n",
      "              \u001b[34;1min \u001b[39;22m(g,) }\n",
      "          ] e 2:i32[]\n",
      "          t\u001b[35m:bool[10]\u001b[39m = eq g 0:i32[]\n",
      "          u\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=floor_divide\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m v\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mw\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] v\n",
      "                x\u001b[35m:i32[10]\u001b[39m = div e w\n",
      "                y\u001b[35m:i32[10]\u001b[39m = sign e\n",
      "                z\u001b[35m:i32[]\u001b[39m = sign w\n",
      "                ba\u001b[35m:bool[10]\u001b[39m = ne y z\n",
      "                bb\u001b[35m:i32[10]\u001b[39m = rem e w\n",
      "                bc\u001b[35m:bool[10]\u001b[39m = ne bb 0:i32[]\n",
      "                bd\u001b[35m:bool[10]\u001b[39m = and ba bc\n",
      "                be\u001b[35m:i32[10]\u001b[39m = sub x 1:i32[]\n",
      "                u\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] bd be x\n",
      "              \u001b[34;1min \u001b[39;22m(u,) }\n",
      "          ] e 2:i32[]\n",
      "          bf\u001b[35m:i32[10]\u001b[39m = mul 3:i32[] e\n",
      "          bg\u001b[35m:i32[10]\u001b[39m = add bf 1:i32[]\n",
      "          f\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] t u bg\n",
      "        \u001b[34;1min \u001b[39;22m(f,) }\n",
      "    ] e\n",
      "  \u001b[34;1min \u001b[39;22m(f,) }\n",
      "\n",
      "What you see:\n",
      "  ‚Ä¢ Input parameters (a:i32[10]) - integer array with 10 elements\n",
      "  ‚Ä¢ Primitive operations: mod, eq, where, floordiv, mul, add\n",
      "  ‚Ä¢ Data flow through the computation\n",
      "  ‚Ä¢ This gets sent to XLA for optimization and compilation!\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  DEMONSTRATION: Why Python Control Flow Breaks JIT\n",
      "======================================================================\n",
      "\n",
      "‚ùå Testing broken_conditional with Python if/else:\n",
      "   Error: TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function broken_conditional at C:\\Users\\pndnt\\AppData\\Local\\Temp\\ipykernel_39696\\1548074232.py:145 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\n",
      "   This happens because JAX can't determine the branch during tracing!\n",
      "\n",
      "‚úÖ Testing correct_conditional with jnp.where():\n",
      "   correct_conditional(5.0)  = 10.0  ‚úì\n",
      "   correct_conditional(-5.0) = -15.0 ‚úì\n",
      "   Both results are CORRECT!\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: if/else vs jnp.where()\n",
      "======================================================================\n",
      "Input values:     [ 5. -3.  2. -8.  0.]\n",
      "jnp.where() results: [ 10.  -9.   4. -24.   0.]\n",
      "Expected (x>0 ? 2x : 3x): [10.0, -9.0, 4.0, -24.0, 0.0]\n",
      "Match: True\n",
      "\n",
      "======================================================================\n",
      "üñ®Ô∏è  DEMONSTRATION: Side Effects Only Happen During Tracing\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  First call (triggers tracing and compilation):\n",
      "   üîç TRACING: Inside function with x = Traced<~float32[]>with<DynamicJaxprTrace>\n",
      "   Result: 20.0\n",
      "\n",
      "2Ô∏è‚É£  Second call (uses cached compiled version):\n",
      "   Result: 40.0\n",
      "   üëÜ Notice: The print INSIDE the function didn't execute!\n",
      "\n",
      "3Ô∏è‚É£  Third call (still using cached version):\n",
      "   Result: 60.0\n",
      "   üëÜ Still no print - using cached compilation\n",
      "\n",
      "üí° Key Point: Side effects (print, global vars, I/O) only happen once!\n",
      "\n",
      "======================================================================\n",
      "‚öñÔ∏è  DEMONSTRATION: JIT Overhead vs Benefit\n",
      "======================================================================\n",
      "\n",
      "üìâ TINY ARRAYS (3 elements, 1000 iterations):\n",
      "  WITHOUT JIT: 0.007308 seconds\n",
      "  WITH JIT:    0.003635 seconds\n",
      "  Speedup:     2.01x\n",
      "\n",
      "üìà LARGE ARRAYS (1M elements, 100 iterations):\n",
      "‚úÖ Results match! First 5 values: [ 4  1 10  2 16]\n",
      "\n",
      "======================================================================\n",
      "üìà PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "Small (1K):       4.70x speedup\n",
      "Medium (100K):    4.99x speedup\n",
      "Large (1M):      14.84x speedup\n",
      "\n",
      "‚úÖ Key Insight: JIT speedup increases with array size!\n",
      "\n",
      "======================================================================\n",
      "üîÑ UNDERSTANDING .block_until_ready()\n",
      "======================================================================\n",
      "\n",
      "JAX executes operations ASYNCHRONOUSLY by default for maximum performance.\n",
      "\n",
      "What this means:\n",
      "1. JAX queues operations and returns control to Python immediately\n",
      "2. Actual computation happens in the background on the accelerator\n",
      "3. Without .block_until_ready(), you'd measure queue time, not compute time!\n",
      "\n",
      "Example:\n",
      "    result = collatz(arr)           # Returns instantly, not done yet!\n",
      "    print(result)                   # NOW it blocks to get the value\n",
      "\n",
      "For accurate timing, ALWAYS use .block_until_ready() after the computation:\n",
      "    result = collatz(arr).block_until_ready()  # ‚úÖ Waits for completion\n",
      "\n",
      "======================================================================\n",
      "üîç JAXPR - JAX's Intermediate Representation\n",
      "======================================================================\n",
      "\n",
      "JAXPR is like assembly language for JAX. It shows the low-level operations\n",
      "that XLA compiles into machine code. Think of it as a peek under the hood!\n",
      "\n",
      "\n",
      "JAXPR for collatz_with_jit with input shape (10,):\n",
      "----------------------------------------------------------------------\n",
      "let _where = { \u001b[34;1mlambda \u001b[39;22m; a\u001b[35m:bool[10]\u001b[39m b\u001b[35m:i32[10]\u001b[39m c\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22md\u001b[35m:i32[10]\u001b[39m = select_n a c b\n",
      "  \u001b[34;1min \u001b[39;22m(d,) } in\n",
      "{ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "    \u001b[39;22mf\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "      name=collatz_with_jit\n",
      "      jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m. \u001b[34;1mlet\n",
      "          \u001b[39;22mg\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=remainder\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m h\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mi\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] h\n",
      "                j\u001b[35m:bool[]\u001b[39m = eq i 0:i32[]\n",
      "                k\u001b[35m:i32[]\u001b[39m = pjit[\n",
      "                  name=_where\n",
      "                  jaxpr={ \u001b[34;1mlambda \u001b[39;22m; j\u001b[35m:bool[]\u001b[39m l\u001b[35m:i32[]\u001b[39m i\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                      \u001b[39;22mk\u001b[35m:i32[]\u001b[39m = select_n j i l\n",
      "                    \u001b[34;1min \u001b[39;22m(k,) }\n",
      "                ] j 1:i32[] i\n",
      "                m\u001b[35m:i32[10]\u001b[39m = rem e k\n",
      "                n\u001b[35m:bool[10]\u001b[39m = ne m 0:i32[]\n",
      "                o\u001b[35m:bool[10]\u001b[39m = lt m 0:i32[]\n",
      "                p\u001b[35m:bool[]\u001b[39m = lt k 0:i32[]\n",
      "                q\u001b[35m:bool[10]\u001b[39m = ne o p\n",
      "                r\u001b[35m:bool[10]\u001b[39m = and q n\n",
      "                s\u001b[35m:i32[10]\u001b[39m = add m k\n",
      "                g\u001b[35m:i32[10]\u001b[39m = select_n r m s\n",
      "              \u001b[34;1min \u001b[39;22m(g,) }\n",
      "          ] e 2:i32[]\n",
      "          t\u001b[35m:bool[10]\u001b[39m = eq g 0:i32[]\n",
      "          u\u001b[35m:i32[10]\u001b[39m = pjit[\n",
      "            name=floor_divide\n",
      "            jaxpr={ \u001b[34;1mlambda \u001b[39;22m; e\u001b[35m:i32[10]\u001b[39m v\u001b[35m:i32[]\u001b[39m. \u001b[34;1mlet\n",
      "                \u001b[39;22mw\u001b[35m:i32[]\u001b[39m = convert_element_type[new_dtype=int32 weak_type=False] v\n",
      "                x\u001b[35m:i32[10]\u001b[39m = div e w\n",
      "                y\u001b[35m:i32[10]\u001b[39m = sign e\n",
      "                z\u001b[35m:i32[]\u001b[39m = sign w\n",
      "                ba\u001b[35m:bool[10]\u001b[39m = ne y z\n",
      "                bb\u001b[35m:i32[10]\u001b[39m = rem e w\n",
      "                bc\u001b[35m:bool[10]\u001b[39m = ne bb 0:i32[]\n",
      "                bd\u001b[35m:bool[10]\u001b[39m = and ba bc\n",
      "                be\u001b[35m:i32[10]\u001b[39m = sub x 1:i32[]\n",
      "                u\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] bd be x\n",
      "              \u001b[34;1min \u001b[39;22m(u,) }\n",
      "          ] e 2:i32[]\n",
      "          bf\u001b[35m:i32[10]\u001b[39m = mul 3:i32[] e\n",
      "          bg\u001b[35m:i32[10]\u001b[39m = add bf 1:i32[]\n",
      "          f\u001b[35m:i32[10]\u001b[39m = pjit[name=_where jaxpr=_where] t u bg\n",
      "        \u001b[34;1min \u001b[39;22m(f,) }\n",
      "    ] e\n",
      "  \u001b[34;1min \u001b[39;22m(f,) }\n",
      "\n",
      "What you see:\n",
      "  ‚Ä¢ Input parameters (a:i32[10]) - integer array with 10 elements\n",
      "  ‚Ä¢ Primitive operations: mod, eq, where, floordiv, mul, add\n",
      "  ‚Ä¢ Data flow through the computation\n",
      "  ‚Ä¢ This gets sent to XLA for optimization and compilation!\n",
      "\n",
      "======================================================================\n",
      "‚ö†Ô∏è  DEMONSTRATION: Why Python Control Flow Breaks JIT\n",
      "======================================================================\n",
      "\n",
      "‚ùå Testing broken_conditional with Python if/else:\n",
      "   Error: TracerBoolConversionError: Attempted boolean conversion of traced array with shape bool[].\n",
      "The error occurred while tracing the function broken_conditional at C:\\Users\\pndnt\\AppData\\Local\\Temp\\ipykernel_39696\\1548074232.py:145 for jit. This concrete value was not available in Python because it depends on the value of the argument x.\n",
      "See https://docs.jax.dev/en/latest/errors.html#jax.errors.TracerBoolConversionError\n",
      "   This happens because JAX can't determine the branch during tracing!\n",
      "\n",
      "‚úÖ Testing correct_conditional with jnp.where():\n",
      "   correct_conditional(5.0)  = 10.0  ‚úì\n",
      "   correct_conditional(-5.0) = -15.0 ‚úì\n",
      "   Both results are CORRECT!\n",
      "\n",
      "======================================================================\n",
      "COMPARISON: if/else vs jnp.where()\n",
      "======================================================================\n",
      "Input values:     [ 5. -3.  2. -8.  0.]\n",
      "jnp.where() results: [ 10.  -9.   4. -24.   0.]\n",
      "Expected (x>0 ? 2x : 3x): [10.0, -9.0, 4.0, -24.0, 0.0]\n",
      "Match: True\n",
      "\n",
      "======================================================================\n",
      "üñ®Ô∏è  DEMONSTRATION: Side Effects Only Happen During Tracing\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  First call (triggers tracing and compilation):\n",
      "   üîç TRACING: Inside function with x = Traced<~float32[]>with<DynamicJaxprTrace>\n",
      "   Result: 20.0\n",
      "\n",
      "2Ô∏è‚É£  Second call (uses cached compiled version):\n",
      "   Result: 40.0\n",
      "   üëÜ Notice: The print INSIDE the function didn't execute!\n",
      "\n",
      "3Ô∏è‚É£  Third call (still using cached version):\n",
      "   Result: 60.0\n",
      "   üëÜ Still no print - using cached compilation\n",
      "\n",
      "üí° Key Point: Side effects (print, global vars, I/O) only happen once!\n",
      "\n",
      "======================================================================\n",
      "‚öñÔ∏è  DEMONSTRATION: JIT Overhead vs Benefit\n",
      "======================================================================\n",
      "\n",
      "üìâ TINY ARRAYS (3 elements, 1000 iterations):\n",
      "  WITHOUT JIT: 0.007308 seconds\n",
      "  WITH JIT:    0.003635 seconds\n",
      "  Speedup:     2.01x\n",
      "\n",
      "üìà LARGE ARRAYS (1M elements, 100 iterations):\n",
      "  WITHOUT JIT: 0.038374 seconds\n",
      "  WITH JIT:    0.058674 seconds\n",
      "  Speedup:     0.65x\n",
      "  ‚úÖ JIT provides significant benefit for large computations!\n",
      "\n",
      "======================================================================\n",
      "üéØ KEY TAKEAWAYS - JIT COMPILATION\n",
      "======================================================================\n",
      "\n",
      "1. ‚úÖ JIT provides 2x-100x speedup for large numerical computations\n",
      "2. ‚úÖ Use jnp.where() for conditionals, NOT Python if/else with array values\n",
      "3. ‚úÖ Pure functions work best (no side effects like print, globals, I/O)\n",
      "4. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
      "5. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
      "6. ‚úÖ JIT benefit increases with array size and computation complexity\n",
      "7. ‚úÖ Inspect with jax.make_jaxpr() to see the compiled representation\n",
      "8. ‚ùå Python control flow (if/else) that depends on array VALUES breaks JIT\n",
      "9. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
      "10. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
      "\n",
      "üí° Best Use Cases: Matrix operations, neural networks, scientific simulations,\n",
      "   repeated computations on large arrays\n",
      "\n",
      "  WITHOUT JIT: 0.038374 seconds\n",
      "  WITH JIT:    0.058674 seconds\n",
      "  Speedup:     0.65x\n",
      "  ‚úÖ JIT provides significant benefit for large computations!\n",
      "\n",
      "======================================================================\n",
      "üéØ KEY TAKEAWAYS - JIT COMPILATION\n",
      "======================================================================\n",
      "\n",
      "1. ‚úÖ JIT provides 2x-100x speedup for large numerical computations\n",
      "2. ‚úÖ Use jnp.where() for conditionals, NOT Python if/else with array values\n",
      "3. ‚úÖ Pure functions work best (no side effects like print, globals, I/O)\n",
      "4. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
      "5. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
      "6. ‚úÖ JIT benefit increases with array size and computation complexity\n",
      "7. ‚úÖ Inspect with jax.make_jaxpr() to see the compiled representation\n",
      "8. ‚ùå Python control flow (if/else) that depends on array VALUES breaks JIT\n",
      "9. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
      "10. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
      "\n",
      "üí° Best Use Cases: Matrix operations, neural networks, scientific simulations,\n",
      "   repeated computations on large arrays\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# JIT COMPILATION - PRACTICAL EXAMPLES\n",
    "# =============================================================================\n",
    "\n",
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 1: JIT vs Non-JIT Performance Comparison\n",
    "# -----------------------------------------------------------------------------\n",
    "# The Collatz conjecture: Take any positive integer n. If n is even, divide it\n",
    "# by 2. If n is odd, multiply it by 3 and add 1. Repeat the process.\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PERFORMANCE COMPARISON: WITH JIT vs WITHOUT JIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# VERSION 1: WITHOUT JIT\n",
    "def collatz_no_jit(x):\n",
    "    \"\"\"\n",
    "    Collatz step WITHOUT JIT compilation.\n",
    "    Uses jnp.where() for vectorized conditional logic.\n",
    "    \"\"\"\n",
    "    return jnp.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# VERSION 2: WITH JIT\n",
    "@jax.jit\n",
    "def collatz_with_jit(x):\n",
    "    \"\"\"\n",
    "    Collatz step WITH JIT compilation.\n",
    "    Same logic, but decorated with @jax.jit for optimization.\n",
    "    \"\"\"\n",
    "    return jnp.where(x % 2 == 0, x // 2, 3 * x + 1)\n",
    "\n",
    "# Create test arrays of different sizes\n",
    "small_arr = jnp.arange(1, 1001)        # 1K elements\n",
    "medium_arr = jnp.arange(1, 100001)     # 100K elements\n",
    "large_arr = jnp.arange(1, 1000001)     # 1M elements\n",
    "\n",
    "print(\"\\nüîß Warming up JIT compiler (first call triggers compilation)...\")\n",
    "_ = collatz_with_jit(large_arr).block_until_ready()\n",
    "print(\"‚úÖ JIT compilation complete! Compiled code is now cached.\\n\")\n",
    "\n",
    "# Test function for timing\n",
    "def benchmark_comparison(arr, label, iterations=10):\n",
    "    \"\"\"Benchmark both versions and compare results.\"\"\"\n",
    "    print(f\"\\nüìä {label} - {len(arr):,} elements, {iterations} iterations:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Benchmark WITHOUT JIT\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result_no_jit = collatz_no_jit(arr).block_until_ready()\n",
    "    time_no_jit = time.time() - start\n",
    "    \n",
    "    # Benchmark WITH JIT\n",
    "    start = time.time()\n",
    "    for _ in range(iterations):\n",
    "        result_with_jit = collatz_with_jit(arr).block_until_ready()\n",
    "    time_with_jit = time.time() - start\n",
    "    \n",
    "    # Calculate speedup\n",
    "    speedup = time_no_jit / time_with_jit if time_with_jit > 0 else float('inf')\n",
    "    \n",
    "    # Display results side-by-side\n",
    "    print(f\"{'WITHOUT JIT:':20} {time_no_jit:8.6f} seconds\")\n",
    "    print(f\"{'WITH JIT:':20} {time_with_jit:8.6f} seconds\")\n",
    "    print(f\"{'SPEEDUP:':20} {speedup:8.2f}x faster\")\n",
    "    \n",
    "    # Verify results match\n",
    "    if jnp.allclose(result_no_jit, result_with_jit):\n",
    "        print(f\"‚úÖ Results match! First 5 values: {result_with_jit[:5]}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Warning: Results differ!\")\n",
    "    \n",
    "    return speedup\n",
    "\n",
    "# Run benchmarks for different array sizes\n",
    "speedup_small = benchmark_comparison(small_arr, \"SMALL ARRAY\", iterations=100)\n",
    "speedup_medium = benchmark_comparison(medium_arr, \"MEDIUM ARRAY\", iterations=50)\n",
    "speedup_large = benchmark_comparison(large_arr, \"LARGE ARRAY\", iterations=10)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìà PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Small (1K):     {speedup_small:6.2f}x speedup\")\n",
    "print(f\"Medium (100K):  {speedup_medium:6.2f}x speedup\")\n",
    "print(f\"Large (1M):     {speedup_large:6.2f}x speedup\")\n",
    "print(\"\\n‚úÖ Key Insight: JIT speedup increases with array size!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING ASYNCHRONOUS EXECUTION\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîÑ UNDERSTANDING .block_until_ready()\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "JAX executes operations ASYNCHRONOUSLY by default for maximum performance.\n",
    "\n",
    "What this means:\n",
    "1. JAX queues operations and returns control to Python immediately\n",
    "2. Actual computation happens in the background on the accelerator\n",
    "3. Without .block_until_ready(), you'd measure queue time, not compute time!\n",
    "\n",
    "Example:\n",
    "    result = collatz(arr)           # Returns instantly, not done yet!\n",
    "    print(result)                   # NOW it blocks to get the value\n",
    "\n",
    "For accurate timing, ALWAYS use .block_until_ready() after the computation:\n",
    "    result = collatz(arr).block_until_ready()  # ‚úÖ Waits for completion\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# UNDERSTANDING JAXPR - JAX's Intermediate Representation\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"üîç JAXPR - JAX's Intermediate Representation\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "JAXPR is like assembly language for JAX. It shows the low-level operations\n",
    "that XLA compiles into machine code. Think of it as a peek under the hood!\n",
    "\"\"\")\n",
    "\n",
    "sample_arr = jnp.arange(1, 11)\n",
    "print(f\"\\nJAXPR for collatz_with_jit with input shape {sample_arr.shape}:\")\n",
    "print(\"-\" * 70)\n",
    "print(jax.make_jaxpr(collatz_with_jit)(sample_arr))\n",
    "print()\n",
    "\n",
    "print(\"What you see:\")\n",
    "print(\"  ‚Ä¢ Input parameters (a:i32[10]) - integer array with 10 elements\")\n",
    "print(\"  ‚Ä¢ Primitive operations: mod, eq, where, floordiv, mul, add\")\n",
    "print(\"  ‚Ä¢ Data flow through the computation\")\n",
    "print(\"  ‚Ä¢ This gets sent to XLA for optimization and compilation!\\n\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 2: Why Python if/else Fails with JIT\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(\"‚ö†Ô∏è  DEMONSTRATION: Why Python Control Flow Breaks JIT\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ‚ùå INCORRECT: Using Python if/else with array values\n",
    "@jax.jit\n",
    "def broken_conditional(x):\n",
    "    \"\"\"\n",
    "    This will behave INCORRECTLY when JIT-compiled!\n",
    "    During tracing, JAX doesn't know x's value, only its shape/type.\n",
    "    It picks ONE branch and always uses that branch.\n",
    "    \"\"\"\n",
    "    if x > 0:  # ‚ùå Compares abstract tracer, not actual value\n",
    "        return x * 2\n",
    "    else:\n",
    "        return x * 3\n",
    "\n",
    "print(\"\\n‚ùå Testing broken_conditional with Python if/else:\")\n",
    "try:\n",
    "    result_pos = broken_conditional(jnp.array(5.0))\n",
    "    print(f\"   broken_conditional(5.0)  = {result_pos}\")\n",
    "    \n",
    "    result_neg = broken_conditional(jnp.array(-5.0))\n",
    "    print(f\"   broken_conditional(-5.0) = {result_neg}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  PROBLEM: Both give same result! Only one branch was compiled.\")\n",
    "    print(f\"   Expected: 10.0 and -15.0, but got {result_pos} and {result_neg}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Error: {type(e).__name__}: {e}\")\n",
    "    print(f\"   This happens because JAX can't determine the branch during tracing!\")\n",
    "\n",
    "# ‚úÖ CORRECT: Using JAX-compatible control flow\n",
    "@jax.jit\n",
    "def correct_conditional(x):\n",
    "    \"\"\"\n",
    "    Correct version using jnp.where().\n",
    "    Evaluates BOTH branches and selects based on condition.\n",
    "    Works with JIT because no Python control flow is needed!\n",
    "    \"\"\"\n",
    "    return jnp.where(x > 0, x * 2, x * 3)\n",
    "\n",
    "print(\"\\n‚úÖ Testing correct_conditional with jnp.where():\")\n",
    "result_pos = correct_conditional(jnp.array(5.0))\n",
    "result_neg = correct_conditional(jnp.array(-5.0))\n",
    "print(f\"   correct_conditional(5.0)  = {result_pos}  ‚úì\")\n",
    "print(f\"   correct_conditional(-5.0) = {result_neg} ‚úì\")\n",
    "print(f\"   Both results are CORRECT!\")\n",
    "\n",
    "# Side-by-side comparison\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON: if/else vs jnp.where()\")\n",
    "print(\"=\" * 70)\n",
    "test_values = jnp.array([5.0, -3.0, 2.0, -8.0, 0.0])\n",
    "correct_results = correct_conditional(test_values)\n",
    "print(f\"Input values:     {test_values}\")\n",
    "print(f\"jnp.where() results: {correct_results}\")\n",
    "print(f\"Expected (x>0 ? 2x : 3x): [10.0, -9.0, 4.0, -24.0, 0.0]\")\n",
    "print(f\"Match: {jnp.allclose(correct_results, jnp.array([10.0, -9.0, 4.0, -24.0, 0.0]))}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 3: Side Effects in JIT - Print Statements\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üñ®Ô∏è  DEMONSTRATION: Side Effects Only Happen During Tracing\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "@jax.jit\n",
    "def function_with_print(x):\n",
    "    \"\"\"\n",
    "    Print statements only execute during FIRST call (tracing phase).\n",
    "    Subsequent calls use cached compiled code without prints!\n",
    "    \"\"\"\n",
    "    print(f\"   üîç TRACING: Inside function with x = {x}\")\n",
    "    return x * 2\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  First call (triggers tracing and compilation):\")\n",
    "result1 = function_with_print(jnp.array(10.0))\n",
    "print(f\"   Result: {result1}\\n\")\n",
    "\n",
    "print(\"2Ô∏è‚É£  Second call (uses cached compiled version):\")\n",
    "result2 = function_with_print(jnp.array(20.0))\n",
    "print(f\"   Result: {result2}\")\n",
    "print(f\"   üëÜ Notice: The print INSIDE the function didn't execute!\\n\")\n",
    "\n",
    "print(\"3Ô∏è‚É£  Third call (still using cached version):\")\n",
    "result3 = function_with_print(jnp.array(30.0))\n",
    "print(f\"   Result: {result3}\")\n",
    "print(f\"   üëÜ Still no print - using cached compilation\\n\")\n",
    "\n",
    "print(\"üí° Key Point: Side effects (print, global vars, I/O) only happen once!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# EXAMPLE 4: When NOT to Use JIT - Compilation Overhead\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚öñÔ∏è  DEMONSTRATION: JIT Overhead vs Benefit\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def simple_add_no_jit(x):\n",
    "    \"\"\"Simple addition without JIT.\"\"\"\n",
    "    return x + 1\n",
    "\n",
    "@jax.jit\n",
    "def simple_add_with_jit(x):\n",
    "    \"\"\"Simple addition with JIT.\"\"\"\n",
    "    return x + 1\n",
    "\n",
    "# SMALL COMPUTATION - JIT overhead dominates\n",
    "print(\"\\nüìâ TINY ARRAYS (3 elements, 1000 iterations):\")\n",
    "small_arr = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_no_jit(small_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "_ = simple_add_with_jit(small_arr).block_until_ready()  # Warm up\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = simple_add_with_jit(small_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"  WITHOUT JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  WITH JIT:    {time_with_jit:.6f} seconds\")\n",
    "speedup = time_no_jit / time_with_jit\n",
    "print(f\"  Speedup:     {speedup:.2f}x\")\n",
    "if speedup < 1.5:\n",
    "    print(f\"  ‚ö†Ô∏è  JIT overhead isn't worth it for tiny computations!\")\n",
    "\n",
    "# LARGE COMPUTATION - JIT benefit is clear\n",
    "print(\"\\nüìà LARGE ARRAYS (1M elements, 100 iterations):\")\n",
    "large_arr = jnp.arange(1000000.0)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_no_jit(large_arr)\n",
    "time_no_jit = time.time() - start\n",
    "\n",
    "_ = simple_add_with_jit(large_arr).block_until_ready()  # Warm up\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = simple_add_with_jit(large_arr).block_until_ready()\n",
    "time_with_jit = time.time() - start\n",
    "\n",
    "print(f\"  WITHOUT JIT: {time_no_jit:.6f} seconds\")\n",
    "print(f\"  WITH JIT:    {time_with_jit:.6f} seconds\")\n",
    "print(f\"  Speedup:     {time_no_jit/time_with_jit:.2f}x\")\n",
    "print(f\"  ‚úÖ JIT provides significant benefit for large computations!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üéØ KEY TAKEAWAYS - JIT COMPILATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. ‚úÖ JIT provides 2x-100x speedup for large numerical computations\n",
    "2. ‚úÖ Use jnp.where() for conditionals, NOT Python if/else with array values\n",
    "3. ‚úÖ Pure functions work best (no side effects like print, globals, I/O)\n",
    "4. ‚úÖ Warm up JIT before timing (first call includes compilation overhead)\n",
    "5. ‚úÖ Use .block_until_ready() for accurate timing (JAX is async by default)\n",
    "6. ‚úÖ JIT benefit increases with array size and computation complexity\n",
    "7. ‚úÖ Inspect with jax.make_jaxpr() to see the compiled representation\n",
    "8. ‚ùå Python control flow (if/else) that depends on array VALUES breaks JIT\n",
    "9. ‚ùå Side effects (print, globals, I/O) only happen during tracing\n",
    "10. ‚ùå Don't JIT tiny functions - compilation overhead isn't worth it\n",
    "\n",
    "üí° Best Use Cases: Matrix operations, neural networks, scientific simulations,\n",
    "   repeated computations on large arrays\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c7b93",
   "metadata": {},
   "source": [
    "## Quick Reference: JIT-Compatible Control Flow\n",
    "\n",
    "When you need conditionals in JIT-compiled functions, use these JAX operations:\n",
    "\n",
    "| Scenario | ‚ùå Don't Use | ‚úÖ Use Instead |\n",
    "|----------|--------------|----------------|\n",
    "| Element-wise conditional | `if x > 0: ...` | `jnp.where(x > 0, true_val, false_val)` |\n",
    "| Scalar conditional | `if x > 0: ...` | `jax.lax.cond(x > 0, true_fn, false_fn, operand)` |\n",
    "| Multiple branches | `if/elif/else` | `jax.lax.switch(index, branches, operand)` |\n",
    "| Dynamic loops | `for i in range(int(x)): ...` | `jax.lax.fori_loop(start, end, body_fn, init)` |\n",
    "| While loops | `while condition: ...` | `jax.lax.while_loop(cond_fn, body_fn, init)` |\n",
    "| Array updates | `arr[i] = val` | `arr.at[i].set(val)` |\n",
    "\n",
    "**Why?** During JIT tracing, JAX works with abstract values (shapes/types), not actual data. Python control flow needs concrete values, which aren't available during tracing. JAX's control flow ops are designed to work with abstract values!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffc387",
   "metadata": {},
   "source": [
    "# Automatic Differentiation in JAX\n",
    "\n",
    "Automatic differentiation (autodiff) lets JAX compute derivatives of any function. You write the forward pass, JAX figures out the gradients. This is what makes training neural networks possible without deriving backprop by hand.\n",
    "\n",
    "## Key Functions\n",
    "\n",
    "- **`jax.grad()`** - Gradient of scalar output w.r.t. first argument\n",
    "- **`jax.value_and_grad()`** - Returns both function value and gradient\n",
    "- **`jax.jacobian()`** - Jacobian matrix for vector-valued functions\n",
    "- **`jax.hessian()`** - Hessian matrix (second derivatives)\n",
    "\n",
    "## How It Works\n",
    "\n",
    "JAX uses **reverse-mode autodiff** (backpropagation) by default. It traces your function during execution and builds a computation graph, then walks backward through it to compute derivatives.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c931ff8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "COMPUTING GRADIENTS WITH JAX\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  BASIC GRADIENT\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = x^2\n",
      "f(3.0) = 9.0\n",
      "f'(3.0) = 6.0\n",
      "Expected: 2*3.0 = 6.0 ‚úì\n",
      "\n",
      "2Ô∏è‚É£  MULTI-VARIABLE GRADIENT\n",
      "----------------------------------------------------------------------\n",
      "Function: g(x,y) = x^2 + 3xy + y^2\n",
      "g(2.0, 1.0) = 11.0\n",
      "‚àÇg/‚àÇx at (2.0,1.0) = 7.0\n",
      "‚àÇg/‚àÇy at (2.0,1.0) = 8.0\n",
      "Expected: ‚àÇg/‚àÇx = 2x + 3y = 7.0 ‚úì\n",
      "Expected: ‚àÇg/‚àÇy = 3x + 2y = 8.0 ‚úì\n",
      "\n",
      "3Ô∏è‚É£  VALUE AND GRADIENT TOGETHER\n",
      "----------------------------------------------------------------------\n",
      "Parameters: [1. 2. 3.]\n",
      "Loss value: 14.0\n",
      "Gradient: [2. 4. 6.]\n",
      "Expected gradient: 2*params = [2. 4. 6.] ‚úì\n",
      "\n",
      "4Ô∏è‚É£  JACOBIAN (for vector-valued functions)\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = [x^2, x^3, sin(x)]\n",
      "Jacobian at x=2.0:\n",
      "  df‚ÇÅ/dx = 2x = 4.0000\n",
      "  df‚ÇÇ/dx = 3x^2 = 12.0000\n",
      "  df‚ÇÉ/dx = cos(x) = -0.4161\n",
      "\n",
      "5Ô∏è‚É£  HESSIAN (second derivatives)\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x,y) = x^2 + y^2\n",
      "Hessian matrix at [1. 2.]:\n",
      "[[2. 0.]\n",
      " [0. 2.]]\n",
      "(Second derivatives: diagonal is 2, off-diagonal is 0)\n",
      "\n",
      "6Ô∏è‚É£  HIGHER-ORDER DERIVATIVES\n",
      "----------------------------------------------------------------------\n",
      "Function: f(x) = x^3\n",
      "f(2.0) = 8.0\n",
      "f'(2.0) = 12.0 (expected: 3x^2 = 12.0)\n",
      "f''(2.0) = 12.0 (expected: 6x = 12.0)\n",
      "f'''(2.0) = 6.0 (expected: 6)\n",
      "\n",
      "7Ô∏è‚É£  GRADIENTS W.R.T. MULTIPLE ARGUMENTS\n",
      "----------------------------------------------------------------------\n",
      "Model: y = w¬∑x + b\n",
      "Gradient w.r.t. weights: [1. 1. 1.]\n",
      "Gradient w.r.t. bias: 1.0\n",
      "(dL/dw = x, dL/db = 1)\n",
      "\n",
      "8Ô∏è‚É£  GRADIENT DESCENT OPTIMIZATION\n",
      "----------------------------------------------------------------------\n",
      "Training linear regression with gradient descent...\n",
      "True parameters: slope=2.0, intercept=1.0\n",
      "Initial params: [0. 0.]\n",
      "Step   0: loss=14.2483, params=[1.4424324 0.218287 ]\n",
      "Step  20: loss=0.3895, params=[2.0481887 0.4179262]\n",
      "Step  40: loss=0.3413, params=[2.0338674 0.5151596]\n",
      "Step  60: loss=0.3023, params=[2.0209825 0.6026397]\n",
      "Step  80: loss=0.2707, params=[2.00939   0.6813449]\n",
      "\n",
      "Final params: slope=1.999, intercept=0.749\n",
      "Final loss: 0.2463\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS - AUTOMATIC DIFFERENTIATION\n",
      "======================================================================\n",
      "\n",
      "‚úÖ jax.grad() computes exact derivatives (not numerical approximations)\n",
      "‚úÖ Works with any Python/JAX function - no manual chain rule needed\n",
      "‚úÖ Efficient reverse-mode autodiff (backpropagation)\n",
      "‚úÖ Can compute higher-order derivatives by composing grad()\n",
      "‚úÖ value_and_grad() gives both function value and gradient\n",
      "‚úÖ Jacobian and Hessian for vector functions and second derivatives\n",
      "‚úÖ argnums parameter controls which arguments to differentiate\n",
      "‚úÖ Combine with JIT for ultra-fast gradient computations\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# AUTOMATIC DIFFERENTIATION - BASICS\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"COMPUTING GRADIENTS WITH JAX\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 1: Basic Gradient - Single Variable\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£  BASIC GRADIENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Simple quadratic function: f(x) = x^2\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "# Create gradient function\n",
    "df_dx = jax.grad(f)\n",
    "\n",
    "# Evaluate at different points\n",
    "x = 3.0\n",
    "print(f\"Function: f(x) = x^2\")\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {df_dx(x)}\")\n",
    "print(f\"Expected: 2*{x} = {2*x} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 2: Multi-variable Function\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£  MULTI-VARIABLE GRADIENT\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def g(x, y):\n",
    "    \"\"\"Function with two inputs: g(x,y) = x^2 + 3xy + y^2\"\"\"\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# grad() computes gradient w.r.t. FIRST argument by default\n",
    "dg_dx = jax.grad(g)\n",
    "\n",
    "# To get gradient w.r.t. other arguments, use argnums\n",
    "dg_dy = jax.grad(g, argnums=1)\n",
    "\n",
    "x, y = 2.0, 1.0\n",
    "print(f\"Function: g(x,y) = x^2 + 3xy + y^2\")\n",
    "print(f\"g({x}, {y}) = {g(x, y)}\")\n",
    "print(f\"‚àÇg/‚àÇx at ({x},{y}) = {dg_dx(x, y)}\")\n",
    "print(f\"‚àÇg/‚àÇy at ({x},{y}) = {dg_dy(x, y)}\")\n",
    "print(f\"Expected: ‚àÇg/‚àÇx = 2x + 3y = {2*x + 3*y} ‚úì\")\n",
    "print(f\"Expected: ‚àÇg/‚àÇy = 3x + 2y = {3*x + 2*y} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 3: value_and_grad - Get Both Function Value and Gradient\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£  VALUE AND GRADIENT TOGETHER\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss(params):\n",
    "    \"\"\"Typical ML loss function\"\"\"\n",
    "    return jnp.sum(params ** 2)\n",
    "\n",
    "# This is super useful for optimization - get loss and gradient in one call\n",
    "val_and_grad_fn = jax.value_and_grad(loss)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "value, gradient = val_and_grad_fn(params)\n",
    "\n",
    "print(f\"Parameters: {params}\")\n",
    "print(f\"Loss value: {value}\")\n",
    "print(f\"Gradient: {gradient}\")\n",
    "print(f\"Expected gradient: 2*params = {2*params} ‚úì\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 4: Gradients of Vector Functions - Jacobian\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£  JACOBIAN (for vector-valued functions)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def vector_func(x):\n",
    "    \"\"\"Returns a vector: [x^2, x^3, sin(x)]\"\"\"\n",
    "    return jnp.array([x**2, x**3, jnp.sin(x)])\n",
    "\n",
    "# Jacobian computes all partial derivatives\n",
    "jacobian_fn = jax.jacobian(vector_func)\n",
    "\n",
    "x = 2.0\n",
    "jac = jacobian_fn(x)\n",
    "print(f\"Function: f(x) = [x^2, x^3, sin(x)]\")\n",
    "print(f\"Jacobian at x={x}:\")\n",
    "print(f\"  df‚ÇÅ/dx = 2x = {jac[0]:.4f}\")\n",
    "print(f\"  df‚ÇÇ/dx = 3x^2 = {jac[1]:.4f}\")\n",
    "print(f\"  df‚ÇÉ/dx = cos(x) = {jac[2]:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 5: Second Derivatives - Hessian\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£  HESSIAN (second derivatives)\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def h(x):\n",
    "    \"\"\"Multi-variable function for Hessian demo\"\"\"\n",
    "    return jnp.array([x[0]**2 + x[1]**2, x[0]*x[1]])\n",
    "\n",
    "# Hessian: matrix of second partial derivatives\n",
    "hessian_fn = jax.hessian(lambda x: x[0]**2 + x[1]**2)\n",
    "\n",
    "x = jnp.array([1.0, 2.0])\n",
    "hess = hessian_fn(x)\n",
    "print(f\"Function: f(x,y) = x^2 + y^2\")\n",
    "print(f\"Hessian matrix at {x}:\")\n",
    "print(hess)\n",
    "print(\"(Second derivatives: diagonal is 2, off-diagonal is 0)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 6: Higher-Order Derivatives\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£  HIGHER-ORDER DERIVATIVES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def cubic(x):\n",
    "    \"\"\"f(x) = x^3\"\"\"\n",
    "    return x ** 3\n",
    "\n",
    "# First derivative\n",
    "first_deriv = jax.grad(cubic)\n",
    "# Second derivative (gradient of gradient)\n",
    "second_deriv = jax.grad(first_deriv)\n",
    "# Third derivative\n",
    "third_deriv = jax.grad(second_deriv)\n",
    "\n",
    "x = 2.0\n",
    "print(f\"Function: f(x) = x^3\")\n",
    "print(f\"f({x}) = {cubic(x)}\")\n",
    "print(f\"f'({x}) = {first_deriv(x)} (expected: 3x^2 = {3*x**2})\")\n",
    "print(f\"f''({x}) = {second_deriv(x)} (expected: 6x = {6*x})\")\n",
    "print(f\"f'''({x}) = {third_deriv(x)} (expected: 6)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 7: Gradient with Respect to Multiple Arguments\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£  GRADIENTS W.R.T. MULTIPLE ARGUMENTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def model(weights, bias, x):\n",
    "    \"\"\"Simple linear model: y = w*x + b\"\"\"\n",
    "    return jnp.dot(weights, x) + bias\n",
    "\n",
    "# Get gradients w.r.t. first two arguments (weights and bias)\n",
    "grad_fn = jax.grad(model, argnums=(0, 1))\n",
    "\n",
    "w = jnp.array([1.0, 2.0, 3.0])\n",
    "b = 0.5\n",
    "x = jnp.array([1.0, 1.0, 1.0])\n",
    "\n",
    "grad_w, grad_b = grad_fn(w, b, x)\n",
    "print(f\"Model: y = w¬∑x + b\")\n",
    "print(f\"Gradient w.r.t. weights: {grad_w}\")\n",
    "print(f\"Gradient w.r.t. bias: {grad_b}\")\n",
    "print(f\"(dL/dw = x, dL/db = 1)\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 8: Practical Example - Gradient Descent\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n8Ô∏è‚É£  GRADIENT DESCENT OPTIMIZATION\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"MSE loss for linear regression\"\"\"\n",
    "    prediction = params[0] * x + params[1]\n",
    "    return jnp.mean((prediction - y) ** 2)\n",
    "\n",
    "# Training data: y = 2x + 1 with some noise\n",
    "np.random.seed(42)\n",
    "x_data = jnp.array(np.linspace(0, 10, 20))\n",
    "y_data = 2 * x_data + 1 + np.random.randn(20) * 0.5\n",
    "\n",
    "# Initialize parameters\n",
    "params = jnp.array([0.0, 0.0])  # [slope, intercept]\n",
    "\n",
    "# Training loop\n",
    "learning_rate = 0.01\n",
    "grad_fn = jax.grad(loss_fn)\n",
    "\n",
    "print(\"Training linear regression with gradient descent...\")\n",
    "print(f\"True parameters: slope=2.0, intercept=1.0\")\n",
    "print(f\"Initial params: {params}\")\n",
    "\n",
    "for step in range(100):\n",
    "    grads = grad_fn(params, x_data, y_data)\n",
    "    params = params - learning_rate * grads\n",
    "    \n",
    "    if step % 20 == 0:\n",
    "        loss = loss_fn(params, x_data, y_data)\n",
    "        print(f\"Step {step:3d}: loss={loss:.4f}, params={params}\")\n",
    "\n",
    "final_loss = loss_fn(params, x_data, y_data)\n",
    "print(f\"\\nFinal params: slope={params[0]:.3f}, intercept={params[1]:.3f}\")\n",
    "print(f\"Final loss: {final_loss:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY POINTS - AUTOMATIC DIFFERENTIATION\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ jax.grad() computes exact derivatives (not numerical approximations)\n",
    "‚úÖ Works with any Python/JAX function - no manual chain rule needed\n",
    "‚úÖ Efficient reverse-mode autodiff (backpropagation)\n",
    "‚úÖ Can compute higher-order derivatives by composing grad()\n",
    "‚úÖ value_and_grad() gives both function value and gradient\n",
    "‚úÖ Jacobian and Hessian for vector functions and second derivatives\n",
    "‚úÖ argnums parameter controls which arguments to differentiate\n",
    "‚úÖ Combine with JIT for ultra-fast gradient computations\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdca59b",
   "metadata": {},
   "source": [
    "# Vectorization with vmap\n",
    "\n",
    "`vmap` is automatic batching. You write code that works on a single example, then `vmap` transforms it to work on batches - no loops needed.\n",
    "\n",
    "## Why vmap Matters\n",
    "\n",
    "Normally you'd write batched operations manually (lots of index wrangling). With `vmap`, you write single-example code and JAX handles the batching. This is particularly useful with `grad` for computing per-sample gradients.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "`jax.vmap(function, in_axes, out_axes)` transforms a function to automatically batch over specified axes. The key parameter is `in_axes` which tells JAX which dimensions to map over.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c67cd6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUTOMATIC BATCHING WITH VMAP\n",
      "======================================================================\n",
      "\n",
      "1Ô∏è‚É£  BASIC VMAP\n",
      "----------------------------------------------------------------------\n",
      "Function for single input: square(x) = x^2\n",
      "Input batch: [1. 2. 3. 4. 5.]\n",
      "Output: [ 1.  4.  9. 16. 25.]\n",
      "vmap automatically applies the function to each element!\n",
      "\n",
      "2Ô∏è‚É£  VMAP WITH MULTIPLE INPUTS\n",
      "----------------------------------------------------------------------\n",
      "Batch: [1. 2. 3.]\n",
      "Fixed weight: 2.5\n",
      "Result: [2.5 5.  7.5]\n",
      "in_axes=(0, None) batches first arg, keeps second fixed\n",
      "\n",
      "With batched weights: [1. 2. 3.]\n",
      "Result: [1. 4. 9.]\n",
      "\n",
      "3Ô∏è‚É£  MATRIX-VECTOR PRODUCTS\n",
      "----------------------------------------------------------------------\n",
      "Matrix:\n",
      "[[1. 2.]\n",
      " [3. 4.]]\n",
      "Vector batch (shape (3, 2)):\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 1.]]\n",
      "Results (shape (3, 2)):\n",
      "[[1. 3.]\n",
      " [2. 4.]\n",
      " [3. 7.]]\n",
      "\n",
      "4Ô∏è‚É£  PER-SAMPLE GRADIENTS\n",
      "----------------------------------------------------------------------\n",
      "Parameters: [1. 2. 3.]\n",
      "Batch size: 4\n",
      "Per-sample gradients (shape (4, 3)):\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Each row is the gradient for one training example!\n",
      "This is critical for differential privacy and some RL algorithms.\n",
      "\n",
      "5Ô∏è‚É£  PERFORMANCE: LOOP vs VMAP\n",
      "----------------------------------------------------------------------\n",
      "Parameters: [1. 2. 3.]\n",
      "Batch size: 4\n",
      "Per-sample gradients (shape (4, 3)):\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "\n",
      "Each row is the gradient for one training example!\n",
      "This is critical for differential privacy and some RL algorithms.\n",
      "\n",
      "5Ô∏è‚É£  PERFORMANCE: LOOP vs VMAP\n",
      "----------------------------------------------------------------------\n",
      "Batch size: 1000\n",
      "Loop version:  15.1654 seconds\n",
      "vmap version:  0.2413 seconds\n",
      "Speedup: 62.85x faster with vmap!\n",
      "\n",
      "6Ô∏è‚É£  NESTED VMAP\n",
      "----------------------------------------------------------------------\n",
      "Set 1 (shape (3, 2)):\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Set 2 (shape (2, 2)):\n",
      "[[1. 1.]\n",
      " [2. 2.]]\n",
      "Pairwise distances (shape (3, 2)):\n",
      "[[1.4142135 2.828427 ]\n",
      " [1.        2.236068 ]\n",
      " [1.        2.236068 ]]\n",
      "Each row shows distances from one point in set1 to all points in set2\n",
      "\n",
      "7Ô∏è‚É£  CONTROLLING OUTPUT AXES\n",
      "----------------------------------------------------------------------\n",
      "Scales: [1. 2. 3.]\n",
      "Output shape with out_axes=0 (default): (3, 2, 2)\n",
      "Result:\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[2. 0.]\n",
      "  [0. 2.]]\n",
      "\n",
      " [[3. 0.]\n",
      "  [0. 3.]]]\n",
      "\n",
      "Output shape with out_axes=1: (2, 3, 2)\n",
      "\n",
      "8Ô∏è‚É£  JIT + VMAP COMBO\n",
      "----------------------------------------------------------------------\n",
      "Batch size: 1000\n",
      "Loop version:  15.1654 seconds\n",
      "vmap version:  0.2413 seconds\n",
      "Speedup: 62.85x faster with vmap!\n",
      "\n",
      "6Ô∏è‚É£  NESTED VMAP\n",
      "----------------------------------------------------------------------\n",
      "Set 1 (shape (3, 2)):\n",
      "[[0. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "Set 2 (shape (2, 2)):\n",
      "[[1. 1.]\n",
      " [2. 2.]]\n",
      "Pairwise distances (shape (3, 2)):\n",
      "[[1.4142135 2.828427 ]\n",
      " [1.        2.236068 ]\n",
      " [1.        2.236068 ]]\n",
      "Each row shows distances from one point in set1 to all points in set2\n",
      "\n",
      "7Ô∏è‚É£  CONTROLLING OUTPUT AXES\n",
      "----------------------------------------------------------------------\n",
      "Scales: [1. 2. 3.]\n",
      "Output shape with out_axes=0 (default): (3, 2, 2)\n",
      "Result:\n",
      "[[[1. 0.]\n",
      "  [0. 1.]]\n",
      "\n",
      " [[2. 0.]\n",
      "  [0. 2.]]\n",
      "\n",
      " [[3. 0.]\n",
      "  [0. 3.]]]\n",
      "\n",
      "Output shape with out_axes=1: (2, 3, 2)\n",
      "\n",
      "8Ô∏è‚É£  JIT + VMAP COMBO\n",
      "----------------------------------------------------------------------\n",
      "Combined jax.jit(jax.vmap(function)) for maximum performance\n",
      "Time for 1000 iterations on batch of 1000: 0.2873s\n",
      "Both JIT compilation and automatic batching working together!\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS - VMAP\n",
      "======================================================================\n",
      "\n",
      "‚úÖ vmap automatically batches functions - no manual loop writing\n",
      "‚úÖ Write single-example code, vmap handles the batching\n",
      "‚úÖ in_axes controls which dimensions to map over (0, None, etc.)\n",
      "‚úÖ Much faster than Python loops (compiled vectorization)\n",
      "‚úÖ Perfect for per-sample gradients: vmap(grad(loss))\n",
      "‚úÖ Can nest vmap for multi-dimensional batching\n",
      "‚úÖ Combine with JIT for maximum performance\n",
      "‚úÖ out_axes controls how outputs are stacked\n",
      "‚úÖ Cleaner code: intent is clear, no index juggling\n",
      "\n",
      "Combined jax.jit(jax.vmap(function)) for maximum performance\n",
      "Time for 1000 iterations on batch of 1000: 0.2873s\n",
      "Both JIT compilation and automatic batching working together!\n",
      "\n",
      "======================================================================\n",
      "KEY POINTS - VMAP\n",
      "======================================================================\n",
      "\n",
      "‚úÖ vmap automatically batches functions - no manual loop writing\n",
      "‚úÖ Write single-example code, vmap handles the batching\n",
      "‚úÖ in_axes controls which dimensions to map over (0, None, etc.)\n",
      "‚úÖ Much faster than Python loops (compiled vectorization)\n",
      "‚úÖ Perfect for per-sample gradients: vmap(grad(loss))\n",
      "‚úÖ Can nest vmap for multi-dimensional batching\n",
      "‚úÖ Combine with JIT for maximum performance\n",
      "‚úÖ out_axes controls how outputs are stacked\n",
      "‚úÖ Cleaner code: intent is clear, no index juggling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# VECTORIZATION WITH VMAP\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUTOMATIC BATCHING WITH VMAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 1: Basic vmap - Single Input Batching\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n1Ô∏è‚É£  BASIC VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def square(x):\n",
    "    \"\"\"Operates on a SINGLE number\"\"\"\n",
    "    return x ** 2\n",
    "\n",
    "# Create vectorized version\n",
    "vectorized_square = jax.vmap(square)\n",
    "\n",
    "# Apply to batch\n",
    "batch = jnp.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "result = vectorized_square(batch)\n",
    "\n",
    "print(\"Function for single input: square(x) = x^2\")\n",
    "print(f\"Input batch: {batch}\")\n",
    "print(f\"Output: {result}\")\n",
    "print(\"vmap automatically applies the function to each element!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 2: Multiple Inputs with Different Batching\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n2Ô∏è‚É£  VMAP WITH MULTIPLE INPUTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def weighted_sum(x, weight):\n",
    "    \"\"\"Compute weighted sum: x * weight\"\"\"\n",
    "    return x * weight\n",
    "\n",
    "# Batch over first argument, keep second fixed\n",
    "# in_axes=(0, None) means: map over axis 0 of x, don't map over weight\n",
    "batched_fn = jax.vmap(weighted_sum, in_axes=(0, None))\n",
    "\n",
    "x_batch = jnp.array([1.0, 2.0, 3.0])\n",
    "weight = 2.5\n",
    "\n",
    "result = batched_fn(x_batch, weight)\n",
    "print(f\"Batch: {x_batch}\")\n",
    "print(f\"Fixed weight: {weight}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(\"in_axes=(0, None) batches first arg, keeps second fixed\")\n",
    "\n",
    "# Batch over both arguments\n",
    "batched_fn_both = jax.vmap(weighted_sum, in_axes=(0, 0))\n",
    "weights_batch = jnp.array([1.0, 2.0, 3.0])\n",
    "result_both = batched_fn_both(x_batch, weights_batch)\n",
    "print(f\"\\nWith batched weights: {weights_batch}\")\n",
    "print(f\"Result: {result_both}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 3: Matrix-Vector Product with vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n3Ô∏è‚É£  MATRIX-VECTOR PRODUCTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def matvec(matrix, vector):\n",
    "    \"\"\"Single matrix-vector product\"\"\"\n",
    "    return jnp.dot(matrix, vector)\n",
    "\n",
    "# Batch over vectors (multiple vectors, same matrix)\n",
    "batch_matvec = jax.vmap(matvec, in_axes=(None, 0))\n",
    "\n",
    "matrix = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "vectors = jnp.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]])\n",
    "\n",
    "results = batch_matvec(matrix, vectors)\n",
    "print(f\"Matrix:\\n{matrix}\")\n",
    "print(f\"Vector batch (shape {vectors.shape}):\\n{vectors}\")\n",
    "print(f\"Results (shape {results.shape}):\\n{results}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 4: Per-Sample Gradients (The Killer App!)\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n4Ô∏è‚É£  PER-SAMPLE GRADIENTS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def loss_single_sample(params, x, y):\n",
    "    \"\"\"Loss for ONE training example\"\"\"\n",
    "    prediction = jnp.dot(params, x)\n",
    "    return (prediction - y) ** 2\n",
    "\n",
    "# Create function that computes gradient for one sample\n",
    "grad_fn_single = jax.grad(loss_single_sample)\n",
    "\n",
    "# Vectorize it to compute per-sample gradients for entire batch!\n",
    "# in_axes=(None, 0, 0) means: same params, batch over x and y\n",
    "per_sample_grads = jax.vmap(grad_fn_single, in_axes=(None, 0, 0))\n",
    "\n",
    "# Test data\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "x_batch = jnp.array([\n",
    "    [1.0, 0.0, 0.0],\n",
    "    [0.0, 1.0, 0.0],\n",
    "    [0.0, 0.0, 1.0],\n",
    "    [1.0, 1.0, 1.0]\n",
    "])\n",
    "y_batch = jnp.array([1.0, 2.0, 3.0, 6.0])\n",
    "\n",
    "grads = per_sample_grads(params, x_batch, y_batch)\n",
    "print(f\"Parameters: {params}\")\n",
    "print(f\"Batch size: {len(x_batch)}\")\n",
    "print(f\"Per-sample gradients (shape {grads.shape}):\")\n",
    "print(grads)\n",
    "print(\"\\nEach row is the gradient for one training example!\")\n",
    "print(\"This is critical for differential privacy and some RL algorithms.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 5: Performance Comparison - Loop vs vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n5Ô∏è‚É£  PERFORMANCE: LOOP vs VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def compute_single(x):\n",
    "    \"\"\"Some computation on a single input\"\"\"\n",
    "    return jnp.sum(jnp.sin(x) ** 2 + jnp.cos(x) ** 2)\n",
    "\n",
    "# Manual loop version\n",
    "def loop_version(batch):\n",
    "    results = []\n",
    "    for x in batch:\n",
    "        results.append(compute_single(x))\n",
    "    return jnp.array(results)\n",
    "\n",
    "# vmap version\n",
    "vmap_version = jax.vmap(compute_single)\n",
    "\n",
    "# Generate test batch\n",
    "batch_size = 1000\n",
    "test_batch = jnp.ones((batch_size, 100))\n",
    "\n",
    "# Warm up JIT\n",
    "_ = vmap_version(test_batch)\n",
    "\n",
    "# Benchmark loop\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = loop_version(test_batch)\n",
    "time_loop = time.time() - start\n",
    "\n",
    "# Benchmark vmap\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    _ = vmap_version(test_batch)\n",
    "time_vmap = time.time() - start\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Loop version:  {time_loop:.4f} seconds\")\n",
    "print(f\"vmap version:  {time_vmap:.4f} seconds\")\n",
    "print(f\"Speedup: {time_loop/time_vmap:.2f}x faster with vmap!\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 6: Nested vmap - Batching Over Multiple Dimensions\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n6Ô∏è‚É£  NESTED VMAP\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def pairwise_distance(x1, x2):\n",
    "    \"\"\"Distance between two vectors\"\"\"\n",
    "    return jnp.sqrt(jnp.sum((x1 - x2) ** 2))\n",
    "\n",
    "# Compute pairwise distances between all vectors in two sets\n",
    "# First vmap: over first set\n",
    "# Second vmap: over second set\n",
    "pairwise_distances = jax.vmap(\n",
    "    lambda x1: jax.vmap(lambda x2: pairwise_distance(x1, x2))(set2)\n",
    ")\n",
    "\n",
    "set1 = jnp.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "set2 = jnp.array([[1.0, 1.0], [2.0, 2.0]])\n",
    "\n",
    "distances = pairwise_distances(set1)\n",
    "print(f\"Set 1 (shape {set1.shape}):\\n{set1}\")\n",
    "print(f\"Set 2 (shape {set2.shape}):\\n{set2}\")\n",
    "print(f\"Pairwise distances (shape {distances.shape}):\\n{distances}\")\n",
    "print(\"Each row shows distances from one point in set1 to all points in set2\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 7: vmap with Different Output Axes\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n7Ô∏è‚É£  CONTROLLING OUTPUT AXES\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def create_matrix(scale):\n",
    "    \"\"\"Create a 2x2 matrix based on scale\"\"\"\n",
    "    return jnp.array([[scale, 0], [0, scale]])\n",
    "\n",
    "# Default: out_axes=0 (stack along first dimension)\n",
    "vmap_default = jax.vmap(create_matrix)\n",
    "scales = jnp.array([1.0, 2.0, 3.0])\n",
    "result_default = vmap_default(scales)\n",
    "\n",
    "print(f\"Scales: {scales}\")\n",
    "print(f\"Output shape with out_axes=0 (default): {result_default.shape}\")\n",
    "print(f\"Result:\\n{result_default}\")\n",
    "\n",
    "# With out_axes=1: stack along second dimension\n",
    "vmap_axis1 = jax.vmap(create_matrix, out_axes=1)\n",
    "result_axis1 = vmap_axis1(scales)\n",
    "print(f\"\\nOutput shape with out_axes=1: {result_axis1.shape}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example 8: Combining JIT and vmap\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n8Ô∏è‚É£  JIT + VMAP COMBO\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "def expensive_computation(x):\n",
    "    \"\"\"Some complex computation\"\"\"\n",
    "    result = x\n",
    "    for _ in range(10):\n",
    "        result = jnp.sin(result) + jnp.cos(result)\n",
    "    return result\n",
    "\n",
    "# Combine transformations: JIT the vmapped function\n",
    "fast_batch_compute = jax.jit(jax.vmap(expensive_computation))\n",
    "\n",
    "batch = jnp.ones(1000)\n",
    "\n",
    "# Warm up\n",
    "_ = fast_batch_compute(batch)\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "for _ in range(1000):\n",
    "    _ = fast_batch_compute(batch).block_until_ready()\n",
    "time_taken = time.time() - start\n",
    "\n",
    "print(\"Combined jax.jit(jax.vmap(function)) for maximum performance\")\n",
    "print(f\"Time for 1000 iterations on batch of 1000: {time_taken:.4f}s\")\n",
    "print(\"Both JIT compilation and automatic batching working together!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY POINTS - VMAP\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "‚úÖ vmap automatically batches functions - no manual loop writing\n",
    "‚úÖ Write single-example code, vmap handles the batching\n",
    "‚úÖ in_axes controls which dimensions to map over (0, None, etc.)\n",
    "‚úÖ Much faster than Python loops (compiled vectorization)\n",
    "‚úÖ Perfect for per-sample gradients: vmap(grad(loss))\n",
    "‚úÖ Can nest vmap for multi-dimensional batching\n",
    "‚úÖ Combine with JIT for maximum performance\n",
    "‚úÖ out_axes controls how outputs are stacked\n",
    "‚úÖ Cleaner code: intent is clear, no index juggling\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea6c06",
   "metadata": {},
   "source": [
    "# Putting It All Together\n",
    "\n",
    "Let's see how JIT, autodiff, and vmap work together. We'll build and train a neural network from scratch using just JAX - no high-level frameworks, just the core transformations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16a5d7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NEURAL NETWORK TRAINING - JAX PRIMITIVES ONLY\n",
      "======================================================================\n",
      "\n",
      "üìä Generating synthetic training data...\n",
      "Training data: 200 samples\n",
      "Input features: 2\n",
      "Output dimension: 1\n",
      "\n",
      "üß† Initializing network: 2 ‚Üí 16 ‚Üí 16 ‚Üí 1\n",
      "Total parameters: 337\n",
      "\n",
      "üèãÔ∏è  Training network...\n",
      "Total parameters: 337\n",
      "\n",
      "üèãÔ∏è  Training network...\n",
      "Epoch    0: Loss = 4.492578\n",
      "Epoch  100: Loss = 0.138198\n",
      "Epoch  200: Loss = 0.075353\n",
      "Epoch  300: Loss = 0.054473\n",
      "Epoch  400: Loss = 0.045023\n",
      "\n",
      "üìà Evaluating trained network...\n",
      "Final training loss: 0.039874\n",
      "Epoch    0: Loss = 4.492578\n",
      "Epoch  100: Loss = 0.138198\n",
      "Epoch  200: Loss = 0.075353\n",
      "Epoch  300: Loss = 0.054473\n",
      "Epoch  400: Loss = 0.045023\n",
      "\n",
      "üìà Evaluating trained network...\n",
      "Final training loss: 0.039874\n",
      "\n",
      "Test predictions vs expected:\n",
      "  Input: [0. 0.], Predicted: 1.0070, Expected: 1.0000\n",
      "  Input: [1. 0.], Predicted: 1.7170, Expected: 1.8415\n",
      "  Input: [0. 1.], Predicted: 0.4573, Expected: 0.5403\n",
      "\n",
      "======================================================================\n",
      "COMPUTING PER-SAMPLE GRADIENTS\n",
      "======================================================================\n",
      "\n",
      "Test predictions vs expected:\n",
      "  Input: [0. 0.], Predicted: 1.0070, Expected: 1.0000\n",
      "  Input: [1. 0.], Predicted: 1.7170, Expected: 1.8415\n",
      "  Input: [0. 1.], Predicted: 0.4573, Expected: 0.5403\n",
      "\n",
      "======================================================================\n",
      "COMPUTING PER-SAMPLE GRADIENTS\n",
      "======================================================================\n",
      "Computed per-sample gradients for 5 examples\n",
      "First layer weight gradients shape: (5, 2, 16)\n",
      "(5 samples, each with its own gradient)\n",
      "Computed per-sample gradients for 5 examples\n",
      "First layer weight gradients shape: (5, 2, 16)\n",
      "(5 samples, each with its own gradient)\n",
      "\n",
      "Average gradient (first weight): -0.001082\n",
      "Per-sample gradients (first weight):\n",
      "  Sample 0: -0.000000\n",
      "  Sample 1: -0.000000\n",
      "  Sample 2: -0.003320\n",
      "  Sample 3: 0.000000\n",
      "  Sample 4: -0.002091\n",
      "\n",
      "======================================================================\n",
      "WHAT WE JUST DID\n",
      "======================================================================\n",
      "\n",
      "1. Built a neural network from scratch using JAX primitives\n",
      "2. Used vmap to batch the forward pass (no manual loop)\n",
      "3. Used jax.grad to compute gradients automatically\n",
      "4. Used jax.jit to compile the loss+gradient computation\n",
      "5. Trained with vanilla gradient descent (no frameworks!)\n",
      "6. Computed per-sample gradients with vmap(grad(...))\n",
      "\n",
      "Key JAX features in action:\n",
      "‚úÖ JIT compilation - fast training loop\n",
      "‚úÖ Automatic differentiation - no manual backprop\n",
      "‚úÖ vmap - automatic batching and per-sample gradients\n",
      "‚úÖ Composability - combine transformations seamlessly\n",
      "\n",
      "This is the foundation of how JAX-based ML libraries (Flax, Haiku, Optax) work!\n",
      "\n",
      "\n",
      "Average gradient (first weight): -0.001082\n",
      "Per-sample gradients (first weight):\n",
      "  Sample 0: -0.000000\n",
      "  Sample 1: -0.000000\n",
      "  Sample 2: -0.003320\n",
      "  Sample 3: 0.000000\n",
      "  Sample 4: -0.002091\n",
      "\n",
      "======================================================================\n",
      "WHAT WE JUST DID\n",
      "======================================================================\n",
      "\n",
      "1. Built a neural network from scratch using JAX primitives\n",
      "2. Used vmap to batch the forward pass (no manual loop)\n",
      "3. Used jax.grad to compute gradients automatically\n",
      "4. Used jax.jit to compile the loss+gradient computation\n",
      "5. Trained with vanilla gradient descent (no frameworks!)\n",
      "6. Computed per-sample gradients with vmap(grad(...))\n",
      "\n",
      "Key JAX features in action:\n",
      "‚úÖ JIT compilation - fast training loop\n",
      "‚úÖ Automatic differentiation - no manual backprop\n",
      "‚úÖ vmap - automatic batching and per-sample gradients\n",
      "‚úÖ Composability - combine transformations seamlessly\n",
      "\n",
      "This is the foundation of how JAX-based ML libraries (Flax, Haiku, Optax) work!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# COMPLETE EXAMPLE - TRAINING A NEURAL NETWORK\n",
    "# =============================================================================\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"NEURAL NETWORK TRAINING - JAX PRIMITIVES ONLY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Network Architecture and Initialization\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def init_network_params(layer_sizes, key):\n",
    "    \"\"\"Initialize neural network parameters with random values.\"\"\"\n",
    "    params = []\n",
    "    keys = jax.random.split(key, len(layer_sizes) - 1)\n",
    "    \n",
    "    for i, (key, n_in, n_out) in enumerate(zip(keys, layer_sizes[:-1], layer_sizes[1:])):\n",
    "        # Xavier initialization\n",
    "        weight_key, bias_key = jax.random.split(key)\n",
    "        scale = jnp.sqrt(2.0 / n_in)\n",
    "        W = scale * jax.random.normal(weight_key, (n_in, n_out))\n",
    "        b = jnp.zeros(n_out)\n",
    "        params.append({'W': W, 'b': b})\n",
    "    \n",
    "    return params\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"ReLU activation function.\"\"\"\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "def forward_pass(params, x):\n",
    "    \"\"\"Forward pass through the network (for a SINGLE example).\"\"\"\n",
    "    activation = x\n",
    "    \n",
    "    # Hidden layers with ReLU\n",
    "    for layer in params[:-1]:\n",
    "        activation = jnp.dot(activation, layer['W']) + layer['b']\n",
    "        activation = relu(activation)\n",
    "    \n",
    "    # Output layer (no activation)\n",
    "    final_layer = params[-1]\n",
    "    output = jnp.dot(activation, final_layer['W']) + final_layer['b']\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Batch the forward pass using vmap\n",
    "batched_forward = jax.vmap(forward_pass, in_axes=(None, 0))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Loss Function and Gradient Computation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def mse_loss(params, x_batch, y_batch):\n",
    "    \"\"\"Mean squared error loss for the entire batch.\"\"\"\n",
    "    predictions = batched_forward(params, x_batch)\n",
    "    return jnp.mean((predictions - y_batch) ** 2)\n",
    "\n",
    "# Create loss and gradient function (JIT compiled for speed)\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(mse_loss))\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Training Loop\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_network(params, x_train, y_train, num_epochs, learning_rate):\n",
    "    \"\"\"Train the network using gradient descent.\"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        loss, grads = loss_and_grad(params, x_train, y_train)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = [\n",
    "            {\n",
    "                'W': layer['W'] - learning_rate * grad_layer['W'],\n",
    "                'b': layer['b'] - learning_rate * grad_layer['b']\n",
    "            }\n",
    "            for layer, grad_layer in zip(params, grads)\n",
    "        ]\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch:4d}: Loss = {loss:.6f}\")\n",
    "    \n",
    "    return params\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Generate Training Data\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìä Generating synthetic training data...\")\n",
    "\n",
    "# Create a simple non-linear function: y = sin(x1) + cos(x2)\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "x_train = np.random.randn(n_samples, 2).astype(np.float32)\n",
    "y_train = (np.sin(x_train[:, 0]) + np.cos(x_train[:, 1])).reshape(-1, 1).astype(np.float32)\n",
    "\n",
    "x_train = jnp.array(x_train)\n",
    "y_train = jnp.array(y_train)\n",
    "\n",
    "print(f\"Training data: {x_train.shape[0]} samples\")\n",
    "print(f\"Input features: {x_train.shape[1]}\")\n",
    "print(f\"Output dimension: {y_train.shape[1]}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Initialize and Train Network\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüß† Initializing network: 2 ‚Üí 16 ‚Üí 16 ‚Üí 1\")\n",
    "\n",
    "layer_sizes = [2, 16, 16, 1]  # Input, hidden1, hidden2, output\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = init_network_params(layer_sizes, key)\n",
    "\n",
    "print(f\"Total parameters: {sum(p['W'].size + p['b'].size for p in params)}\")\n",
    "\n",
    "print(\"\\nüèãÔ∏è  Training network...\")\n",
    "params = train_network(params, x_train, y_train, num_epochs=500, learning_rate=0.01)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluate Trained Network\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nüìà Evaluating trained network...\")\n",
    "\n",
    "final_loss, _ = loss_and_grad(params, x_train, y_train)\n",
    "print(f\"Final training loss: {final_loss:.6f}\")\n",
    "\n",
    "# Test on a few examples\n",
    "test_inputs = jnp.array([[0.0, 0.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "predictions = batched_forward(params, test_inputs)\n",
    "expected = jnp.array([[jnp.sin(x[0]) + jnp.cos(x[1])] for x in test_inputs])\n",
    "\n",
    "print(\"\\nTest predictions vs expected:\")\n",
    "for i, (inp, pred, exp) in enumerate(zip(test_inputs, predictions, expected)):\n",
    "    print(f\"  Input: {inp}, Predicted: {pred[0]:.4f}, Expected: {exp[0]:.4f}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Demonstrate Per-Sample Gradients\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPUTING PER-SAMPLE GRADIENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def loss_single(params, x, y):\n",
    "    \"\"\"Loss for a single example.\"\"\"\n",
    "    pred = forward_pass(params, x)\n",
    "    return jnp.sum((pred - y) ** 2)\n",
    "\n",
    "# Create per-sample gradient function\n",
    "per_sample_grad_fn = jax.vmap(jax.grad(loss_single), in_axes=(None, 0, 0))\n",
    "\n",
    "# Compute per-sample gradients for a small batch\n",
    "small_batch_x = x_train[:5]\n",
    "small_batch_y = y_train[:5]\n",
    "\n",
    "per_sample_grads = per_sample_grad_fn(params, small_batch_x, small_batch_y)\n",
    "\n",
    "print(f\"Computed per-sample gradients for {len(small_batch_x)} examples\")\n",
    "print(f\"First layer weight gradients shape: {per_sample_grads[0]['W'].shape}\")\n",
    "print(\"(5 samples, each with its own gradient)\")\n",
    "\n",
    "# Show that these differ from the averaged gradient\n",
    "avg_grad = jax.grad(mse_loss)(params, small_batch_x, small_batch_y)\n",
    "print(f\"\\nAverage gradient (first weight): {avg_grad[0]['W'][0, 0]:.6f}\")\n",
    "print(f\"Per-sample gradients (first weight):\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sample {i}: {per_sample_grads[0]['W'][i, 0, 0]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"WHAT WE JUST DID\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\"\"\n",
    "1. Built a neural network from scratch using JAX primitives\n",
    "2. Used vmap to batch the forward pass (no manual loop)\n",
    "3. Used jax.grad to compute gradients automatically\n",
    "4. Used jax.jit to compile the loss+gradient computation\n",
    "5. Trained with vanilla gradient descent (no frameworks!)\n",
    "6. Computed per-sample gradients with vmap(grad(...))\n",
    "\n",
    "Key JAX features in action:\n",
    "‚úÖ JIT compilation - fast training loop\n",
    "‚úÖ Automatic differentiation - no manual backprop\n",
    "‚úÖ vmap - automatic batching and per-sample gradients\n",
    "‚úÖ Composability - combine transformations seamlessly\n",
    "\n",
    "This is the foundation of how JAX-based ML libraries (Flax, Haiku, Optax) work!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e95f440",
   "metadata": {},
   "source": [
    "# Summary and Next Steps\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "This notebook covered the three core JAX transformations:\n",
    "\n",
    "1. **JIT Compilation** (`jax.jit`) - Makes code run 10-100x faster by compiling to optimized machine code\n",
    "2. **Automatic Differentiation** (`jax.grad`, `jax.jacobian`, etc.) - Computes exact derivatives without manual calculus\n",
    "3. **Automatic Batching** (`jax.vmap`) - Transforms single-example code to work on batches efficiently\n",
    "\n",
    "These three transformations compose freely. You can JIT a vmapped gradient function, or vmap a JIT-compiled function - they just work together.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- JAX arrays are **immutable** - use `.at[].set()` instead of assignment\n",
    "- Use **JAX control flow** (`jnp.where`, `jax.lax.cond`) inside JIT functions, not Python `if/else`\n",
    "- **Pure functions** work best - avoid side effects like print statements\n",
    "- `vmap` writes much cleaner code than manual batching\n",
    "- Combine transformations for maximum power: `jax.jit(jax.vmap(jax.grad(...)))`\n",
    "\n",
    "## What's Next\n",
    "\n",
    "- **Random Numbers**: JAX uses explicit random keys (no global state)\n",
    "- **pytrees**: JAX's way of handling nested structures (dicts, lists of arrays)\n",
    "- **pmap**: Parallel map for multi-GPU/TPU computation\n",
    "- **High-level libraries**: Flax (neural networks), Optax (optimizers), Haiku\n",
    "- **Scan and while loops**: `jax.lax.scan` for efficient sequential operations\n",
    "\n",
    "## Where JAX Shines\n",
    "\n",
    "- Training neural networks (especially with custom architectures)\n",
    "- Scientific computing and numerical optimization\n",
    "- Differential equations and physics simulations\n",
    "- Anywhere you need fast numerical code with gradients\n",
    "- Research where you want control over the training loop\n",
    "\n",
    "JAX gives you NumPy's simplicity with the performance and capabilities of a modern ML framework. That's the sweet spot.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
